{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1. lets load pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a3c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34764a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyPDFLoader\n",
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\\nLarge Language Model Inference\\nQingyuan Li 1 Bo Zhang 1 Liang Ye1 Yifan Zhang 1 Wei Wu1 Yerui Sun1 Lin Ma 1 Yuchen Xie1\\nAbstract\\nThe ever-increasing sizes of large language mod-\\nels necessitate distributed solutions for fast infer-\\nence that exploit multi-dimensional parallelism,\\nwhere computational loads are split across vari-\\nous accelerators such as GPU clusters. However,\\nthis approach often introduces significant com-\\nmunication overhead, especially on devices with\\nlimited bandwidth. In this paper, we introduce\\nFlash Communication, a novel low-bit compres-\\nsion technique designed to alleviate the tensor-\\nparallelism communication bottleneck during in-\\nference. Our method substantially boosts intra-\\nnode communication speed by more than 3× and\\nreduces the time-to-first-token by 2×, with nearly\\nno sacrifice in model accuracy. Extensive experi-\\nments on various up-to-date LLMs demonstrate\\nthe effectiveness of our approach.\\n1. Introduction\\nTo date, the number of parameters of large language models\\nhas tremendously increased. For instance, GPT-3 (Brown\\net al., 2020) has 175B, DeepSeek V2 (Liu et al., 2024)\\nutilizes 236B, LLaMA-3 (Dubey et al., 2024) reaches 450B.\\nTheir enormous sizes create big challenges for both training\\nand inference.\\nTo tackle the scaling difficulties of large language models,\\nthe research community now resorts to multiple parallelism\\nstrategies across a large group of computing accelerators.\\nSince previous parallelism methods focus on resolving the\\ntraining challenges, we quickly review these methods for\\na background check. Particularly, data parallelism (Dean\\net al., 2012; Ben-Nun & Hoefler, 2019) is first introduced\\nto allocate the training samples onto multiple GPUs where\\neach GPU retains a duplicate of the model and processes\\nits own given batch of samples. Synchronization is hence\\nrequired at the end of each iteration to update the model pa-\\n*Equal contribution 1Meituan. Correspondence to: Qingyuan\\nLi <liqingyuan02@meituan.com>.\\nPreprint.\\nL40 FP16/FP16L40 FP16/INT4L40 INT8/FP16L40 INT8/INT4A100 FP16/FP16A100 FP16/INT4A100 INT8/FP16A100 INT8/INT4\\n0\\n20\\n40\\n60\\n80\\n100Cost Percentage\\n52.2\\n71.3\\n21.9\\n40.1\\n75.1 80.6\\n59.2 65.3\\n42.4\\n22.3\\n65.9\\n37.7\\n18.4 11.6\\n25.1 17.4\\n3.4 3.9\\n5.6\\n10.1\\n3.8 4.0\\n2.8 6.2\\n2.0 2.5 6.6 12.0\\n2.7 3.9\\n12.9 11.1\\nLLaMA-3-70B Latency Breakdown\\nFP16 GEMM\\nINT8 GEMM\\nFP16 AllReduce\\nFlash AllReduce\\nNorm&Act\\nOthers\\nFigure 1.Prefill cost breakdown of LLaMA-3-70B operations\\nwith and without Flash Communication, as measured by\\nNSys (NVIDIA, 2024b). Tested on 4 ×L40/A100 GPUs (TP=4)\\nwith a batch size of 8, each with 1024 input and 64 output tokens.\\nNCCL (NVIDIA, 2024d)’s Ring All-Reduce is applied. The notion\\nof x-ticks (e.g. L40 FP16/FP16) denotes GPU type, model weight\\nprecision, and communication precision, respectively.\\nrameters. In the LLM era, ZeRO (Rajbhandari et al., 2020)\\nand FSDP (Zhao et al., 2023) renovate data parallelism by\\nsharding models on all devices but virtually rendering a\\nwhole model on a single device through All-Gather com-\\nmunication. In contrast, pipeline parallelism partitions se-\\nquential layers onto different GPUs where point-to-point\\ncommunication is adopted to transmit activation and gradi-\\nents. However, it creates data dependency which leads to\\nsubstantial GPU idle time, called bubbles. To improve GPU\\nutilization, GPipe (Huang et al., 2019) schedules micro-\\nbatches in a pipeline with forward passes and then followed\\nby backward passes. PipeDream (Harlap et al., 2018) pro-\\nposes one-forward one-backward (1F1B) to further reduce\\nthe bubble ratio. Megatron-LM (Narayanan et al., 2021)\\nadvances PipeDream by allowing each device to perform\\ncomputation for multiple non-contiguous subsets of layers.\\nAnother dimension to split the model is tensor parallelism\\nwhich splits the tensors of each layer and performs All-\\nReduce to aggregate the activation and gradients from all\\ndevices. Megatron-LM (Shoeybi et al., 2019; Narayanan\\n1\\narXiv:2412.04964v1  [cs.AI]  6 Dec 2024'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='Flash Communication\\net al., 2021) is such an example that devises delicate con-\\njugate tensor slicing schemes (e.g. column-wise first and\\nrow-wise next for the MLP layer) to remove unnecessary\\nsynchronization inside a transformer block.\\nWith the rise of LLMs developed for the long-context sce-\\nnario, sequential parallelism (Korthikanti et al., 2023) is pro-\\nposed to divide activation of LayerNorm and Dropout layers\\nin the sequence dimension as they are sequence-independent.\\nIt also jointly combines with tensor-parallelism by replacing\\ntwo All-Reduce operations with one All-Gather and one\\nReduce-Scatter to merge the communication cost. However,\\nself-attention and MLP layers are left untouched for sequen-\\ntial parallelism. In this regard,context parallelism(NVIDIA,\\n2024b) is designed to separate all layers in sequence dimen-\\nsion. To break the sequence dependency in self-attention,\\nRing Attention (Liu et al., 2023) applies blockwise self-\\nattention and feedforward (Dao, 2023; Liu & Abbeel, 2023)\\nin a distributed environment with point-to-point communi-\\ncation. On top of this, Deepspeed-Ulysses (Jacobs et al.,\\n2023) exchanges point-to-point communication for All2All\\nfor faster speed.\\nAnother emerging direction is sparse architectures repre-\\nsented by mixture-of-experts models (Jiang et al., 2024;\\nTeam, 2024; Liu et al., 2024). Expert parallelism (Fedus\\net al., 2022) parallelizes the experts on different GPUs which\\nrequires All2All communication. Deepspeed-MoE (Rajb-\\nhandari et al., 2022) propose hierarchical All2All communi-\\ncation to reduce the number of communication hops.\\nAs large language models continue to scale up, modern\\nframeworks like DeepSpeed (Microsoft, 2024), and Mega-\\ntron (NVIDIA, 2024a) tend to make joint use of the afore-\\nmentioned parallelism to accelerate the training process.\\nNevertheless, they easily meet communication bottlenecks\\nas they require many collective operations. This overhead\\ngrows as the model becomes larger.\\nMeanwhile, the communication bottleneck is also pro-\\nnounced when serving large language models in the cloud.\\nConstrained by strict service level objectives (SLOs), multi-\\nple parallelism schemes are adopted to speed up the infer-\\nence, where tensor parallelism is the most popular option\\namong all. Besides, due to the lower bandwidth of infer-\\nence GPUs, communication can account for more than half\\nof the prefill inference cost where an 80-layer LLaMA-3-\\n70B (Dubey et al., 2024) carries out 160 all-reduce opera-\\ntions at each forward pass on 4 ×L40 GPUs, as shown in\\nFigure 1. Therefore, to enable a faster speed, we must make\\nefficient use of limited intra-node bandwidth.\\nIn this work, we design a novel technique to reduce the com-\\nmunication cost introduced by tensor parallelism without\\nsubstantially sacrificing accuracy. Our contributions are,\\n1. Through detailed measurements, we unveil the com-\\nmunication bottleneck problem that also recurs in large\\nlanguage model (LLM) inference. For instance, com-\\nmunication can account for up to 65% of the total\\nlatency on NVIDIA L40 GPUs (Fig. 1).\\n2. We design an efficient communication mechanism\\ncalled Flash Communication, which applies low-bit\\nfine-grained quantization on activations to reduce com-\\nmunication volume and employs a two-step all-reduce\\nstrategy to minimize communication hops.\\n3. We implement a fused CUDA kernel called Flash All-\\nReduce to perform Flash Communication, achieving\\nup to a 2 × reduction in time-to-first-token (TTFT)\\non NVIDIA L40 GPUs. Even on A100 GPUs with\\nhigher communication bandwidth, we observe notable\\nlatency reductions, demonstrating the effectiveness of\\nour method.\\n2. Related Work\\nBefore diving into the investigated problem, we cover\\nsome fundamental knowledge required for discussion in\\nAppendix A. We suggest that readers without prior experi-\\nence quickly review the content.\\nCommunication efficiency is crucial to distributed training\\nand serving, as it directly affects the total processing time\\nand cost. Several techniques have been proposed to opti-\\nmize communication in distributed training in recent years,\\nincluding topology optimization, pipeline optimization, and\\ncompression methods.\\n2.1. Topology Optmization\\nTopology optimization adjusts communication patterns to\\nmatch the physical topology of hardware to reduce com-\\nmunication latency/hops, mainly ring-based and tree-based.\\nRing-All-Reduce (Baidu Research, 2024) organizes work-\\ners in a ring topology so that the overall communication\\nlatency is constant regardless of the number of workers. Say\\na worker transmits data of volume M to a group of N − 1\\nworkers, the total communication volume is2M(N −1)/N,\\nwhich is approximately 2M when N >> 1. However, it\\ndoesn’t take the physical topology into account, where intra-\\nand inter-node communication have different bandwidths.\\nHence the average speed depends largely on the lowest band-\\nwidth in such a strategy. Hierarchical Ring-All-Reduce (Jia\\net al., 2018) highlights the importance of hierarchical struc-\\ntures in managing overheads, which employs three-phase\\nall-reduce for separate intra- and inter-node communication.\\nLater, 2D-Torus (Mikami et al., 2018) organizes GPUs in a\\n2D-grid of (X, Y) so that the inter-node horizontal commu-\\nnication volume is X times smaller than that of hierarchical\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Flash Communication\\nRing-All-Reduce. NCCL (NVIDIA, 2019) introduces dou-\\nble binary trees (Sanders et al., 2009) provides logarithmic\\nlatency by reducing hops from 2(N − 1) to 2log(N). How-\\never, it is more prone to result in suboptimal bandwidth\\nutilization, as only a subset of nodes are engaged in any\\ngiven communication step.\\nWith the rise of sparse architectures like mixture-of-experts,\\nAll2All collective operation is common for communication\\nin expert parallelism. DeepSpeed-MoE (Rajbhandari et al.,\\n2022) and HetuMoE (Nie et al., 2022) both utilize a scalable\\nhierarchical scheme to reduce all-to-all communication hops\\nfor faster speed.\\nBesides, NVLink SHARP (NVIDIA, 2023) is a hardware\\nimprovement that offloads collective operations from GPUs\\nto the network devices, hence eliminating the need to send\\ndata multiple times between endpoints.\\n2.2. Pipeline Optimization\\nPipelining optimization aims to maximize resource utiliza-\\ntion with optimized scheduling strategies, mainly by over-\\nlapping computation with communication. Domino (Wang\\net al., 2024) breaks data dependency in Tensor-parallelism\\nby splitting activations row-wisely and weights column-\\nwisely into smaller independent parts. FLUX (Chang et al.,\\n2024) divides computation and communication operations\\ninto much finer-grained operations and later merges them\\nin a larger kernel to effectively hide communication. Dis-\\ntributedGEMM (Hassani et al., 2024) provides an imple-\\nmentation based on CUTLUSS (NVIDIA, 2024f) using P2P\\ncommunication. ScMoE (Cai et al., 2024) implements a\\nshortcut-connected MoE architecture to effectively decouple\\ncommunication from its conventional sequence, allowing\\nfor a substantial overlap.\\n2.3. Communication Compression\\nCompression techniques like sparsification and quantiza-\\ntion are proposed to balance communication reduction with\\nacceptable performance degradation. Sparse Communica-\\ntion (Aji & Heafield, 2017) observes that gradient updates\\nare mostly close to zero and maps them directly to zero to\\nonly exchange sparse matrices among distributed nodes. In\\ncontrast, DISCO (Qin et al., 2024) aims to achieve sparse\\ncommunication by gradually pruning the network to gen-\\nerate sparse features. QSDP (Markov et al., 2023) remove\\nFSDP’s communication bottleneck by performing both gra-\\ndient and weight quantization. ZeRO++ (Wang et al., 2023)\\napplies All-Gather with blockwise weight quantization and\\nan All2All-based gradient quantization to reduce the com-\\nmunication volume when collecting weights and gradients.\\n3. Method\\n3.1. Motivation\\nTensor Parallelism (TP) is now supported in almost all main-\\nstream inference frameworks like TensorRT-LLM (NVIDIA,\\n2023), vLLM (Kwon et al., 2023), SGLang (sgl-project,\\n2024), and LMDeploy (LMDeploy, 2023), becoming the\\nmost adopted scheme in LLM inference. However, TP\\ncomes at a non-negligible cost due to heavy communica-\\ntion, which in the case of larger language models creates an\\nexcessive communication overhead. For example, the com-\\nmunication overhead of LLaMA-3-70B on L40 GPUs easily\\nmeets the bottleneck as the input token length increases,\\nshown by the cost breakdown of LLaMA-3-70B operations\\nin Figure 2. Although high-end training-purpose accelera-\\ntors like NVIDIA A100 where GPUs are connected through\\nNVLink (NVIDIA, 2024e), the communication overhead\\nstill reaches a notable 20%. We can easily conclude that TP\\ncommunication is the inference bottleneck.\\n4 8 16 32 64 128\\nSequence Length\\n0\\n20\\n40\\n60\\n80\\n100Cost Percentage\\n78.4 73.6\\n65.1\\n56.3 55.6 54.8\\n17.9 22.1\\n31.0\\n40.0 41.8 42.8\\n0.6 0.7 0.8 0.9 1.0 1.23.1 3.6 3.0 2.8 1.6 1.2\\nLLaMA-3-70B Latency Breakdown\\nGEMM AllReduce Norm&Act Others\\nFigure 2.Prefill cost breakdown of LLaMA-3-70B operations at\\nvarious sequence lengths. Tested on 4×L40/A100 GPUs (TP=4)\\nwith a batch size of 8.\\nFor communication optimization, one can think of overlap-\\nping the communication with computation to hide overhead\\nbut it requires sophisticated design with scheduling which\\nis harder to implement in any given inference framework,\\nfor which reason NanoFlow (Zhu et al., 2024) invents a\\nnew serving framework to circumvent the difficulty. On\\nthe contrary, compression is a handy option but none of\\nthe above-mentioned methods investigated communication\\nquantization for LLM inference. It could be due to the acti-\\nvation quantization challenge posed at the inference stage,\\nwhich is pointed out by LLM.int8() (Dettmers et al., 2022)\\nand SmoothQuant (Xiao et al., 2024) that activations are\\nharder to quantize because of outliers. The communication\\nquantization method for training doesn’t suffer from this\\nproblem as it only quantizes weights and gradients (Wang\\net al., 2023). Besides, during training, communication quan-\\ntization degradation can be compensated by further learning.\\nWhereas at inference, the quantization loss is nearly irre-\\nversible.\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Flash Communication\\nAt the same time, the existing Ring All-Reduce operation\\nadopted in tensor parallelism remains a bottleneck at infer-\\nence since it is inclined to be constrained by lower band-\\nwidth. Furthermore, to integrate quantization with Ring\\nAll-Reduce, also shown by ZeRO++ (Wang et al., 2023),\\nit requires N times of sequential quantization and dequan-\\ntization in a complete Reduce-Scatter, which worsens the\\nlatency.\\nThe above issues call for a delicate orchestration of the\\nquantization approach and a better All-Reduce scheme.\\n3.2. Flash Communication\\nMotivated by the above, we approach the inference chal-\\nlenges in the distributed scenario with quantization and\\ntopology optimization. In the paper, we specifically exam-\\nine tensor parallelism as it is the most popular paradigm.\\nTake tensor parallelism in LLaMA-3 (Dubey et al., 2024) as\\nan example, the tensors of QKV projections are first sliced\\ncolumn-wisely and then the output projection row-wisely.\\nAfter that, an All-Reduce operation is required to collect\\nactivations on each device, shown in Fig. 3. Similarly in\\nthe feedforward network, the gate projection and the up\\nprojection are split by column and the down projection\\nby row, then another All-Reduce is needed to sum up the\\nactivations from both devices. To reduce the communication\\nvolume, we are left to compress the activation from the\\noutput projection and the down projection.\\nRMSNorm\\nGate \\nw/ Swish\\nUp\\nDown\\nRMSNorm\\nW1\\nW2\\nSelfAttentionQKV Out\\n+\\nFlash \\nAll-Reduce\\nW1\\nW2\\n+×\\nFlash \\nAll-Reduce\\nW1\\nW2\\nW1\\nW2\\nW1\\nW2\\nFigure 3.Tensor parallelism for a LLaMA-3 transformer block.\\nOur Flash All-Reduce is applied to speed up communication.\\n3.2.1. Q UANTIZATION CHALLENGE\\nTo obtain an optimal trade-off between accuracy and latency,\\nwe choose to apply low-bit quantization. From Fig. 4, we\\nobserve that fine granularity is necessary since per-token\\nquantization at larger block sizes suffers from performance\\ncollapse in terms of C4 perplexity, albeit the asymmetric\\nversion is relatively better.\\nHowever, we discover that it is non-trivial to apply low-bit\\n8192 4096 2048 1024 512 256 128\\nBlock Size\\n102\\n104\\n106\\nC4 Perplexity\\nper-token \\nPer-token Fine-granularity\\nINT4Sym\\nINT4Asym\\nFigure 4.Activation quantization with various block sizes of\\nLLaMA-3-8B on C4. Starting from 4096 (the length of hidden\\ndimension), the granularity becomes finer till 128.\\nactivation quantization in this scenario. To investigate the\\nquantization sensitivity, we calculate the layerwise mean\\nsquared errors (MSE) before and after activation quantiza-\\ntion on LLaMA-3-8B, as depicted in Figure 5. We find that\\nthe down projection dproj is much harder to quantize than\\nthe output projection oproj , as the former MSEs are quite\\ndistinct even on a logarithmic scale. This phenomenon is\\nalso discovered by (Li et al., 2023; Ashkboos et al., 2023;\\nYu et al., 2024).\\n0 10 20 30\\nLayer\\n10 6\\n10 5\\n10 4\\n10 3\\n10 2\\nMSE\\ndproj\\noproj\\n0 10 20 30\\nLayer\\n10 7\\n10 6\\n10 5\\n10 4\\n10 3\\nMSE\\nRS (INT4)\\nAG (INT8)\\nAG (INT4)\\nFigure 5.Left: Comparison of oproj and dproj All-Reduce Quan-\\ntization MSE. Right: MSE of quantization before Reduce-Scatter\\n(RS) vs. All-Gather (AG).\\nBesides, an All-Reduce comprises a pair of Reduce-Scatter\\nand All-Gather operations, where the quantization corre-\\nsponding to each operation exhibits different levels of diffi-\\nculty, see Fig. 5 right. This is as expected since the quantiza-\\ntion before Reduce-Scatter only introduces rounding errors\\nwhile in the case of All-Gather, it includes both rounding\\nand accumulated errors. Alternatively, we could use a higher\\nprecision for the quantization before All-Gather to improve\\naccuracy.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='Flash Communication\\nGPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nA0 A1 A2 A3\\nB0 B1 B2 B3\\nC0 C1 C2 C3\\nD0 D1 D2 D3\\nQ\\nA0 B0 C0 D0\\nA1 B1 C1 D1\\nA2 B2 C2 D2\\nA3 B3 C3 D3\\nDQ\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nDQ\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nQ\\nAll2All Reduce \\nSum\\nAll \\nGather\\nFigure 6.Flash communication’s two-step All-Reduce. The communication volume is quantized and dequantized only twice.\\n3.2.2. A LGORITHM\\nConsidering the above issues, we design a two-step quanti-\\nzation strategy to replace vanilla Ring All-Reduce, as por-\\ntrayed in Fig. 6. Its integration with tensor parallelism is\\nshown in Fig.3. We name our overall strategy as two-step\\nAll-Reduce.\\nFig. 6 illustrates how Flash Communication works. First,\\nwe divide the computation volume (activation) on each GPU\\nby the number of ranks. After fine-grained quantization on\\nactivation, we perform All2All communication so that each\\ndevice receives its computation load for reduction. After\\non-device reduction, the sum is again quantized to speed up\\nthe transmission. We then perform All-Gather to collect all\\nresults and dequantization to recover float values on each\\ndevice. This two-step workflow is also formulated by Alg. 1.\\nAlgorithm 1 Flash All-Reduce\\nInput: Communication volume M, world size N, chunk\\nsize C, quantization bit-width b, group size g\\nOutput: Reduced sum Sdq\\nDivide M into T = ⌈M/C⌉ chunks.\\nfor 1 ≤ i ≤ T do\\n// Quantize volume to obtain zeros and scales\\nMq\\ni , zi, si = FinegrainedQuantize(Mi, b, g);\\n// Each device sends and receives volume from others\\nAll2All(Mq\\ni , zi, si, N);\\nfor 1 ≤ j ≤ N do\\nMdq\\nij = Dequantize(Mq\\nij , zij , ij );\\nend for\\nSi = ReduceSum(Mdq\\ni0 , Mdq\\ni1 , ··· , Mdq\\niN );\\nSq\\ni , zs\\ni , ss\\ni = FinegrainedQuantize(Si, b, g);\\n// Each device collects the reduced sum from others\\nAll-Gather(Sq\\ni , zs\\ni , ss\\ni , N);\\nfor 1 ≤ j ≤ N do\\nSdq\\nij = Dequantize(Sq\\ni , zs\\ni , ss\\ni );\\nend for\\nend for\\nTable 1.Comparison of Ring All-Reduce vs. Flash All-Reduce\\nMETHOD RING ALL-REDUCE FLASH ALL-REDUCE\\nTOTAL VOLUME 2M(N − 1)/N 2M(N − 1)/N\\nREDUCE STEP N − 1 1\\nREDUCE -SCATTER M/N M (N − 1)/N\\nGATHER STEP N − 1 1\\nALL-GATHER M/N M (N − 1)/N\\nQDQ S TEP N 2\\n3.2.3. K ERNEL DESIGN\\nFor efficiency, we implement a fused Flash All-Reduce\\nkernel to encompass all the above collective communication\\noperations and quantization steps. Compared with Ring\\nAll-Reduce in Table 1, Flash All-Reduce cuts quantization-\\ndequantization steps from N to 2, and Reduce/Gather steps\\nfrom N −1 to 1. Although the size of total volumes remains\\nthe same, each of our volumes is quantized to lower bits,\\nsubstantially reducing the amount of data to transmit. We\\nsummarize three key aspects in designing our kernel below.\\nFast Fine-grained Quantization The total communication\\nvolume M for each rank is divided into T chunks for trans-\\nmission. Given a chunk size C, we draw how GPU threads\\nare organized in parallel to process the chunk information\\nin Fig. 7. A chunk is split into N blocks and each block\\ncorresponds to 32 warps, where each warp is a collection of\\n32 threads and each thread can process eight FP16 elements.\\nTake our asymmetric quantization with a group size of 128\\nas an example, we perform quantization on each group of\\n128 elements using 16 threads. Specifically, we leverage\\nthe CUDA API function shfl xor sync to iteratively\\nexchange information among these warp threads to achieve\\nmax/min reduction efficiently.\\nFast Communication. Instead of calling All2All primitive,\\nwe utilize GPU peer direct memory access from CUDA Run-\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Flash Communication\\nTotal 1024 threads\\nQuantized \\nVolume\\nscales & zeros\\nWarp1Warp0 …… Warp31Warp30\\nWarp0 Warp1 Warp30 Warp31\\n……\\n……\\n……\\n……\\n……\\n……Warp0 Warp1 Warp30 Warp31\\nBlock 0\\nBlock 1\\nBlock N\\n8192 x INT8 128 x FP16\\nFigure 7.Thread mapping of fast fine-grained quantization.\\ntime API (NVIDIA Corporation, 2024) to transmit quan-\\ntized volume, where we can directly fetch data from differ-\\nent ranks, substantially boosting the communication speed.\\nFast Dequantization. Once quantization volumes are re-\\nceived, we have to dequantize them into FP16 for sum re-\\nduction. As na ¨ıve INT4 to FP16 conversion incurs over-\\nhead, we utilize the dequantization layout in (Kim et al.,\\n2022). To coordinate its ordering online, we also employ\\nfast INT4 packing (LMDeploy, 2023), as illustrated in\\nFig. 8. Given that two 32-bit unsigned integers U0 and\\nU1 holding 4 INT4-quantized activations (each stored in\\nthe lower 4 bits out of 8 bits) to transmit, we first perform\\nthe right shift by 12 bits, and then apply bitwise OR to\\nitself. Later we select the target bits from these two inte-\\ngers with CUDA Math API byte perm. In this way,\\nwe can pack 8 4-bit integers in a convenient order to de-\\nquantize. Next we apply lop3.b321 to perform logical\\noperation (0xF0 & 0xCC)|0xAA on the packed vari-\\nable with mask 0x000F000F and 0x64006400, then we\\nsubtract it with 0x64006400, which effectively represents\\nW1 and W0 in FP16. The dequantization can be performed\\niteratively by varying the masks for the rest INT4 integers.\\nfW1 fW0\\nW7 W6 W5 W4 W3 W2 W1 W0\\nlop3.b32 ( , 0x000F000F , 0x64006400, (0xF0 & 0xCC) | 0xAA ) W5W7 W1W3 W4W6 W0W2\\n, 0x64006400)sub.f16x2 ( 0x40x6 W10x0 0x40x6 W00x0\\nW7 W6 W5W7 W4W6 W3 W2 W1W3 W0W2__byte_perm ( , 0x5140),\\nU0 |= U0 >> 12 U1 |= U1 >> 12\\nW7 W6 W3 W2\\n}\\nFigure 8.Fast INT4 packing and dequantization. For simplicity,\\nonly the dequantization of W1 and W0 is shown. An empty square\\nmeans zero.\\n1https://docs.nvidia.com/cuda/parallel-\\nthread-execution/#logic-and-shift-\\ninstructions-lop3\\nINT6 Quantization. Given that quantization prior to All-\\nGather leads to greater loss, as illustrated in Figure 5 (left),\\nwe opt for an INT8 bit-width for All-Gather operations and\\nmaintain INT4 for ReduceSum, effectively creating an INT6\\nsolution. As later shown in Table 3, the INT6 configuration\\nstrikes a commendable balance between performance and\\ncommunication efficiency.\\n4. Experiments\\n4.1. Setup\\nUnless otherwise noted, we use an input token length of\\n1024 and an output token length of 64 for the inference mea-\\nsurement. Latencies are tested on NVIDIA L40 and A100\\nSXM GPUs. The baseline uses FP16 for communication.\\n4.2. Accuracy Comparison\\nFP16 Weights. We evaluate the accuracy of LLaMA-2 and\\nLLaMA-3 models on PIQA (Bisk et al., 2020), ARCC and\\nARCE (Clark et al., 2018), HellaSwag (Zellers et al., 2019),\\nWinoGrande (Sakaguchi et al., 2021) in various commu-\\nnication quantization bit widths, shown in Table 2. In all\\ncases, asymmetric INT8 quantization obtains the best ac-\\ncuracies. Asymmetric INT4 is also better than symmetric\\nINT4. C4 (Raffel et al., 2020) and WikiText (Merity et al.,\\n2016) results are shown in Table 6 of Appendix B.1.\\nTable 2.Accuracy of LLaMA models with various communication\\nquantization strategies. All model weights are kept in FP16 pre-\\ncision. Prec: Communication precision. All INT quantization is\\nasymmetrical. HS: HellaSwag, WG: WinoGrande.\\nMODEL PREC PIQA ARC C ARCE HS WG A VG\\n2-7B FP16 79.11 46.33 74.58 76.01 69.30 64.83\\nINT8 79.11 45.99 74.75 76.10 69.06 64.77\\nINT6 78.78 45.99 74.79 75.75 68.75 64.68\\nINT4 78.02 45.82 74.49 75.63 67.25 64.23\\n2-13B FP16 80.52 49.15 77.48 79.39 72.14 67.83\\nINT8 80.69 49.06 77.61 79.34 71.59 67.78\\nINT6 79.98 49.32 77.06 79.17 71.11 67.11\\nINT4 79.60 48.21 76.89 78.92 71.90 67.04\\n2-70B FP16 82.70 57.34 81.02 83.80 77.98 72.43\\nINT8 82.75 57.68 80.93 83.80 77.98 72.47\\nINT6 82.48 57.00 80.85 83.77 76.87 72.23\\nINT4 82.92 57.25 80.43 83.60 77.27 71.99\\n3-8B FP16 80.79 53.41 77.69 79.16 72.77 68.64\\nINT8 80.58 52.47 77.40 79.09 73.09 68.45\\nINT6 79.98 51.37 77.65 78.73 73.16 68.06\\nINT4 80.09 51.02 75.84 78.11 70.48 66.92\\n3-70B FP16 84.55 64.33 85.86 84.89 80.35 75.25\\nINT8 84.55 63.91 85.82 84.90 80.82 75.27\\nINT6 84.11 61.69 85.44 84.87 80.35 74.76\\nINT4 83.13 61.35 83.33 84.69 78.93 73.82\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='Flash Communication\\n8 16 32 64\\nBatch Size\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0TTFT Speed-up Ratio\\n1.48\\n1.72\\n2.06\\n1.45\\n1.68\\n1.99\\n1.39\\n1.58\\n1.82\\n1.37\\n1.55\\n1.78\\nLLaMA-3-8B (INT8) on L40, TP=4\\nFP16\\nINT8\\nINT6\\nINT4\\n8 16 32 64\\nBatch Size\\n0.6\\n0.8\\n1.0\\n1.2TTFT Speed-up Ratio\\n1.10\\n1.16\\n1.19\\n1.10\\n1.16\\n1.19\\n1.10\\n1.15\\n1.19\\n1.09\\n1.15\\n1.18\\nLLaMA-3-70B (INT8) on A100, TP=8\\nFP16\\nINT8\\nINT6\\nINT4\\nFigure 9.TTFT speed-up ratio of 8-bit LLaMA-3-8B under various communication quantization bit widths on L40 with TP=4 (left), and\\n8-bit LLaMA-3-70B on A100 with TP=8 (right).\\nINT8 Weights. As model weights are quantized, the im-\\npact of communication overhead becomes more pronounced.\\nThis observation motivates us to explore communication\\nquantization in this context. We begin by quantizing the\\nweights of the LLaMA model using Smoothquant (Xiao\\net al., 2024). Subsequently, we implement fine-grained com-\\nmunication quantization to assess its impact on performance\\nas detailed in Table 3. Results on C4 (Raffel et al., 2020),\\nWikiText-2 (Merity et al., 2016) is shown in Appendix B.1.\\nTable 3.Accuracy of LLaMA models with various communication\\nquantization strategies (denoted as ‘Comm Prec’). All model\\nweights are quantized into INT8 precision with SmoothQuant\\n(α = 0.85 except 0.9 for LLaMA2-70B). Prec: Communication\\nprecision. All INT quantization is asymmetrical.\\nMODEL PREC PIQA ARC C ARC E HS WG A VG\\n2-7B FP16 79.00 46.16 74.24 75.89 68.75 68.81\\nINT8 79.27 45.65 74.37 75.89 68.51 68.74\\nINT6 78.56 45.39 74.75 75.75 68.67 68.62\\nINT4 77.53 45.31 74.20 75.51 68.59 68.23\\n2-13B FP16 80.25 49.49 77.27 79.37 71.59 71.59\\nINT8 80.41 49.32 77.53 79.21 72.22 71.74\\nINT6 80.09 49.40 77.02 79.14 71.74 71.48\\nINT4 79.11 49.06 76.30 78.89 71.27 70.93\\n2-70B FP16 83.08 57.94 80.98 83.54 77.66 76.64\\nINT8 83.08 57.76 80.68 83.55 78.14 76.64\\nINT6 82.81 57.94 80.85 83.78 76.80 76.44\\nINT4 82.92 56.83 80.35 83.41 78.14 76.33\\n3-8B FP16 80.36 51.96 77.69 78.71 73.48 72.44\\nINT8 80.09 52.90 77.57 78.79 73.09 72.49\\nINT6 80.03 52.56 79.12 78.38 71.82 72.38\\nINT4 78.94 50.17 77.10 77.87 70.56 70.93\\n3-70B FP16 84.44 63.99 85.56 84.55 79.72 79.65\\nINT8 83.84 63.48 85.56 84.67 79.79 79.47\\nINT6 83.57 61.26 83.67 84.66 80.58 78.75\\nINT4 82.70 61.60 83.29 84.51 77.82 77.98\\n4.3. Latency and Throughput Performance\\nFig. 9 illustrates weight-quantized LLaMA-3-8B and\\nLLaMA-3-70B’s TTFT comparison with and without Flash\\ncommunication. The lowest quantization bit yields the most\\ngain, i.e. 2.06× and 1.19× on L40 and A100 respectively.\\nMore measurements are listed in Appendix C.\\n5. Ablation Study\\n5.1. Integer vs. Low-bit Float\\nTable. 4 shows the difference between INTx and FPx com-\\nmunication quantization. In general, INT8 performs compa-\\nrably with FP8, while the asymmetric version of INT8 is the\\nbest among all. FP6 is a mixed version of FP8 (Micikevicius\\net al., 2022) and FP4 (Rouhani et al., 2023), which is a fair\\ncomparison with similarly mixed INT6.\\nTable 4.LLaMA models’ C4 Perplexity with INTx vs FPx quanti-\\nzation with a group size of 128. Weights are in FP16.\\nPREC 2-7B 2-13B 2-70B 3-8B 3-70B\\nFP8 Sym 6.98 6.47 5.52 8.90 6.75\\nINT8 Sym 6.98 6.47 5.52 8.90 6.74\\nINT8 Asym 6.98 6.47 5.52 8.89 6.74\\nFP6 Sym 7.09 6.52 5.56 9.22 6.87\\nINT6 Sym 7.15 6.57 5.59 9.48 6.94\\nINT6 Asym 7.08 6.51 5.55 9.20 6.86\\nFP4 Sym 7.24 6.60 5.61 9.72 7.07\\nINT4 Asym 7.50 6.71 5.69 10.51 7.30\\nINT4 Asym 7.21 6.58 5.60 9.68 7.04\\n5.2. Na¨ıve vs. Rotation-based Quantization\\nAs previously shown in Fig. 4, the C4 perplexity of\\ncoarse quantization suffers performance collapse, while fine-\\ngrained quantization gradually resolves the problem as the\\ngroup size increases. We investigate whether the Hadamard\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Flash Communication\\ntransform popularized by QuaRot (Ashkboos et al., 2024)\\ncould alleviate the issue in the coarse setting in Table 5. It\\nturns out that Hadamard transform with coarse quantization\\nreadily performs well, however, it loses its advantage in\\nfiner granularity cases as compared with na¨ıve asymmetric\\nquantization. Besides, it doesn’t exhibit gains on FP8.\\nTable 5.LLaMA models’ C4 perplexity with communication quan-\\ntization (na¨ıve vs. rotation) at various granularity levels and pre-\\ncision formats. Asymmetric quantization is used for INT4 and\\nsymmetric for FP8. HT: Hadamard Transform\\nMODELS GROUP INT4 W/ HT FP8 W/ HT\\n3-8B 8192 2363367.8 10.61 8.91 8.96\\n2048 284.56 10.11 8.91 8.96\\n128 9.68 9.67 8.90 8.94\\n3-70B 8192 7417.79 7.86 6.75 6.81\\n1024 10.47 7.76 6.75 6.81\\n128 7.04 7.64 6.75 6.82\\n2-7B 8192 47601520 7.86 7.01 7.01\\n1024 8.78 7.35 6.98 7.01\\n128 7.21 7.24 6.98 7.01\\n2-13B 8192 306.39 6.80 6.47 6.49\\n1024 7.43 6.68 6.47 6.49\\n128 6.58 6.62 6.47 6.49\\n2-70B 8192 49.96 5.71 5.52 5.54\\n1024 6.17 5.67 5.52 5.54\\n128 5.60 5.64 5.52 5.54\\n5.3. Flash All-Reduce vs. Ring All-Reduce\\n64MB 128MB 256MB 512MB\\n1GB\\nCommunication Volume\\n0\\n20000\\n40000\\n60000Latency ( s)\\n1.74x\\n2.27x\\n3.18x\\nAll-Reduce Latency on L40, TP=4\\nNCCL Ring AR\\nFlash AR INT8\\nFlash AR INT6\\nFlash AR INT4\\nFigure 10.Flash Communication’s All-Reduce (Flash AR) Perfor-\\nmance compared with NCCL’s ring version. NCCL’s latency is\\ntested with nccl-test (NVIDIA, 2016-2024).\\nAssembling several boosting techniques, the speed of our\\nFlash All-Reduce kernel surpasses that of Ring All-Reduce\\nby a large margin. Fig. 10 exhibits the latency measurement\\ngiven a certain amount of communication volume. When the\\ncommunication volume becomes obvious (e.g. larger than\\n64MB), our quantized All-Reduce is crucial to reduce the\\ncost, where the INT4 version brings at most 3.18× kernel\\nlatency reduction. Noticeably, the mixed precision version\\nINT6 obtains a good trade-off between INT8 and INT4.\\nWe further show that the number of streaming processors\\n(SMs) matters in Fig. 11. When the communication volume\\nis of small size, a smaller number of SMs is beneficial as\\nless kernel launch and inter-block synchronization overhead\\nis produced. When the volume gets larger, more SMs are\\nrequired for calculation. A configuration of 48 SMs strikes\\na better balance between communication and computation.\\n16KB 32KB 64KB 128KB 256KB 512KB 1MB\\n20\\n40Latency ( s)\\nAll-Reduce Latency on L40, TP=4\\nSM=4\\nSM=48\\nSM=128\\n2MB 4MB 8MB 16MB32MB64MB128MB256MB512MB\\n1GB\\nCommunication Volume\\n0\\n20000Latency ( s)\\n SM=4\\nSM=48\\nSM=128\\nFigure 11.The number of SMs affects the communication latency\\nat different sizes of communication volume. SM=48 is similar to\\nSM=128 in larger volumes.\\n6. Conclusion\\nOur work presents a novel technique to reduce the commu-\\nnication volume associated with tensor parallelism while\\nmaintaining accuracy. Our key contributions include a com-\\nprehensive analysis that reveals the communication bottle-\\nneck in Large Language Model (LLM) inference, the design\\nof a fast communication mechanism known as Flash Com-\\nmunication, and the demonstration of its implementation,\\nwhich has been shown to achieve up to a 2× TTFT reduction.\\nFlash Communication employs fine-grained quantization on\\nactivations and a two-step All-Reduce strategy to decrease\\ncommunication volumes significantly. We have conducted\\nextensive experiments on NVIDIA L40 and A100 GPUs\\nacross various configurations and with several state-of-the-\\nart LLMs, which have consistently demonstrated the effec-\\ntiveness of our approach. These findings address a critical\\nchallenge in parallel computing and pave the way for more\\nefficient and scalable LLM inference.\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Flash Communication\\nReferences\\nAji, A. F. and Heafield, K. Sparse communication for\\ndistributed gradient descent. In Proceedings of the\\n2017 Conference on Empirical Methods in Natural Lan-\\nguage Processing. Association for Computational Lin-\\nguistics, 2017. doi: 10 .18653/v1/d17-1045. URL\\nhttp://dx.doi.org/10.18653/v1/D17-1045.\\nAshkboos, S., Markov, I., Frantar, E., Zhong, T., Wang, X.,\\nRen, J., Hoefler, T., and Alistarh, D. Towards end-to-\\nend 4-bit inference on generative large language models.\\narXiv preprint arXiv:2310.09259, 2023.\\nAshkboos, S., Mohtashami, A., Croci, M. L., Li, B.,\\nCameron, P., Jaggi, M., Alistarh, D., Hoefler, T., and\\nHensman, J. Quarot: Outlier-free 4-bit inference in ro-\\ntated llms. arXiv preprint arXiv:2404.00456, 2024.\\nBaidu Research. baidu-allreduce: A C++ library demonstrat-\\ning ring allreduce and ring allgather techniques. GitHub\\nrepository, 2024. URL https://github.com/\\nbaidu-research/baidu-allreduce.\\nBen-Nun, T. and Hoefler, T. Demystifying parallel and dis-\\ntributed deep learning: An in-depth concurrency analysis.\\nACM Computing Surveys (CSUR), 52(4):1–43, 2019.\\nBisk, Y ., Zellers, R., Gao, J., Choi, Y ., et al. Piqa: Reasoning\\nabout physical commonsense in natural language. In Pro-\\nceedings of the AAAI conference on artificial intelligence,\\nvolume 34, pp. 7432–7439, 2020.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\n2020.\\nCai, W., Jiang, J., Qin, L., Cui, J., Kim, S., and Huang,\\nJ. Shortcut-connected expert parallelism for accel-\\nerating mixture-of-experts, 2024. URL https://\\narxiv.org/abs/2404.05019.\\nChang, L.-W., Bao, W., Hou, Q., Jiang, C., Zheng, N.,\\nZhong, Y ., Zhang, X., Song, Z., Yao, C., Jiang, Z., Lin,\\nH., Jin, X., and Liu, X. Flux: Fast software-based com-\\nmunication overlap on gpus through kernel fusion, 2024.\\nURL https://arxiv.org/abs/2406.06858.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\nquestion answering? try arc, the ai2 reasoning challenge.\\narXiv preprint arXiv:1803.05457, 2018.\\nDao, T. Flashattention-2: Faster attention with bet-\\nter parallelism and work partitioning. arXiv preprint\\narXiv:2307.08691, 2023.\\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M.,\\nMao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K.,\\net al. Large scale distributed deep networks. Advances in\\nneural information processing systems, 25, 2012.\\nDettmers, T., Lewis, M., Belkada, Y ., and Zettlemoyer,\\nL. Llm.int8(): 8-bit matrix multiplication for transform-\\ners at scale, 2022. URL https://arxiv.org/abs/\\n2208.07339.\\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\\nA., et al. The llama 3 herd of models. arXiv preprint\\narXiv:2407.21783, 2024.\\nFacebook. Gloo: Collective communications library with\\nvarious primitives for multi-machine training. https:\\n//github.com/facebookincubator/gloo,\\n2024. Accessed: 2024-11-23.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transformers:\\nScaling to trillion parameter models with simple and ef-\\nficient sparsity. Journal of Machine Learning Research,\\n23(120):1–39, 2022.\\nHarlap, A., Narayanan, D., Phanishayee, A., Seshadri, V .,\\nDevanur, N., Ganger, G., and Gibbons, P. Pipedream:\\nFast and efficient pipeline parallel dnn training. arXiv\\npreprint arXiv:1806.03377, 2018.\\nHassani, A., Isaev, M., McDonald, N., Ren, J.,\\nThakkar, V ., Wu, H., and Shi, H. Distributed\\ngemm, 2024. URL https://blog.shi-labs.com/\\ndistributed-gemm-88be6a481e2b. Accessed:\\n2024-12-04.\\nHidayetoglu, M., de Gonzalo, S. G., Slaughter, E., Surana,\\nP., Hwu, W.-m., Gropp, W., and Aiken, A. Hiccl: A hier-\\narchical collective communication library. arXiv preprint\\narXiv:2408.05962, 2024.\\nHuang, Y ., Cheng, Y ., Bapna, A., Firat, O., Chen, D., Chen,\\nM., Lee, H., Ngiam, J., Le, Q. V ., Wu, Y ., et al. Gpipe:\\nEfficient training of giant neural networks using pipeline\\nparallelism. Advances in neural information processing\\nsystems, 32, 2019.\\nIEEE. IEEE Standard for Floating-Point Arithmetic.\\nTechnical Report IEEE 754-1985, Institute of Elec-\\ntrical and Electronics Engineers, New York, NY,\\n1985. URL https://standards.ieee.org/\\nieee/754/6210/.\\nJacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song,\\nS. L., Rajbhandari, S., and He, Y . Deepspeed ulysses:\\nSystem optimizations for enabling training of extreme\\nlong sequence transformer models. arXiv preprint\\narXiv:2309.14509, 2023.\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Flash Communication\\nJia, X., Song, S., He, W., Wang, Y ., Rong, H., Zhou, F.,\\nXie, L., Guo, Z., Yang, Y ., Yu, L., et al. Highly scal-\\nable deep learning training system with mixed-precision:\\nTraining imagenet in four minutes. arXiv preprint\\narXiv:1807.11205, 2018.\\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,\\nB., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna,\\nE. B., Bressand, F., et al. Mixtral of experts. arXiv\\npreprint arXiv:2401.04088, 2024.\\nKim, Y . J., Henry, R., Fahim, R., and Awadalla, H. H.\\nWho says elephants can’t run: Bringing large scale\\nmoe models into cloud scale production. arXiv preprint\\narXiv:2211.10017, 2022.\\nKorthikanti, V . A., Casper, J., Lym, S., McAfee, L., Ander-\\nsch, M., Shoeybi, M., and Catanzaro, B. Reducing acti-\\nvation recomputation in large transformer models. Pro-\\nceedings of Machine Learning and Systems, 5:341–353,\\n2023.\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y ., Zheng, L., Yu,\\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\\nmemory management for large language model serving\\nwith pagedattention. In Proceedings of the ACM SIGOPS\\n29th Symposium on Operating Systems Principles, 2023.\\nLi, Q., Zhang, Y ., Li, L., Yao, P., Zhang, B., Chu, X., Sun,\\nY ., Du, L., and Xie, Y . Fptq: Fine-grained post-training\\nquantization for large language models. arXiv preprint\\narXiv:2308.15987, 2023.\\nLiu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C.,\\nDengr, C., Ruan, C., Dai, D., Guo, D., et al. Deepseek-v2:\\nA strong, economical, and efficient mixture-of-experts\\nlanguage model. arXiv preprint arXiv:2405.04434, 2024.\\nLiu, H. and Abbeel, P. Blockwise parallel transformers for\\nlarge context models. In Oh, A., Naumann, T., Globerson,\\nA., Saenko, K., Hardt, M., and Levine, S. (eds.),Advances\\nin Neural Information Processing Systems , volume 36,\\npp. 8828–8844. Curran Associates, Inc., 2023.\\nLiu, H., Zaharia, M., and Abbeel, P. Ring attention with\\nblockwise transformers for near-infinite context. arXiv\\npreprint arXiv:2310.01889, 2023.\\nLMDeploy. Lmdeploy: A toolkit for compressing, de-\\nploying, and serving llm. https://github.com/\\nInternLM/lmdeploy, 2023.\\nMarkov, I., Vladu, A., Guo, Q., and Alistarh, D. Quantized\\ndistributed training of large models with convergence\\nguarantees, 2023. URL https://arxiv.org/abs/\\n2302.02390.\\nMerity, S., Xiong, C., Bradbury, J., and Socher, R.\\nPointer sentinel mixture models. arXiv preprint\\narXiv:1609.07843, 2016.\\nMicikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey,\\nP., Grisenthwaite, R., Ha, S., Heinecke, A., Judd, P.,\\nKamalu, J., et al. Fp8 formats for deep learning. arXiv\\npreprint arXiv:2209.05433, 2022.\\nMicrosoft. DeepSpeed: Deep learning optimization li-\\nbrary. GitHub repository, 2024. URL https://\\ngithub.com/microsoft/DeepSpeed.\\nMicrosoft. Microsoft collective communication library\\n(msccl). https://github.com/microsoft/\\nmsccl, 2024. Accessed: 2024-11-23.\\nMikami, H., Suganuma, H., Tanaka, Y ., Kageyama, Y ., et al.\\nMassively distributed sgd: Imagenet/resnet-50 training in\\na flash. arXiv preprint arXiv:1811.05233, 2018.\\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Pat-\\nwary, M., Korthikanti, V ., Vainbrand, D., Kashinkunti, P.,\\nBernauer, J., Catanzaro, B., et al. Efficient large-scale\\nlanguage model training on gpu clusters using megatron-\\nlm. In Proceedings of the International Conference for\\nHigh Performance Computing, Networking, Storage and\\nAnalysis, pp. 1–15, 2021.\\nNie, X., Zhao, P., Miao, X., Zhao, T., and Cui, B. Hetumoe:\\nAn efficient trillion-scale mixture-of-expert distributed\\ntraining system, 2022. URL https://arxiv.org/\\nabs/2203.14685.\\nNVIDIA. NCCL Tests. https://github.com/\\nNVIDIA/nccl-tests, 2016-2024. Accessed: 2024-\\n12-06.\\nNVIDIA. Massively scale your deep learning training\\nwith nccl 2.4. https://developer.nvidia.com/\\nblog/massively-scale-deep-learning-\\ntraining-nccl-2-4/ , 2019. Accessed: 2024-11-\\n23.\\nNVIDIA. NVIDIA Scalable Hierarchical Aggrega-\\ntion and Reduction Protocol (SHARP) v2.6.1 Release\\nNotes. Technical Report Revision 2.6.1, NVIDIA, 2023.\\nURL https://docs.nvidia.com/networking/\\ndisplay/sharpv261/release+notes. Last up-\\ndated on May 23, 2023.\\nNVIDIA. TensorRT-LLM. GitHub repository,\\n2023. URL https://github.com/NVIDIA/\\nTensorRT-LLM.\\nNVIDIA. Megatron-LM: Ongoing research training trans-\\nformer models at scale. GitHub repository, 2024a. URL\\nhttps://github.com/NVIDIA/Megatron-LM.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='Flash Communication\\nNVIDIA. NVIDIA Nsight Systems. Web Page,\\n2024b. URL https://developer.nvidia.com/\\nnsight-systems.\\nNVIDIA. Collective operations. https:\\n//docs.nvidia.com/deeplearning/\\nnccl/user-guide/docs/usage/\\ncollectives.html, 2024a. Accessed: 2024-\\n11-23.\\nNVIDIA. Context parallelism overview. https:\\n//docs.nvidia.com/megatron-core/\\ndeveloper-guide/latest/api-guide/\\ncontext parallel.html, 2024b. Accessed:\\n2024-11-23.\\nNVIDIA. Nvidia l40: Delivering unprecedented visual\\ncomputing performance for the data center. https:\\n//images.nvidia.cn/content/Solutions/\\ndata-center/vgpu-L40-datasheet .pdf,\\n2024c. Accessed: 2024-11-23.\\nNVIDIA. Optimized primitives for collective multi-gpu\\ncommunication. https://github.com/NVIDIA/\\nnccl, 2024d. Accessed: 2024-11-23.\\nNVIDIA. Nvlink & nvswitch for advanced multi-gpu com-\\nmunication. https://www.nvidia.com/en-us/\\ndata-center/nvlink/, 2024e. Accessed: 2024-\\n11-26.\\nNVIDIA. Cuda templates for linear algebra subrou-\\ntines, 2024f. URL https://github.com/NVIDIA/\\ncutlass. Accessed: 2024-12-04.\\nNVIDIA Corporation. Peer Device Memory Ac-\\ncess. Technical report, NVIDIA, 2024. URL\\nhttps://docs.nvidia.com/cuda/cuda-\\nruntime-api/group CUDART PEER.html.\\nAccessed: 2024-12-04.\\nQin, M., Sun, C., Hofmann, J., and Vucinic, D. Disco:\\nDistributed inference with sparse communications. In\\nProceedings of the IEEE/CVF Winter Conference on Ap-\\nplications of Computer Vision, pp. 2432–2440, 2024.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y ., Li, W., and Liu, P. Exploring\\nthe limits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research, 21\\n(140):1–67, 2020.\\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y . Zero:\\nMemory optimizations toward training trillion parameter\\nmodels. In SC20: International Conference for High Per-\\nformance Computing, Networking, Storage and Analysis,\\npp. 1–16. IEEE, 2020.\\nRajbhandari, S., Li, C., Yao, Z., Zhang, M., Aminabadi,\\nR. Y ., Awan, A. A., Rasley, J., and He, Y . Deepspeed-moe:\\nAdvancing mixture-of-experts inference and training to\\npower next-generation ai scale. In International con-\\nference on machine learning, pp. 18332–18346. PMLR,\\n2022.\\nRouhani, B. D., Zhao, R., More, A., Hall, M., Khodamoradi,\\nA., Deng, S., Choudhary, D., Cornea, M., Dellinger, E.,\\nDenolf, K., et al. Microscaling data formats for deep\\nlearning. arXiv preprint arXiv:2310.10537, 2023.\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y .\\nWinogrande: An adversarial winograd schema challenge\\nat scale. Communications of the ACM , 64(9):99–106,\\n2021.\\nSanders, P., Speck, J., and Tr¨aff, J. L. Two-tree algorithms\\nfor full bandwidth broadcast, reduction and scan. Parallel\\nComputing, 35(12):581–594, 2009.\\nSergeev, A. and Balso, M. D. Horovod: fast and easy\\ndistributed deep learning in TensorFlow. arXiv preprint\\narXiv:1802.05799, 2018.\\nsgl-project. SGLang: A Fast Serving Framework for Large\\nLanguage Models and Vision Language Models. GitHub\\nrepository, 2024. URL https://github.com/sgl-\\nproject/sglang. Accessed: 2024-12-01.\\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\\nJ., and Catanzaro, B. Megatron-lm: Training multi-\\nbillion parameter language models using model paral-\\nlelism. arXiv preprint arXiv:1909.08053, 2019.\\nTeam, Q. Qwen1.5-moe: Matching 7b model perfor-\\nmance with 1/3 activated parameters”, February 2024.\\nURL https://qwenlm.github.io/blog/qwen-\\nmoe/.\\nWang, G., Qin, H., Jacobs, S. A., Holmes, C., Rajbhandari,\\nS., Ruwase, O., Yan, F., Yang, L., and He, Y . Zero++:\\nExtremely efficient collective communication for giant\\nmodel training. arXiv preprint arXiv:2306.10209, 2023.\\nWang, G., Zhang, C., Shen, Z., Li, A., and Ruwase, O.\\nDomino: Eliminating communication in llm training\\nvia generic tensor slicing and overlapping, 2024. URL\\nhttps://arxiv.org/abs/2409.15241.\\nXiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han,\\nS. Smoothquant: Accurate and efficient post-training\\nquantization for large language models, 2024. URL\\nhttps://arxiv.org/abs/2211.10438.\\nYing, C., Kumar, S., Chen, D., Wang, T., and Cheng, Y . Im-\\nage classification at supercomputer scale. arXiv preprint\\narXiv:1811.06992, 2018.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='Flash Communication\\nYu, M., Wang, D., Shan, Q., and Wan, A. The su-\\nper weight in large language models. arXiv preprint\\narXiv:2411.07191, 2024.\\nZellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi,\\nY . Hellaswag: Can a machine really finish your sentence?\\narXiv preprint arXiv:1905.07830, 2019.\\nZhang, T., Lin, Z., Yang, G., and Sa, C. D. Qpytorch: A\\nlow-precision arithmetic simulation framework, 2019.\\nZhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M.,\\nWright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al.\\nPytorch fsdp: experiences on scaling fully sharded data\\nparallel. arXiv preprint arXiv:2304.11277, 2023.\\nZhu, K., Zhao, Y ., Zhao, L., Zuo, G., Gu, Y ., Xie, D.,\\nGao, Y ., Xu, Q., Tang, T., Ye, Z., Kamahori, K., Lin,\\nC.-Y ., Wang, S., Krishnamurthy, A., and Kasikci, B.\\nNanoflow: Towards optimal large language model serv-\\ning throughput, 2024. URL https://arxiv.org/\\nabs/2408.12757.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Flash Communication\\nA. Background\\nA.1. GPU Topology\\nModern inference GPUs are connected via various hardware configurations. Figure 12 shows a typical simplified physical\\ntopology where every node contains 8 GPUs. Every two adjacent GPUs are connected via a PCIe of 64GB/s band-\\nwidth (NVIDIA, 2024c). Cross-GPU communication may take several different paths, e.g. GPU 0 to 1 has the shortest route,\\nbut from GPU 0 to 4 it has to go across two NUMA (Non-uniform memory access) nodes. Cross-node communication relies\\non NICs (Network interface cards) that transmit data via Ethernet, whose bandwidth is usually 100Gbps.\\nGPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nGPU 4\\nGPU 5\\nGPU 6\\nGPU 7\\nPCIe PCIe PCIe PCIe\\nCPUCPU\\nGPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nGPU 4\\nGPU 5\\nGPU 6\\nGPU 7\\nPCIe PCIe PCIe PCIe\\nCPUCPU\\nNIC NIC100Gbps\\n64GB/s64GB/s\\nFigure 12.Physical topology of two NVIDIA 8× L40 GPU nodes connected for inference. Each node has 8 GPUs interconnected with\\nPCI Switches and 2 NUMA nodes. For simplicity, NIC is shown to only connect with the last PCI.\\nFor high-performance large-scale training, high-end GPUs like A100 SXM GPUs enjoy a much wider bandwidth due to\\nthe combination use of NVLink (NVIDIA, 2024e), NVSwitch, and InfiniBand NIC. Each GPU in the same node directly\\nconnected with all other GPUs could reach 600GB/s via NVSwitch, and inter-node communication could have a bandwidth\\nof 200Gbps. These advanced hardware configurations tremendously accelerate the training of large language models.\\nWarp scheduling is critical for GPU utilization. L40 is shipped with 142 streaming multiprocessors (SM) while A100\\nhas 108. A warp consists of a group of 32 threads, which is the minimum scheduling unit for SM. Multiple warps can be\\nsimultaneously executed on an SM.\\nA.2. Collective Communication\\nLibraries. Due to the hierarchical design of GPU clusters, collective communication methods are crucial in distributed\\ntraining and inference. To synchronize the workloads across GPUs, communication libraries like NCCL (NVIDIA, 2024d),\\nMSCCL (Microsoft, 2024), HiCCL (Hidayetoglu et al., 2024), Gloo (Facebook, 2024), and Horovod (Sergeev & Balso,\\n2018) are developed to provide efficient collective communication operations for a group of devices. These libraries usually\\nhide the physical topology and organize GPUs in a ring (Mikami et al., 2018; Jia et al., 2018; Ying et al., 2018) or a tree. In\\nring-based topology, GPUs are connected hand by hand to create a logical circle, which maximizes the utilization of the full\\nbandwidth. In contrast, tree-based topology, especially double binary tree (Sanders et al., 2009), guarantees logarithmic\\ncommunication hops. Therefore, it is more beneficial to use ring-based communication for intra-GPUs and a tree-based\\napproach for inter-GPU clusters.\\nOperations. Collective operations such as broadcast, aggregation (Reduce/All-Reduce/Reduce-Scatter), collection\\n(Gather/All-Gather), and All2All are shipped out-of-box in most collective communication libraries. For instance, NCCL\\nprovides a series of such collective operations (NVIDIA, 2024a) where each rank processes or transmits the same amount of\\ndata. Reduce-Scatter sums data across nodes and then scatters the result to corresponding nodes. All-Gather collects data\\nfrom all nodes to all nodes. All-Reduce is a many-to-many reduce operation, where the same reduce operation is applied\\non all nodes. All2All exchanges data between all nodes, with each node sending and receiving an equal amount of data.\\nAll2All can be implemented with multiple point-to-point communication operations.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Flash Communication\\nA.3. Quantization Fundamentals\\nQuantization is a mapping from floating numbers to integers. We utilize asymmetric quantization which is formulated below,\\ns = Xmax − Xmin\\n2n − 1 , z= ⌈−Xmin\\ns ⌉ (1)\\nQ(X) =clamp(⌈X/s⌉ + z, 0, 2n − 1) (2)\\nwhere Xmax and Xmin denotes the maximum and minimum value of X, n is the quantization bit-width, s is called the\\nscale and z the zero point. Q(x) quantizes float X to integer to the target bitwidth.\\nSymmetric quantization is formulated as follows,\\ns = |X|max\\n2n−1 − 1 (3)\\nQ(X) =clamp(⌈X/s⌉, −2n−1, 2n−1 − 1) (4)\\nIEEE 754 standards for FP16. IEEE 754 (IEEE, 1985) FP16 includes 16 bits in total, which comprises 1 bit for the sign\\n(S), 5 bits for the exponent (E), and 10 bits for the mantissa or fraction (F). The bias for the exponent is 15, which means\\nthat the actual exponent value must be added to 15 to get the stored exponent value. Also, notice there’s an assumed leading\\n1 in the fractional part.\\nFP8 and FP4 Format. FP8 (Micikevicius et al., 2022) format is designed to advance FP16 with two encodings, E4M3\\n(4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa). E5M3 follows IEEE 754 conventions.\\nFP4 (Rouhani et al., 2023) is of E2M1. For quantization to FP4, we utilize QPyTorch (Zhang et al., 2019) for simulation.\\nB. Additional Experiments\\nB.1. C4 and WikiText\\nThe C4 and WikiText perplexity of FP16-weight LLaMA models is given in Table 6 while the INT8-weight version is shown\\nin Table 7. Both communication volumes are quantized with Flash communication.\\nTable 6.LLaMA models’ perplexity of C4 (upper rows) and WikiText2 (lower rows) with fine-grained communication quantization with a\\ngroup size of 128.\\nMODEL INT8 Asym FP8 INT6 Asym INT4 Asym\\n3-8B 8.89 8.90 9.20 9.68\\n3-70B 6.74 6.75 6.85 7.04\\n2-7B 6.98 6.98 7.07 7.21\\n2-13B 6.47 6.47 6.50 6.58\\n2-70B 5.52 5.52 5.55 5.60\\n3-8B 6.14 6.15 6.37 6.70\\n3-70B 2.86 2.87 3.00 3.21\\n2-7B 5.47 5.48 5.55 5.66\\n2-13B 4.88 4.89 4.93 4.99\\n2-70B 3.32 3.32 3.35 3.40\\nC. Additional Latency Measurements\\nWe list the latency measurements of LLaMA models under various configurations (weight precision, tensor parallelism,\\nGPU cards) in the following Fig. 13 and Fig. 14.\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Flash Communication\\nTable 7.The 8-bit LLaMA models’ perplexity of C4 (upper rows) and WikiText2 (lower rows) with fine-grained communication\\nquantization with a group size of 128.\\nMODEL INT8 Asym FP8 INT6 Asym INT4 Asym\\nLLAMA -2-7B 7.00 7.01 7.10 7.24\\nLLAMA -2-13B 6.51 6.51 6.55 6.63\\nLLAMA -2-70B 5.54 5.54 5.57 5.62\\nLLAMA -3-8B 9.01 9.02 9.33 9.85\\nLLAMA -3-70B 6.82 6.83 6.95 7.13\\nLLAMA -2-7B 5.50 5.51 5.59 5.69\\nLLAMA -2-13B 4.92 4.92 4.97 5.03\\nLLAMA -2-70B 3.35 3.35 3.39 3.43\\nLLAMA -3-8B 6.25 6.26 6.49 6.84\\nLLAMA -3-70B 2.96 2.97 3.13 3.32\\n8 16 32 64\\nBatch Size\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50TTFT Speed-up Ratio\\n1.29\\n1.40\\n1.53\\n1.25\\n1.36\\n1.48\\n1.21\\n1.29\\n1.39\\n1.20\\n1.28\\n1.37\\nLLaMA-3-8B (INT8) on L40, TP=2\\nFP16\\nINT8\\nINT6\\nINT4\\n8 16 32 64\\nBatch Size\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0TTFT Speed-up Ratio\\n1.091.081.10\\n1.041.051.07\\n1.021.031.05 1.081.101.11\\nLLaMA-3-8B (FP16) on L40, TP=2\\nFP16\\nINT8\\nINT6\\nINT4\\nFigure 13.TTFT speed-up ratio of LLaMA-3-8B (INT8 vs. FP16) under various communication quantization bit widths on L40 with\\nTP=2.\\n8 16 32 64\\nBatch Size\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1TTFT Speed-up Ratio\\n1.06\\n1.091.11\\n1.06\\n1.081.10\\n1.05\\n1.071.09\\n1.04\\n1.071.08\\nLLaMA-3-70B (INT8) on A100, TP=4\\nFP16\\nINT8\\nINT6\\nINT4\\n8 16 32 64\\nBatch Size\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1TTFT Speed-up Ratio\\n1.031.041.05 1.031.041.05\\n1.031.041.05 1.031.041.05\\nLLaMA-3-70B (FP16) on A100, TP=4\\nFP16\\nINT8\\nINT6\\nINT4\\nFigure 14.TTFT speed-up ratio of LLaMA-3-70B (INT8 vs. FP16) under various communication quantization bit widths on A100 with\\nTP=4.\\n15')]\n"
     ]
    }
   ],
   "source": [
    "##MOST RECOMMENDED TECHNIQUE\n",
    "#PyPDFLoader\n",
    "\n",
    "print(\"PyPDFLoader\")\n",
    "try:\n",
    "    pypdfloader= PyPDFLoader(\"data/pdf/amy.pdf\")\n",
    "    py_doc=pypdfloader.load()\n",
    "    print(py_doc)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"error : {e}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5126a91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyPDFLoader\n",
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\\nLarge Language Model Inference\\nQingyuan Li 1 Bo Zhang 1 Liang Ye1 Yifan Zhang 1 Wei Wu1 Yerui Sun1 Lin Ma 1 Yuchen Xie1\\nAbstract\\nThe ever-increasing sizes of large language mod-\\nels necessitate distributed solutions for fast infer-\\nence that exploit multi-dimensional parallelism,\\nwhere computational loads are split across vari-\\nous accelerators such as GPU clusters. However,\\nthis approach often introduces significant com-\\nmunication overhead, especially on devices with\\nlimited bandwidth. In this paper, we introduce\\nFlash Communication, a novel low-bit compres-\\nsion technique designed to alleviate the tensor-\\nparallelism communication bottleneck during in-\\nference. Our method substantially boosts intra-\\nnode communication speed by more than 3× and\\nreduces the time-to-first-token by 2×, with nearly\\nno sacrifice in model accuracy. Extensive experi-\\nments on various up-to-date LLMs demonstrate\\nthe effectiveness of our approach.\\n1. Introduction\\nTo date, the number of parameters of large language models\\nhas tremendously increased. For instance, GPT-3 (Brown\\net al., 2020) has 175B, DeepSeek V2 (Liu et al., 2024)\\nutilizes 236B, LLaMA-3 (Dubey et al., 2024) reaches 450B.\\nTheir enormous sizes create big challenges for both training\\nand inference.\\nTo tackle the scaling difficulties of large language models,\\nthe research community now resorts to multiple parallelism\\nstrategies across a large group of computing accelerators.\\nSince previous parallelism methods focus on resolving the\\ntraining challenges, we quickly review these methods for\\na background check. Particularly, data parallelism (Dean\\net al., 2012; Ben-Nun & Hoefler, 2019) is first introduced\\nto allocate the training samples onto multiple GPUs where\\neach GPU retains a duplicate of the model and processes\\nits own given batch of samples. Synchronization is hence\\nrequired at the end of each iteration to update the model pa-\\n*Equal contribution 1Meituan. Correspondence to: Qingyuan\\nLi <liqingyuan02@meituan.com>.\\nPreprint.\\nL40 FP16/FP16L40 FP16/INT4L40 INT8/FP16L40 INT8/INT4A100 FP16/FP16A100 FP16/INT4A100 INT8/FP16A100 INT8/INT4\\n0\\n20\\n40\\n60\\n80\\n100Cost Percentage\\n52.2\\n71.3\\n21.9\\n40.1\\n75.1 80.6\\n59.2 65.3\\n42.4\\n22.3\\n65.9\\n37.7\\n18.4 11.6\\n25.1 17.4\\n3.4 3.9\\n5.6\\n10.1\\n3.8 4.0\\n2.8 6.2\\n2.0 2.5 6.6 12.0\\n2.7 3.9\\n12.9 11.1\\nLLaMA-3-70B Latency Breakdown\\nFP16 GEMM\\nINT8 GEMM\\nFP16 AllReduce\\nFlash AllReduce\\nNorm&Act\\nOthers\\nFigure 1.Prefill cost breakdown of LLaMA-3-70B operations\\nwith and without Flash Communication, as measured by\\nNSys (NVIDIA, 2024b). Tested on 4 ×L40/A100 GPUs (TP=4)\\nwith a batch size of 8, each with 1024 input and 64 output tokens.\\nNCCL (NVIDIA, 2024d)’s Ring All-Reduce is applied. The notion\\nof x-ticks (e.g. L40 FP16/FP16) denotes GPU type, model weight\\nprecision, and communication precision, respectively.\\nrameters. In the LLM era, ZeRO (Rajbhandari et al., 2020)\\nand FSDP (Zhao et al., 2023) renovate data parallelism by\\nsharding models on all devices but virtually rendering a\\nwhole model on a single device through All-Gather com-\\nmunication. In contrast, pipeline parallelism partitions se-\\nquential layers onto different GPUs where point-to-point\\ncommunication is adopted to transmit activation and gradi-\\nents. However, it creates data dependency which leads to\\nsubstantial GPU idle time, called bubbles. To improve GPU\\nutilization, GPipe (Huang et al., 2019) schedules micro-\\nbatches in a pipeline with forward passes and then followed\\nby backward passes. PipeDream (Harlap et al., 2018) pro-\\nposes one-forward one-backward (1F1B) to further reduce\\nthe bubble ratio. Megatron-LM (Narayanan et al., 2021)\\nadvances PipeDream by allowing each device to perform\\ncomputation for multiple non-contiguous subsets of layers.\\nAnother dimension to split the model is tensor parallelism\\nwhich splits the tensors of each layer and performs All-\\nReduce to aggregate the activation and gradients from all\\ndevices. Megatron-LM (Shoeybi et al., 2019; Narayanan\\n1\\narXiv:2412.04964v1  [cs.AI]  6 Dec 2024'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='Flash Communication\\net al., 2021) is such an example that devises delicate con-\\njugate tensor slicing schemes (e.g. column-wise first and\\nrow-wise next for the MLP layer) to remove unnecessary\\nsynchronization inside a transformer block.\\nWith the rise of LLMs developed for the long-context sce-\\nnario, sequential parallelism (Korthikanti et al., 2023) is pro-\\nposed to divide activation of LayerNorm and Dropout layers\\nin the sequence dimension as they are sequence-independent.\\nIt also jointly combines with tensor-parallelism by replacing\\ntwo All-Reduce operations with one All-Gather and one\\nReduce-Scatter to merge the communication cost. However,\\nself-attention and MLP layers are left untouched for sequen-\\ntial parallelism. In this regard,context parallelism(NVIDIA,\\n2024b) is designed to separate all layers in sequence dimen-\\nsion. To break the sequence dependency in self-attention,\\nRing Attention (Liu et al., 2023) applies blockwise self-\\nattention and feedforward (Dao, 2023; Liu & Abbeel, 2023)\\nin a distributed environment with point-to-point communi-\\ncation. On top of this, Deepspeed-Ulysses (Jacobs et al.,\\n2023) exchanges point-to-point communication for All2All\\nfor faster speed.\\nAnother emerging direction is sparse architectures repre-\\nsented by mixture-of-experts models (Jiang et al., 2024;\\nTeam, 2024; Liu et al., 2024). Expert parallelism (Fedus\\net al., 2022) parallelizes the experts on different GPUs which\\nrequires All2All communication. Deepspeed-MoE (Rajb-\\nhandari et al., 2022) propose hierarchical All2All communi-\\ncation to reduce the number of communication hops.\\nAs large language models continue to scale up, modern\\nframeworks like DeepSpeed (Microsoft, 2024), and Mega-\\ntron (NVIDIA, 2024a) tend to make joint use of the afore-\\nmentioned parallelism to accelerate the training process.\\nNevertheless, they easily meet communication bottlenecks\\nas they require many collective operations. This overhead\\ngrows as the model becomes larger.\\nMeanwhile, the communication bottleneck is also pro-\\nnounced when serving large language models in the cloud.\\nConstrained by strict service level objectives (SLOs), multi-\\nple parallelism schemes are adopted to speed up the infer-\\nence, where tensor parallelism is the most popular option\\namong all. Besides, due to the lower bandwidth of infer-\\nence GPUs, communication can account for more than half\\nof the prefill inference cost where an 80-layer LLaMA-3-\\n70B (Dubey et al., 2024) carries out 160 all-reduce opera-\\ntions at each forward pass on 4 ×L40 GPUs, as shown in\\nFigure 1. Therefore, to enable a faster speed, we must make\\nefficient use of limited intra-node bandwidth.\\nIn this work, we design a novel technique to reduce the com-\\nmunication cost introduced by tensor parallelism without\\nsubstantially sacrificing accuracy. Our contributions are,\\n1. Through detailed measurements, we unveil the com-\\nmunication bottleneck problem that also recurs in large\\nlanguage model (LLM) inference. For instance, com-\\nmunication can account for up to 65% of the total\\nlatency on NVIDIA L40 GPUs (Fig. 1).\\n2. We design an efficient communication mechanism\\ncalled Flash Communication, which applies low-bit\\nfine-grained quantization on activations to reduce com-\\nmunication volume and employs a two-step all-reduce\\nstrategy to minimize communication hops.\\n3. We implement a fused CUDA kernel called Flash All-\\nReduce to perform Flash Communication, achieving\\nup to a 2 × reduction in time-to-first-token (TTFT)\\non NVIDIA L40 GPUs. Even on A100 GPUs with\\nhigher communication bandwidth, we observe notable\\nlatency reductions, demonstrating the effectiveness of\\nour method.\\n2. Related Work\\nBefore diving into the investigated problem, we cover\\nsome fundamental knowledge required for discussion in\\nAppendix A. We suggest that readers without prior experi-\\nence quickly review the content.\\nCommunication efficiency is crucial to distributed training\\nand serving, as it directly affects the total processing time\\nand cost. Several techniques have been proposed to opti-\\nmize communication in distributed training in recent years,\\nincluding topology optimization, pipeline optimization, and\\ncompression methods.\\n2.1. Topology Optmization\\nTopology optimization adjusts communication patterns to\\nmatch the physical topology of hardware to reduce com-\\nmunication latency/hops, mainly ring-based and tree-based.\\nRing-All-Reduce (Baidu Research, 2024) organizes work-\\ners in a ring topology so that the overall communication\\nlatency is constant regardless of the number of workers. Say\\na worker transmits data of volume M to a group of N − 1\\nworkers, the total communication volume is2M(N −1)/N,\\nwhich is approximately 2M when N >> 1. However, it\\ndoesn’t take the physical topology into account, where intra-\\nand inter-node communication have different bandwidths.\\nHence the average speed depends largely on the lowest band-\\nwidth in such a strategy. Hierarchical Ring-All-Reduce (Jia\\net al., 2018) highlights the importance of hierarchical struc-\\ntures in managing overheads, which employs three-phase\\nall-reduce for separate intra- and inter-node communication.\\nLater, 2D-Torus (Mikami et al., 2018) organizes GPUs in a\\n2D-grid of (X, Y) so that the inter-node horizontal commu-\\nnication volume is X times smaller than that of hierarchical\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Flash Communication\\nRing-All-Reduce. NCCL (NVIDIA, 2019) introduces dou-\\nble binary trees (Sanders et al., 2009) provides logarithmic\\nlatency by reducing hops from 2(N − 1) to 2log(N). How-\\never, it is more prone to result in suboptimal bandwidth\\nutilization, as only a subset of nodes are engaged in any\\ngiven communication step.\\nWith the rise of sparse architectures like mixture-of-experts,\\nAll2All collective operation is common for communication\\nin expert parallelism. DeepSpeed-MoE (Rajbhandari et al.,\\n2022) and HetuMoE (Nie et al., 2022) both utilize a scalable\\nhierarchical scheme to reduce all-to-all communication hops\\nfor faster speed.\\nBesides, NVLink SHARP (NVIDIA, 2023) is a hardware\\nimprovement that offloads collective operations from GPUs\\nto the network devices, hence eliminating the need to send\\ndata multiple times between endpoints.\\n2.2. Pipeline Optimization\\nPipelining optimization aims to maximize resource utiliza-\\ntion with optimized scheduling strategies, mainly by over-\\nlapping computation with communication. Domino (Wang\\net al., 2024) breaks data dependency in Tensor-parallelism\\nby splitting activations row-wisely and weights column-\\nwisely into smaller independent parts. FLUX (Chang et al.,\\n2024) divides computation and communication operations\\ninto much finer-grained operations and later merges them\\nin a larger kernel to effectively hide communication. Dis-\\ntributedGEMM (Hassani et al., 2024) provides an imple-\\nmentation based on CUTLUSS (NVIDIA, 2024f) using P2P\\ncommunication. ScMoE (Cai et al., 2024) implements a\\nshortcut-connected MoE architecture to effectively decouple\\ncommunication from its conventional sequence, allowing\\nfor a substantial overlap.\\n2.3. Communication Compression\\nCompression techniques like sparsification and quantiza-\\ntion are proposed to balance communication reduction with\\nacceptable performance degradation. Sparse Communica-\\ntion (Aji & Heafield, 2017) observes that gradient updates\\nare mostly close to zero and maps them directly to zero to\\nonly exchange sparse matrices among distributed nodes. In\\ncontrast, DISCO (Qin et al., 2024) aims to achieve sparse\\ncommunication by gradually pruning the network to gen-\\nerate sparse features. QSDP (Markov et al., 2023) remove\\nFSDP’s communication bottleneck by performing both gra-\\ndient and weight quantization. ZeRO++ (Wang et al., 2023)\\napplies All-Gather with blockwise weight quantization and\\nan All2All-based gradient quantization to reduce the com-\\nmunication volume when collecting weights and gradients.\\n3. Method\\n3.1. Motivation\\nTensor Parallelism (TP) is now supported in almost all main-\\nstream inference frameworks like TensorRT-LLM (NVIDIA,\\n2023), vLLM (Kwon et al., 2023), SGLang (sgl-project,\\n2024), and LMDeploy (LMDeploy, 2023), becoming the\\nmost adopted scheme in LLM inference. However, TP\\ncomes at a non-negligible cost due to heavy communica-\\ntion, which in the case of larger language models creates an\\nexcessive communication overhead. For example, the com-\\nmunication overhead of LLaMA-3-70B on L40 GPUs easily\\nmeets the bottleneck as the input token length increases,\\nshown by the cost breakdown of LLaMA-3-70B operations\\nin Figure 2. Although high-end training-purpose accelera-\\ntors like NVIDIA A100 where GPUs are connected through\\nNVLink (NVIDIA, 2024e), the communication overhead\\nstill reaches a notable 20%. We can easily conclude that TP\\ncommunication is the inference bottleneck.\\n4 8 16 32 64 128\\nSequence Length\\n0\\n20\\n40\\n60\\n80\\n100Cost Percentage\\n78.4 73.6\\n65.1\\n56.3 55.6 54.8\\n17.9 22.1\\n31.0\\n40.0 41.8 42.8\\n0.6 0.7 0.8 0.9 1.0 1.23.1 3.6 3.0 2.8 1.6 1.2\\nLLaMA-3-70B Latency Breakdown\\nGEMM AllReduce Norm&Act Others\\nFigure 2.Prefill cost breakdown of LLaMA-3-70B operations at\\nvarious sequence lengths. Tested on 4×L40/A100 GPUs (TP=4)\\nwith a batch size of 8.\\nFor communication optimization, one can think of overlap-\\nping the communication with computation to hide overhead\\nbut it requires sophisticated design with scheduling which\\nis harder to implement in any given inference framework,\\nfor which reason NanoFlow (Zhu et al., 2024) invents a\\nnew serving framework to circumvent the difficulty. On\\nthe contrary, compression is a handy option but none of\\nthe above-mentioned methods investigated communication\\nquantization for LLM inference. It could be due to the acti-\\nvation quantization challenge posed at the inference stage,\\nwhich is pointed out by LLM.int8() (Dettmers et al., 2022)\\nand SmoothQuant (Xiao et al., 2024) that activations are\\nharder to quantize because of outliers. The communication\\nquantization method for training doesn’t suffer from this\\nproblem as it only quantizes weights and gradients (Wang\\net al., 2023). Besides, during training, communication quan-\\ntization degradation can be compensated by further learning.\\nWhereas at inference, the quantization loss is nearly irre-\\nversible.\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Flash Communication\\nAt the same time, the existing Ring All-Reduce operation\\nadopted in tensor parallelism remains a bottleneck at infer-\\nence since it is inclined to be constrained by lower band-\\nwidth. Furthermore, to integrate quantization with Ring\\nAll-Reduce, also shown by ZeRO++ (Wang et al., 2023),\\nit requires N times of sequential quantization and dequan-\\ntization in a complete Reduce-Scatter, which worsens the\\nlatency.\\nThe above issues call for a delicate orchestration of the\\nquantization approach and a better All-Reduce scheme.\\n3.2. Flash Communication\\nMotivated by the above, we approach the inference chal-\\nlenges in the distributed scenario with quantization and\\ntopology optimization. In the paper, we specifically exam-\\nine tensor parallelism as it is the most popular paradigm.\\nTake tensor parallelism in LLaMA-3 (Dubey et al., 2024) as\\nan example, the tensors of QKV projections are first sliced\\ncolumn-wisely and then the output projection row-wisely.\\nAfter that, an All-Reduce operation is required to collect\\nactivations on each device, shown in Fig. 3. Similarly in\\nthe feedforward network, the gate projection and the up\\nprojection are split by column and the down projection\\nby row, then another All-Reduce is needed to sum up the\\nactivations from both devices. To reduce the communication\\nvolume, we are left to compress the activation from the\\noutput projection and the down projection.\\nRMSNorm\\nGate \\nw/ Swish\\nUp\\nDown\\nRMSNorm\\nW1\\nW2\\nSelfAttentionQKV Out\\n+\\nFlash \\nAll-Reduce\\nW1\\nW2\\n+×\\nFlash \\nAll-Reduce\\nW1\\nW2\\nW1\\nW2\\nW1\\nW2\\nFigure 3.Tensor parallelism for a LLaMA-3 transformer block.\\nOur Flash All-Reduce is applied to speed up communication.\\n3.2.1. Q UANTIZATION CHALLENGE\\nTo obtain an optimal trade-off between accuracy and latency,\\nwe choose to apply low-bit quantization. From Fig. 4, we\\nobserve that fine granularity is necessary since per-token\\nquantization at larger block sizes suffers from performance\\ncollapse in terms of C4 perplexity, albeit the asymmetric\\nversion is relatively better.\\nHowever, we discover that it is non-trivial to apply low-bit\\n8192 4096 2048 1024 512 256 128\\nBlock Size\\n102\\n104\\n106\\nC4 Perplexity\\nper-token \\nPer-token Fine-granularity\\nINT4Sym\\nINT4Asym\\nFigure 4.Activation quantization with various block sizes of\\nLLaMA-3-8B on C4. Starting from 4096 (the length of hidden\\ndimension), the granularity becomes finer till 128.\\nactivation quantization in this scenario. To investigate the\\nquantization sensitivity, we calculate the layerwise mean\\nsquared errors (MSE) before and after activation quantiza-\\ntion on LLaMA-3-8B, as depicted in Figure 5. We find that\\nthe down projection dproj is much harder to quantize than\\nthe output projection oproj , as the former MSEs are quite\\ndistinct even on a logarithmic scale. This phenomenon is\\nalso discovered by (Li et al., 2023; Ashkboos et al., 2023;\\nYu et al., 2024).\\n0 10 20 30\\nLayer\\n10 6\\n10 5\\n10 4\\n10 3\\n10 2\\nMSE\\ndproj\\noproj\\n0 10 20 30\\nLayer\\n10 7\\n10 6\\n10 5\\n10 4\\n10 3\\nMSE\\nRS (INT4)\\nAG (INT8)\\nAG (INT4)\\nFigure 5.Left: Comparison of oproj and dproj All-Reduce Quan-\\ntization MSE. Right: MSE of quantization before Reduce-Scatter\\n(RS) vs. All-Gather (AG).\\nBesides, an All-Reduce comprises a pair of Reduce-Scatter\\nand All-Gather operations, where the quantization corre-\\nsponding to each operation exhibits different levels of diffi-\\nculty, see Fig. 5 right. This is as expected since the quantiza-\\ntion before Reduce-Scatter only introduces rounding errors\\nwhile in the case of All-Gather, it includes both rounding\\nand accumulated errors. Alternatively, we could use a higher\\nprecision for the quantization before All-Gather to improve\\naccuracy.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='Flash Communication\\nGPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nA0 A1 A2 A3\\nB0 B1 B2 B3\\nC0 C1 C2 C3\\nD0 D1 D2 D3\\nQ\\nA0 B0 C0 D0\\nA1 B1 C1 D1\\nA2 B2 C2 D2\\nA3 B3 C3 D3\\nDQ\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nDQ\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nQ\\nAll2All Reduce \\nSum\\nAll \\nGather\\nFigure 6.Flash communication’s two-step All-Reduce. The communication volume is quantized and dequantized only twice.\\n3.2.2. A LGORITHM\\nConsidering the above issues, we design a two-step quanti-\\nzation strategy to replace vanilla Ring All-Reduce, as por-\\ntrayed in Fig. 6. Its integration with tensor parallelism is\\nshown in Fig.3. We name our overall strategy as two-step\\nAll-Reduce.\\nFig. 6 illustrates how Flash Communication works. First,\\nwe divide the computation volume (activation) on each GPU\\nby the number of ranks. After fine-grained quantization on\\nactivation, we perform All2All communication so that each\\ndevice receives its computation load for reduction. After\\non-device reduction, the sum is again quantized to speed up\\nthe transmission. We then perform All-Gather to collect all\\nresults and dequantization to recover float values on each\\ndevice. This two-step workflow is also formulated by Alg. 1.\\nAlgorithm 1 Flash All-Reduce\\nInput: Communication volume M, world size N, chunk\\nsize C, quantization bit-width b, group size g\\nOutput: Reduced sum Sdq\\nDivide M into T = ⌈M/C⌉ chunks.\\nfor 1 ≤ i ≤ T do\\n// Quantize volume to obtain zeros and scales\\nMq\\ni , zi, si = FinegrainedQuantize(Mi, b, g);\\n// Each device sends and receives volume from others\\nAll2All(Mq\\ni , zi, si, N);\\nfor 1 ≤ j ≤ N do\\nMdq\\nij = Dequantize(Mq\\nij , zij , ij );\\nend for\\nSi = ReduceSum(Mdq\\ni0 , Mdq\\ni1 , ··· , Mdq\\niN );\\nSq\\ni , zs\\ni , ss\\ni = FinegrainedQuantize(Si, b, g);\\n// Each device collects the reduced sum from others\\nAll-Gather(Sq\\ni , zs\\ni , ss\\ni , N);\\nfor 1 ≤ j ≤ N do\\nSdq\\nij = Dequantize(Sq\\ni , zs\\ni , ss\\ni );\\nend for\\nend for\\nTable 1.Comparison of Ring All-Reduce vs. Flash All-Reduce\\nMETHOD RING ALL-REDUCE FLASH ALL-REDUCE\\nTOTAL VOLUME 2M(N − 1)/N 2M(N − 1)/N\\nREDUCE STEP N − 1 1\\nREDUCE -SCATTER M/N M (N − 1)/N\\nGATHER STEP N − 1 1\\nALL-GATHER M/N M (N − 1)/N\\nQDQ S TEP N 2\\n3.2.3. K ERNEL DESIGN\\nFor efficiency, we implement a fused Flash All-Reduce\\nkernel to encompass all the above collective communication\\noperations and quantization steps. Compared with Ring\\nAll-Reduce in Table 1, Flash All-Reduce cuts quantization-\\ndequantization steps from N to 2, and Reduce/Gather steps\\nfrom N −1 to 1. Although the size of total volumes remains\\nthe same, each of our volumes is quantized to lower bits,\\nsubstantially reducing the amount of data to transmit. We\\nsummarize three key aspects in designing our kernel below.\\nFast Fine-grained Quantization The total communication\\nvolume M for each rank is divided into T chunks for trans-\\nmission. Given a chunk size C, we draw how GPU threads\\nare organized in parallel to process the chunk information\\nin Fig. 7. A chunk is split into N blocks and each block\\ncorresponds to 32 warps, where each warp is a collection of\\n32 threads and each thread can process eight FP16 elements.\\nTake our asymmetric quantization with a group size of 128\\nas an example, we perform quantization on each group of\\n128 elements using 16 threads. Specifically, we leverage\\nthe CUDA API function shfl xor sync to iteratively\\nexchange information among these warp threads to achieve\\nmax/min reduction efficiently.\\nFast Communication. Instead of calling All2All primitive,\\nwe utilize GPU peer direct memory access from CUDA Run-\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Flash Communication\\nTotal 1024 threads\\nQuantized \\nVolume\\nscales & zeros\\nWarp1Warp0 …… Warp31Warp30\\nWarp0 Warp1 Warp30 Warp31\\n……\\n……\\n……\\n……\\n……\\n……Warp0 Warp1 Warp30 Warp31\\nBlock 0\\nBlock 1\\nBlock N\\n8192 x INT8 128 x FP16\\nFigure 7.Thread mapping of fast fine-grained quantization.\\ntime API (NVIDIA Corporation, 2024) to transmit quan-\\ntized volume, where we can directly fetch data from differ-\\nent ranks, substantially boosting the communication speed.\\nFast Dequantization. Once quantization volumes are re-\\nceived, we have to dequantize them into FP16 for sum re-\\nduction. As na ¨ıve INT4 to FP16 conversion incurs over-\\nhead, we utilize the dequantization layout in (Kim et al.,\\n2022). To coordinate its ordering online, we also employ\\nfast INT4 packing (LMDeploy, 2023), as illustrated in\\nFig. 8. Given that two 32-bit unsigned integers U0 and\\nU1 holding 4 INT4-quantized activations (each stored in\\nthe lower 4 bits out of 8 bits) to transmit, we first perform\\nthe right shift by 12 bits, and then apply bitwise OR to\\nitself. Later we select the target bits from these two inte-\\ngers with CUDA Math API byte perm. In this way,\\nwe can pack 8 4-bit integers in a convenient order to de-\\nquantize. Next we apply lop3.b321 to perform logical\\noperation (0xF0 & 0xCC)|0xAA on the packed vari-\\nable with mask 0x000F000F and 0x64006400, then we\\nsubtract it with 0x64006400, which effectively represents\\nW1 and W0 in FP16. The dequantization can be performed\\niteratively by varying the masks for the rest INT4 integers.\\nfW1 fW0\\nW7 W6 W5 W4 W3 W2 W1 W0\\nlop3.b32 ( , 0x000F000F , 0x64006400, (0xF0 & 0xCC) | 0xAA ) W5W7 W1W3 W4W6 W0W2\\n, 0x64006400)sub.f16x2 ( 0x40x6 W10x0 0x40x6 W00x0\\nW7 W6 W5W7 W4W6 W3 W2 W1W3 W0W2__byte_perm ( , 0x5140),\\nU0 |= U0 >> 12 U1 |= U1 >> 12\\nW7 W6 W3 W2\\n}\\nFigure 8.Fast INT4 packing and dequantization. For simplicity,\\nonly the dequantization of W1 and W0 is shown. An empty square\\nmeans zero.\\n1https://docs.nvidia.com/cuda/parallel-\\nthread-execution/#logic-and-shift-\\ninstructions-lop3\\nINT6 Quantization. Given that quantization prior to All-\\nGather leads to greater loss, as illustrated in Figure 5 (left),\\nwe opt for an INT8 bit-width for All-Gather operations and\\nmaintain INT4 for ReduceSum, effectively creating an INT6\\nsolution. As later shown in Table 3, the INT6 configuration\\nstrikes a commendable balance between performance and\\ncommunication efficiency.\\n4. Experiments\\n4.1. Setup\\nUnless otherwise noted, we use an input token length of\\n1024 and an output token length of 64 for the inference mea-\\nsurement. Latencies are tested on NVIDIA L40 and A100\\nSXM GPUs. The baseline uses FP16 for communication.\\n4.2. Accuracy Comparison\\nFP16 Weights. We evaluate the accuracy of LLaMA-2 and\\nLLaMA-3 models on PIQA (Bisk et al., 2020), ARCC and\\nARCE (Clark et al., 2018), HellaSwag (Zellers et al., 2019),\\nWinoGrande (Sakaguchi et al., 2021) in various commu-\\nnication quantization bit widths, shown in Table 2. In all\\ncases, asymmetric INT8 quantization obtains the best ac-\\ncuracies. Asymmetric INT4 is also better than symmetric\\nINT4. C4 (Raffel et al., 2020) and WikiText (Merity et al.,\\n2016) results are shown in Table 6 of Appendix B.1.\\nTable 2.Accuracy of LLaMA models with various communication\\nquantization strategies. All model weights are kept in FP16 pre-\\ncision. Prec: Communication precision. All INT quantization is\\nasymmetrical. HS: HellaSwag, WG: WinoGrande.\\nMODEL PREC PIQA ARC C ARCE HS WG A VG\\n2-7B FP16 79.11 46.33 74.58 76.01 69.30 64.83\\nINT8 79.11 45.99 74.75 76.10 69.06 64.77\\nINT6 78.78 45.99 74.79 75.75 68.75 64.68\\nINT4 78.02 45.82 74.49 75.63 67.25 64.23\\n2-13B FP16 80.52 49.15 77.48 79.39 72.14 67.83\\nINT8 80.69 49.06 77.61 79.34 71.59 67.78\\nINT6 79.98 49.32 77.06 79.17 71.11 67.11\\nINT4 79.60 48.21 76.89 78.92 71.90 67.04\\n2-70B FP16 82.70 57.34 81.02 83.80 77.98 72.43\\nINT8 82.75 57.68 80.93 83.80 77.98 72.47\\nINT6 82.48 57.00 80.85 83.77 76.87 72.23\\nINT4 82.92 57.25 80.43 83.60 77.27 71.99\\n3-8B FP16 80.79 53.41 77.69 79.16 72.77 68.64\\nINT8 80.58 52.47 77.40 79.09 73.09 68.45\\nINT6 79.98 51.37 77.65 78.73 73.16 68.06\\nINT4 80.09 51.02 75.84 78.11 70.48 66.92\\n3-70B FP16 84.55 64.33 85.86 84.89 80.35 75.25\\nINT8 84.55 63.91 85.82 84.90 80.82 75.27\\nINT6 84.11 61.69 85.44 84.87 80.35 74.76\\nINT4 83.13 61.35 83.33 84.69 78.93 73.82\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='Flash Communication\\n8 16 32 64\\nBatch Size\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0TTFT Speed-up Ratio\\n1.48\\n1.72\\n2.06\\n1.45\\n1.68\\n1.99\\n1.39\\n1.58\\n1.82\\n1.37\\n1.55\\n1.78\\nLLaMA-3-8B (INT8) on L40, TP=4\\nFP16\\nINT8\\nINT6\\nINT4\\n8 16 32 64\\nBatch Size\\n0.6\\n0.8\\n1.0\\n1.2TTFT Speed-up Ratio\\n1.10\\n1.16\\n1.19\\n1.10\\n1.16\\n1.19\\n1.10\\n1.15\\n1.19\\n1.09\\n1.15\\n1.18\\nLLaMA-3-70B (INT8) on A100, TP=8\\nFP16\\nINT8\\nINT6\\nINT4\\nFigure 9.TTFT speed-up ratio of 8-bit LLaMA-3-8B under various communication quantization bit widths on L40 with TP=4 (left), and\\n8-bit LLaMA-3-70B on A100 with TP=8 (right).\\nINT8 Weights. As model weights are quantized, the im-\\npact of communication overhead becomes more pronounced.\\nThis observation motivates us to explore communication\\nquantization in this context. We begin by quantizing the\\nweights of the LLaMA model using Smoothquant (Xiao\\net al., 2024). Subsequently, we implement fine-grained com-\\nmunication quantization to assess its impact on performance\\nas detailed in Table 3. Results on C4 (Raffel et al., 2020),\\nWikiText-2 (Merity et al., 2016) is shown in Appendix B.1.\\nTable 3.Accuracy of LLaMA models with various communication\\nquantization strategies (denoted as ‘Comm Prec’). All model\\nweights are quantized into INT8 precision with SmoothQuant\\n(α = 0.85 except 0.9 for LLaMA2-70B). Prec: Communication\\nprecision. All INT quantization is asymmetrical.\\nMODEL PREC PIQA ARC C ARC E HS WG A VG\\n2-7B FP16 79.00 46.16 74.24 75.89 68.75 68.81\\nINT8 79.27 45.65 74.37 75.89 68.51 68.74\\nINT6 78.56 45.39 74.75 75.75 68.67 68.62\\nINT4 77.53 45.31 74.20 75.51 68.59 68.23\\n2-13B FP16 80.25 49.49 77.27 79.37 71.59 71.59\\nINT8 80.41 49.32 77.53 79.21 72.22 71.74\\nINT6 80.09 49.40 77.02 79.14 71.74 71.48\\nINT4 79.11 49.06 76.30 78.89 71.27 70.93\\n2-70B FP16 83.08 57.94 80.98 83.54 77.66 76.64\\nINT8 83.08 57.76 80.68 83.55 78.14 76.64\\nINT6 82.81 57.94 80.85 83.78 76.80 76.44\\nINT4 82.92 56.83 80.35 83.41 78.14 76.33\\n3-8B FP16 80.36 51.96 77.69 78.71 73.48 72.44\\nINT8 80.09 52.90 77.57 78.79 73.09 72.49\\nINT6 80.03 52.56 79.12 78.38 71.82 72.38\\nINT4 78.94 50.17 77.10 77.87 70.56 70.93\\n3-70B FP16 84.44 63.99 85.56 84.55 79.72 79.65\\nINT8 83.84 63.48 85.56 84.67 79.79 79.47\\nINT6 83.57 61.26 83.67 84.66 80.58 78.75\\nINT4 82.70 61.60 83.29 84.51 77.82 77.98\\n4.3. Latency and Throughput Performance\\nFig. 9 illustrates weight-quantized LLaMA-3-8B and\\nLLaMA-3-70B’s TTFT comparison with and without Flash\\ncommunication. The lowest quantization bit yields the most\\ngain, i.e. 2.06× and 1.19× on L40 and A100 respectively.\\nMore measurements are listed in Appendix C.\\n5. Ablation Study\\n5.1. Integer vs. Low-bit Float\\nTable. 4 shows the difference between INTx and FPx com-\\nmunication quantization. In general, INT8 performs compa-\\nrably with FP8, while the asymmetric version of INT8 is the\\nbest among all. FP6 is a mixed version of FP8 (Micikevicius\\net al., 2022) and FP4 (Rouhani et al., 2023), which is a fair\\ncomparison with similarly mixed INT6.\\nTable 4.LLaMA models’ C4 Perplexity with INTx vs FPx quanti-\\nzation with a group size of 128. Weights are in FP16.\\nPREC 2-7B 2-13B 2-70B 3-8B 3-70B\\nFP8 Sym 6.98 6.47 5.52 8.90 6.75\\nINT8 Sym 6.98 6.47 5.52 8.90 6.74\\nINT8 Asym 6.98 6.47 5.52 8.89 6.74\\nFP6 Sym 7.09 6.52 5.56 9.22 6.87\\nINT6 Sym 7.15 6.57 5.59 9.48 6.94\\nINT6 Asym 7.08 6.51 5.55 9.20 6.86\\nFP4 Sym 7.24 6.60 5.61 9.72 7.07\\nINT4 Asym 7.50 6.71 5.69 10.51 7.30\\nINT4 Asym 7.21 6.58 5.60 9.68 7.04\\n5.2. Na¨ıve vs. Rotation-based Quantization\\nAs previously shown in Fig. 4, the C4 perplexity of\\ncoarse quantization suffers performance collapse, while fine-\\ngrained quantization gradually resolves the problem as the\\ngroup size increases. We investigate whether the Hadamard\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Flash Communication\\ntransform popularized by QuaRot (Ashkboos et al., 2024)\\ncould alleviate the issue in the coarse setting in Table 5. It\\nturns out that Hadamard transform with coarse quantization\\nreadily performs well, however, it loses its advantage in\\nfiner granularity cases as compared with na¨ıve asymmetric\\nquantization. Besides, it doesn’t exhibit gains on FP8.\\nTable 5.LLaMA models’ C4 perplexity with communication quan-\\ntization (na¨ıve vs. rotation) at various granularity levels and pre-\\ncision formats. Asymmetric quantization is used for INT4 and\\nsymmetric for FP8. HT: Hadamard Transform\\nMODELS GROUP INT4 W/ HT FP8 W/ HT\\n3-8B 8192 2363367.8 10.61 8.91 8.96\\n2048 284.56 10.11 8.91 8.96\\n128 9.68 9.67 8.90 8.94\\n3-70B 8192 7417.79 7.86 6.75 6.81\\n1024 10.47 7.76 6.75 6.81\\n128 7.04 7.64 6.75 6.82\\n2-7B 8192 47601520 7.86 7.01 7.01\\n1024 8.78 7.35 6.98 7.01\\n128 7.21 7.24 6.98 7.01\\n2-13B 8192 306.39 6.80 6.47 6.49\\n1024 7.43 6.68 6.47 6.49\\n128 6.58 6.62 6.47 6.49\\n2-70B 8192 49.96 5.71 5.52 5.54\\n1024 6.17 5.67 5.52 5.54\\n128 5.60 5.64 5.52 5.54\\n5.3. Flash All-Reduce vs. Ring All-Reduce\\n64MB 128MB 256MB 512MB\\n1GB\\nCommunication Volume\\n0\\n20000\\n40000\\n60000Latency ( s)\\n1.74x\\n2.27x\\n3.18x\\nAll-Reduce Latency on L40, TP=4\\nNCCL Ring AR\\nFlash AR INT8\\nFlash AR INT6\\nFlash AR INT4\\nFigure 10.Flash Communication’s All-Reduce (Flash AR) Perfor-\\nmance compared with NCCL’s ring version. NCCL’s latency is\\ntested with nccl-test (NVIDIA, 2016-2024).\\nAssembling several boosting techniques, the speed of our\\nFlash All-Reduce kernel surpasses that of Ring All-Reduce\\nby a large margin. Fig. 10 exhibits the latency measurement\\ngiven a certain amount of communication volume. When the\\ncommunication volume becomes obvious (e.g. larger than\\n64MB), our quantized All-Reduce is crucial to reduce the\\ncost, where the INT4 version brings at most 3.18× kernel\\nlatency reduction. Noticeably, the mixed precision version\\nINT6 obtains a good trade-off between INT8 and INT4.\\nWe further show that the number of streaming processors\\n(SMs) matters in Fig. 11. When the communication volume\\nis of small size, a smaller number of SMs is beneficial as\\nless kernel launch and inter-block synchronization overhead\\nis produced. When the volume gets larger, more SMs are\\nrequired for calculation. A configuration of 48 SMs strikes\\na better balance between communication and computation.\\n16KB 32KB 64KB 128KB 256KB 512KB 1MB\\n20\\n40Latency ( s)\\nAll-Reduce Latency on L40, TP=4\\nSM=4\\nSM=48\\nSM=128\\n2MB 4MB 8MB 16MB32MB64MB128MB256MB512MB\\n1GB\\nCommunication Volume\\n0\\n20000Latency ( s)\\n SM=4\\nSM=48\\nSM=128\\nFigure 11.The number of SMs affects the communication latency\\nat different sizes of communication volume. SM=48 is similar to\\nSM=128 in larger volumes.\\n6. Conclusion\\nOur work presents a novel technique to reduce the commu-\\nnication volume associated with tensor parallelism while\\nmaintaining accuracy. Our key contributions include a com-\\nprehensive analysis that reveals the communication bottle-\\nneck in Large Language Model (LLM) inference, the design\\nof a fast communication mechanism known as Flash Com-\\nmunication, and the demonstration of its implementation,\\nwhich has been shown to achieve up to a 2× TTFT reduction.\\nFlash Communication employs fine-grained quantization on\\nactivations and a two-step All-Reduce strategy to decrease\\ncommunication volumes significantly. We have conducted\\nextensive experiments on NVIDIA L40 and A100 GPUs\\nacross various configurations and with several state-of-the-\\nart LLMs, which have consistently demonstrated the effec-\\ntiveness of our approach. These findings address a critical\\nchallenge in parallel computing and pave the way for more\\nefficient and scalable LLM inference.\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Flash Communication\\nReferences\\nAji, A. F. and Heafield, K. Sparse communication for\\ndistributed gradient descent. In Proceedings of the\\n2017 Conference on Empirical Methods in Natural Lan-\\nguage Processing. Association for Computational Lin-\\nguistics, 2017. doi: 10 .18653/v1/d17-1045. URL\\nhttp://dx.doi.org/10.18653/v1/D17-1045.\\nAshkboos, S., Markov, I., Frantar, E., Zhong, T., Wang, X.,\\nRen, J., Hoefler, T., and Alistarh, D. Towards end-to-\\nend 4-bit inference on generative large language models.\\narXiv preprint arXiv:2310.09259, 2023.\\nAshkboos, S., Mohtashami, A., Croci, M. L., Li, B.,\\nCameron, P., Jaggi, M., Alistarh, D., Hoefler, T., and\\nHensman, J. Quarot: Outlier-free 4-bit inference in ro-\\ntated llms. arXiv preprint arXiv:2404.00456, 2024.\\nBaidu Research. baidu-allreduce: A C++ library demonstrat-\\ning ring allreduce and ring allgather techniques. GitHub\\nrepository, 2024. URL https://github.com/\\nbaidu-research/baidu-allreduce.\\nBen-Nun, T. and Hoefler, T. Demystifying parallel and dis-\\ntributed deep learning: An in-depth concurrency analysis.\\nACM Computing Surveys (CSUR), 52(4):1–43, 2019.\\nBisk, Y ., Zellers, R., Gao, J., Choi, Y ., et al. Piqa: Reasoning\\nabout physical commonsense in natural language. In Pro-\\nceedings of the AAAI conference on artificial intelligence,\\nvolume 34, pp. 7432–7439, 2020.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\n2020.\\nCai, W., Jiang, J., Qin, L., Cui, J., Kim, S., and Huang,\\nJ. Shortcut-connected expert parallelism for accel-\\nerating mixture-of-experts, 2024. URL https://\\narxiv.org/abs/2404.05019.\\nChang, L.-W., Bao, W., Hou, Q., Jiang, C., Zheng, N.,\\nZhong, Y ., Zhang, X., Song, Z., Yao, C., Jiang, Z., Lin,\\nH., Jin, X., and Liu, X. Flux: Fast software-based com-\\nmunication overlap on gpus through kernel fusion, 2024.\\nURL https://arxiv.org/abs/2406.06858.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\nquestion answering? try arc, the ai2 reasoning challenge.\\narXiv preprint arXiv:1803.05457, 2018.\\nDao, T. Flashattention-2: Faster attention with bet-\\nter parallelism and work partitioning. arXiv preprint\\narXiv:2307.08691, 2023.\\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M.,\\nMao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K.,\\net al. Large scale distributed deep networks. Advances in\\nneural information processing systems, 25, 2012.\\nDettmers, T., Lewis, M., Belkada, Y ., and Zettlemoyer,\\nL. Llm.int8(): 8-bit matrix multiplication for transform-\\ners at scale, 2022. URL https://arxiv.org/abs/\\n2208.07339.\\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\\nA., et al. The llama 3 herd of models. arXiv preprint\\narXiv:2407.21783, 2024.\\nFacebook. Gloo: Collective communications library with\\nvarious primitives for multi-machine training. https:\\n//github.com/facebookincubator/gloo,\\n2024. Accessed: 2024-11-23.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transformers:\\nScaling to trillion parameter models with simple and ef-\\nficient sparsity. Journal of Machine Learning Research,\\n23(120):1–39, 2022.\\nHarlap, A., Narayanan, D., Phanishayee, A., Seshadri, V .,\\nDevanur, N., Ganger, G., and Gibbons, P. Pipedream:\\nFast and efficient pipeline parallel dnn training. arXiv\\npreprint arXiv:1806.03377, 2018.\\nHassani, A., Isaev, M., McDonald, N., Ren, J.,\\nThakkar, V ., Wu, H., and Shi, H. Distributed\\ngemm, 2024. URL https://blog.shi-labs.com/\\ndistributed-gemm-88be6a481e2b. Accessed:\\n2024-12-04.\\nHidayetoglu, M., de Gonzalo, S. G., Slaughter, E., Surana,\\nP., Hwu, W.-m., Gropp, W., and Aiken, A. Hiccl: A hier-\\narchical collective communication library. arXiv preprint\\narXiv:2408.05962, 2024.\\nHuang, Y ., Cheng, Y ., Bapna, A., Firat, O., Chen, D., Chen,\\nM., Lee, H., Ngiam, J., Le, Q. V ., Wu, Y ., et al. Gpipe:\\nEfficient training of giant neural networks using pipeline\\nparallelism. Advances in neural information processing\\nsystems, 32, 2019.\\nIEEE. IEEE Standard for Floating-Point Arithmetic.\\nTechnical Report IEEE 754-1985, Institute of Elec-\\ntrical and Electronics Engineers, New York, NY,\\n1985. URL https://standards.ieee.org/\\nieee/754/6210/.\\nJacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song,\\nS. L., Rajbhandari, S., and He, Y . Deepspeed ulysses:\\nSystem optimizations for enabling training of extreme\\nlong sequence transformer models. arXiv preprint\\narXiv:2309.14509, 2023.\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Flash Communication\\nJia, X., Song, S., He, W., Wang, Y ., Rong, H., Zhou, F.,\\nXie, L., Guo, Z., Yang, Y ., Yu, L., et al. Highly scal-\\nable deep learning training system with mixed-precision:\\nTraining imagenet in four minutes. arXiv preprint\\narXiv:1807.11205, 2018.\\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,\\nB., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna,\\nE. B., Bressand, F., et al. Mixtral of experts. arXiv\\npreprint arXiv:2401.04088, 2024.\\nKim, Y . J., Henry, R., Fahim, R., and Awadalla, H. H.\\nWho says elephants can’t run: Bringing large scale\\nmoe models into cloud scale production. arXiv preprint\\narXiv:2211.10017, 2022.\\nKorthikanti, V . A., Casper, J., Lym, S., McAfee, L., Ander-\\nsch, M., Shoeybi, M., and Catanzaro, B. Reducing acti-\\nvation recomputation in large transformer models. Pro-\\nceedings of Machine Learning and Systems, 5:341–353,\\n2023.\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y ., Zheng, L., Yu,\\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\\nmemory management for large language model serving\\nwith pagedattention. In Proceedings of the ACM SIGOPS\\n29th Symposium on Operating Systems Principles, 2023.\\nLi, Q., Zhang, Y ., Li, L., Yao, P., Zhang, B., Chu, X., Sun,\\nY ., Du, L., and Xie, Y . Fptq: Fine-grained post-training\\nquantization for large language models. arXiv preprint\\narXiv:2308.15987, 2023.\\nLiu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C.,\\nDengr, C., Ruan, C., Dai, D., Guo, D., et al. Deepseek-v2:\\nA strong, economical, and efficient mixture-of-experts\\nlanguage model. arXiv preprint arXiv:2405.04434, 2024.\\nLiu, H. and Abbeel, P. Blockwise parallel transformers for\\nlarge context models. In Oh, A., Naumann, T., Globerson,\\nA., Saenko, K., Hardt, M., and Levine, S. (eds.),Advances\\nin Neural Information Processing Systems , volume 36,\\npp. 8828–8844. Curran Associates, Inc., 2023.\\nLiu, H., Zaharia, M., and Abbeel, P. Ring attention with\\nblockwise transformers for near-infinite context. arXiv\\npreprint arXiv:2310.01889, 2023.\\nLMDeploy. Lmdeploy: A toolkit for compressing, de-\\nploying, and serving llm. https://github.com/\\nInternLM/lmdeploy, 2023.\\nMarkov, I., Vladu, A., Guo, Q., and Alistarh, D. Quantized\\ndistributed training of large models with convergence\\nguarantees, 2023. URL https://arxiv.org/abs/\\n2302.02390.\\nMerity, S., Xiong, C., Bradbury, J., and Socher, R.\\nPointer sentinel mixture models. arXiv preprint\\narXiv:1609.07843, 2016.\\nMicikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey,\\nP., Grisenthwaite, R., Ha, S., Heinecke, A., Judd, P.,\\nKamalu, J., et al. Fp8 formats for deep learning. arXiv\\npreprint arXiv:2209.05433, 2022.\\nMicrosoft. DeepSpeed: Deep learning optimization li-\\nbrary. GitHub repository, 2024. URL https://\\ngithub.com/microsoft/DeepSpeed.\\nMicrosoft. Microsoft collective communication library\\n(msccl). https://github.com/microsoft/\\nmsccl, 2024. Accessed: 2024-11-23.\\nMikami, H., Suganuma, H., Tanaka, Y ., Kageyama, Y ., et al.\\nMassively distributed sgd: Imagenet/resnet-50 training in\\na flash. arXiv preprint arXiv:1811.05233, 2018.\\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Pat-\\nwary, M., Korthikanti, V ., Vainbrand, D., Kashinkunti, P.,\\nBernauer, J., Catanzaro, B., et al. Efficient large-scale\\nlanguage model training on gpu clusters using megatron-\\nlm. In Proceedings of the International Conference for\\nHigh Performance Computing, Networking, Storage and\\nAnalysis, pp. 1–15, 2021.\\nNie, X., Zhao, P., Miao, X., Zhao, T., and Cui, B. Hetumoe:\\nAn efficient trillion-scale mixture-of-expert distributed\\ntraining system, 2022. URL https://arxiv.org/\\nabs/2203.14685.\\nNVIDIA. NCCL Tests. https://github.com/\\nNVIDIA/nccl-tests, 2016-2024. Accessed: 2024-\\n12-06.\\nNVIDIA. Massively scale your deep learning training\\nwith nccl 2.4. https://developer.nvidia.com/\\nblog/massively-scale-deep-learning-\\ntraining-nccl-2-4/ , 2019. Accessed: 2024-11-\\n23.\\nNVIDIA. NVIDIA Scalable Hierarchical Aggrega-\\ntion and Reduction Protocol (SHARP) v2.6.1 Release\\nNotes. Technical Report Revision 2.6.1, NVIDIA, 2023.\\nURL https://docs.nvidia.com/networking/\\ndisplay/sharpv261/release+notes. Last up-\\ndated on May 23, 2023.\\nNVIDIA. TensorRT-LLM. GitHub repository,\\n2023. URL https://github.com/NVIDIA/\\nTensorRT-LLM.\\nNVIDIA. Megatron-LM: Ongoing research training trans-\\nformer models at scale. GitHub repository, 2024a. URL\\nhttps://github.com/NVIDIA/Megatron-LM.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='Flash Communication\\nNVIDIA. NVIDIA Nsight Systems. Web Page,\\n2024b. URL https://developer.nvidia.com/\\nnsight-systems.\\nNVIDIA. Collective operations. https:\\n//docs.nvidia.com/deeplearning/\\nnccl/user-guide/docs/usage/\\ncollectives.html, 2024a. Accessed: 2024-\\n11-23.\\nNVIDIA. Context parallelism overview. https:\\n//docs.nvidia.com/megatron-core/\\ndeveloper-guide/latest/api-guide/\\ncontext parallel.html, 2024b. Accessed:\\n2024-11-23.\\nNVIDIA. Nvidia l40: Delivering unprecedented visual\\ncomputing performance for the data center. https:\\n//images.nvidia.cn/content/Solutions/\\ndata-center/vgpu-L40-datasheet .pdf,\\n2024c. Accessed: 2024-11-23.\\nNVIDIA. Optimized primitives for collective multi-gpu\\ncommunication. https://github.com/NVIDIA/\\nnccl, 2024d. Accessed: 2024-11-23.\\nNVIDIA. Nvlink & nvswitch for advanced multi-gpu com-\\nmunication. https://www.nvidia.com/en-us/\\ndata-center/nvlink/, 2024e. Accessed: 2024-\\n11-26.\\nNVIDIA. Cuda templates for linear algebra subrou-\\ntines, 2024f. URL https://github.com/NVIDIA/\\ncutlass. Accessed: 2024-12-04.\\nNVIDIA Corporation. Peer Device Memory Ac-\\ncess. Technical report, NVIDIA, 2024. URL\\nhttps://docs.nvidia.com/cuda/cuda-\\nruntime-api/group CUDART PEER.html.\\nAccessed: 2024-12-04.\\nQin, M., Sun, C., Hofmann, J., and Vucinic, D. Disco:\\nDistributed inference with sparse communications. In\\nProceedings of the IEEE/CVF Winter Conference on Ap-\\nplications of Computer Vision, pp. 2432–2440, 2024.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y ., Li, W., and Liu, P. Exploring\\nthe limits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research, 21\\n(140):1–67, 2020.\\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y . Zero:\\nMemory optimizations toward training trillion parameter\\nmodels. In SC20: International Conference for High Per-\\nformance Computing, Networking, Storage and Analysis,\\npp. 1–16. IEEE, 2020.\\nRajbhandari, S., Li, C., Yao, Z., Zhang, M., Aminabadi,\\nR. Y ., Awan, A. A., Rasley, J., and He, Y . Deepspeed-moe:\\nAdvancing mixture-of-experts inference and training to\\npower next-generation ai scale. In International con-\\nference on machine learning, pp. 18332–18346. PMLR,\\n2022.\\nRouhani, B. D., Zhao, R., More, A., Hall, M., Khodamoradi,\\nA., Deng, S., Choudhary, D., Cornea, M., Dellinger, E.,\\nDenolf, K., et al. Microscaling data formats for deep\\nlearning. arXiv preprint arXiv:2310.10537, 2023.\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y .\\nWinogrande: An adversarial winograd schema challenge\\nat scale. Communications of the ACM , 64(9):99–106,\\n2021.\\nSanders, P., Speck, J., and Tr¨aff, J. L. Two-tree algorithms\\nfor full bandwidth broadcast, reduction and scan. Parallel\\nComputing, 35(12):581–594, 2009.\\nSergeev, A. and Balso, M. D. Horovod: fast and easy\\ndistributed deep learning in TensorFlow. arXiv preprint\\narXiv:1802.05799, 2018.\\nsgl-project. SGLang: A Fast Serving Framework for Large\\nLanguage Models and Vision Language Models. GitHub\\nrepository, 2024. URL https://github.com/sgl-\\nproject/sglang. Accessed: 2024-12-01.\\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\\nJ., and Catanzaro, B. Megatron-lm: Training multi-\\nbillion parameter language models using model paral-\\nlelism. arXiv preprint arXiv:1909.08053, 2019.\\nTeam, Q. Qwen1.5-moe: Matching 7b model perfor-\\nmance with 1/3 activated parameters”, February 2024.\\nURL https://qwenlm.github.io/blog/qwen-\\nmoe/.\\nWang, G., Qin, H., Jacobs, S. A., Holmes, C., Rajbhandari,\\nS., Ruwase, O., Yan, F., Yang, L., and He, Y . Zero++:\\nExtremely efficient collective communication for giant\\nmodel training. arXiv preprint arXiv:2306.10209, 2023.\\nWang, G., Zhang, C., Shen, Z., Li, A., and Ruwase, O.\\nDomino: Eliminating communication in llm training\\nvia generic tensor slicing and overlapping, 2024. URL\\nhttps://arxiv.org/abs/2409.15241.\\nXiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han,\\nS. Smoothquant: Accurate and efficient post-training\\nquantization for large language models, 2024. URL\\nhttps://arxiv.org/abs/2211.10438.\\nYing, C., Kumar, S., Chen, D., Wang, T., and Cheng, Y . Im-\\nage classification at supercomputer scale. arXiv preprint\\narXiv:1811.06992, 2018.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='Flash Communication\\nYu, M., Wang, D., Shan, Q., and Wan, A. The su-\\nper weight in large language models. arXiv preprint\\narXiv:2411.07191, 2024.\\nZellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi,\\nY . Hellaswag: Can a machine really finish your sentence?\\narXiv preprint arXiv:1905.07830, 2019.\\nZhang, T., Lin, Z., Yang, G., and Sa, C. D. Qpytorch: A\\nlow-precision arithmetic simulation framework, 2019.\\nZhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M.,\\nWright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al.\\nPytorch fsdp: experiences on scaling fully sharded data\\nparallel. arXiv preprint arXiv:2304.11277, 2023.\\nZhu, K., Zhao, Y ., Zhao, L., Zuo, G., Gu, Y ., Xie, D.,\\nGao, Y ., Xu, Q., Tang, T., Ye, Z., Kamahori, K., Lin,\\nC.-Y ., Wang, S., Krishnamurthy, A., and Kasikci, B.\\nNanoflow: Towards optimal large language model serv-\\ning throughput, 2024. URL https://arxiv.org/\\nabs/2408.12757.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Flash Communication\\nA. Background\\nA.1. GPU Topology\\nModern inference GPUs are connected via various hardware configurations. Figure 12 shows a typical simplified physical\\ntopology where every node contains 8 GPUs. Every two adjacent GPUs are connected via a PCIe of 64GB/s band-\\nwidth (NVIDIA, 2024c). Cross-GPU communication may take several different paths, e.g. GPU 0 to 1 has the shortest route,\\nbut from GPU 0 to 4 it has to go across two NUMA (Non-uniform memory access) nodes. Cross-node communication relies\\non NICs (Network interface cards) that transmit data via Ethernet, whose bandwidth is usually 100Gbps.\\nGPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nGPU 4\\nGPU 5\\nGPU 6\\nGPU 7\\nPCIe PCIe PCIe PCIe\\nCPUCPU\\nGPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nGPU 4\\nGPU 5\\nGPU 6\\nGPU 7\\nPCIe PCIe PCIe PCIe\\nCPUCPU\\nNIC NIC100Gbps\\n64GB/s64GB/s\\nFigure 12.Physical topology of two NVIDIA 8× L40 GPU nodes connected for inference. Each node has 8 GPUs interconnected with\\nPCI Switches and 2 NUMA nodes. For simplicity, NIC is shown to only connect with the last PCI.\\nFor high-performance large-scale training, high-end GPUs like A100 SXM GPUs enjoy a much wider bandwidth due to\\nthe combination use of NVLink (NVIDIA, 2024e), NVSwitch, and InfiniBand NIC. Each GPU in the same node directly\\nconnected with all other GPUs could reach 600GB/s via NVSwitch, and inter-node communication could have a bandwidth\\nof 200Gbps. These advanced hardware configurations tremendously accelerate the training of large language models.\\nWarp scheduling is critical for GPU utilization. L40 is shipped with 142 streaming multiprocessors (SM) while A100\\nhas 108. A warp consists of a group of 32 threads, which is the minimum scheduling unit for SM. Multiple warps can be\\nsimultaneously executed on an SM.\\nA.2. Collective Communication\\nLibraries. Due to the hierarchical design of GPU clusters, collective communication methods are crucial in distributed\\ntraining and inference. To synchronize the workloads across GPUs, communication libraries like NCCL (NVIDIA, 2024d),\\nMSCCL (Microsoft, 2024), HiCCL (Hidayetoglu et al., 2024), Gloo (Facebook, 2024), and Horovod (Sergeev & Balso,\\n2018) are developed to provide efficient collective communication operations for a group of devices. These libraries usually\\nhide the physical topology and organize GPUs in a ring (Mikami et al., 2018; Jia et al., 2018; Ying et al., 2018) or a tree. In\\nring-based topology, GPUs are connected hand by hand to create a logical circle, which maximizes the utilization of the full\\nbandwidth. In contrast, tree-based topology, especially double binary tree (Sanders et al., 2009), guarantees logarithmic\\ncommunication hops. Therefore, it is more beneficial to use ring-based communication for intra-GPUs and a tree-based\\napproach for inter-GPU clusters.\\nOperations. Collective operations such as broadcast, aggregation (Reduce/All-Reduce/Reduce-Scatter), collection\\n(Gather/All-Gather), and All2All are shipped out-of-box in most collective communication libraries. For instance, NCCL\\nprovides a series of such collective operations (NVIDIA, 2024a) where each rank processes or transmits the same amount of\\ndata. Reduce-Scatter sums data across nodes and then scatters the result to corresponding nodes. All-Gather collects data\\nfrom all nodes to all nodes. All-Reduce is a many-to-many reduce operation, where the same reduce operation is applied\\non all nodes. All2All exchanges data between all nodes, with each node sending and receiving an equal amount of data.\\nAll2All can be implemented with multiple point-to-point communication operations.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Flash Communication\\nA.3. Quantization Fundamentals\\nQuantization is a mapping from floating numbers to integers. We utilize asymmetric quantization which is formulated below,\\ns = Xmax − Xmin\\n2n − 1 , z= ⌈−Xmin\\ns ⌉ (1)\\nQ(X) =clamp(⌈X/s⌉ + z, 0, 2n − 1) (2)\\nwhere Xmax and Xmin denotes the maximum and minimum value of X, n is the quantization bit-width, s is called the\\nscale and z the zero point. Q(x) quantizes float X to integer to the target bitwidth.\\nSymmetric quantization is formulated as follows,\\ns = |X|max\\n2n−1 − 1 (3)\\nQ(X) =clamp(⌈X/s⌉, −2n−1, 2n−1 − 1) (4)\\nIEEE 754 standards for FP16. IEEE 754 (IEEE, 1985) FP16 includes 16 bits in total, which comprises 1 bit for the sign\\n(S), 5 bits for the exponent (E), and 10 bits for the mantissa or fraction (F). The bias for the exponent is 15, which means\\nthat the actual exponent value must be added to 15 to get the stored exponent value. Also, notice there’s an assumed leading\\n1 in the fractional part.\\nFP8 and FP4 Format. FP8 (Micikevicius et al., 2022) format is designed to advance FP16 with two encodings, E4M3\\n(4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa). E5M3 follows IEEE 754 conventions.\\nFP4 (Rouhani et al., 2023) is of E2M1. For quantization to FP4, we utilize QPyTorch (Zhang et al., 2019) for simulation.\\nB. Additional Experiments\\nB.1. C4 and WikiText\\nThe C4 and WikiText perplexity of FP16-weight LLaMA models is given in Table 6 while the INT8-weight version is shown\\nin Table 7. Both communication volumes are quantized with Flash communication.\\nTable 6.LLaMA models’ perplexity of C4 (upper rows) and WikiText2 (lower rows) with fine-grained communication quantization with a\\ngroup size of 128.\\nMODEL INT8 Asym FP8 INT6 Asym INT4 Asym\\n3-8B 8.89 8.90 9.20 9.68\\n3-70B 6.74 6.75 6.85 7.04\\n2-7B 6.98 6.98 7.07 7.21\\n2-13B 6.47 6.47 6.50 6.58\\n2-70B 5.52 5.52 5.55 5.60\\n3-8B 6.14 6.15 6.37 6.70\\n3-70B 2.86 2.87 3.00 3.21\\n2-7B 5.47 5.48 5.55 5.66\\n2-13B 4.88 4.89 4.93 4.99\\n2-70B 3.32 3.32 3.35 3.40\\nC. Additional Latency Measurements\\nWe list the latency measurements of LLaMA models under various configurations (weight precision, tensor parallelism,\\nGPU cards) in the following Fig. 13 and Fig. 14.\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Flash Communication\\nTable 7.The 8-bit LLaMA models’ perplexity of C4 (upper rows) and WikiText2 (lower rows) with fine-grained communication\\nquantization with a group size of 128.\\nMODEL INT8 Asym FP8 INT6 Asym INT4 Asym\\nLLAMA -2-7B 7.00 7.01 7.10 7.24\\nLLAMA -2-13B 6.51 6.51 6.55 6.63\\nLLAMA -2-70B 5.54 5.54 5.57 5.62\\nLLAMA -3-8B 9.01 9.02 9.33 9.85\\nLLAMA -3-70B 6.82 6.83 6.95 7.13\\nLLAMA -2-7B 5.50 5.51 5.59 5.69\\nLLAMA -2-13B 4.92 4.92 4.97 5.03\\nLLAMA -2-70B 3.35 3.35 3.39 3.43\\nLLAMA -3-8B 6.25 6.26 6.49 6.84\\nLLAMA -3-70B 2.96 2.97 3.13 3.32\\n8 16 32 64\\nBatch Size\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50TTFT Speed-up Ratio\\n1.29\\n1.40\\n1.53\\n1.25\\n1.36\\n1.48\\n1.21\\n1.29\\n1.39\\n1.20\\n1.28\\n1.37\\nLLaMA-3-8B (INT8) on L40, TP=2\\nFP16\\nINT8\\nINT6\\nINT4\\n8 16 32 64\\nBatch Size\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0TTFT Speed-up Ratio\\n1.091.081.10\\n1.041.051.07\\n1.021.031.05 1.081.101.11\\nLLaMA-3-8B (FP16) on L40, TP=2\\nFP16\\nINT8\\nINT6\\nINT4\\nFigure 13.TTFT speed-up ratio of LLaMA-3-8B (INT8 vs. FP16) under various communication quantization bit widths on L40 with\\nTP=2.\\n8 16 32 64\\nBatch Size\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1TTFT Speed-up Ratio\\n1.06\\n1.091.11\\n1.06\\n1.081.10\\n1.05\\n1.071.09\\n1.04\\n1.071.08\\nLLaMA-3-70B (INT8) on A100, TP=4\\nFP16\\nINT8\\nINT6\\nINT4\\n8 16 32 64\\nBatch Size\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1TTFT Speed-up Ratio\\n1.031.041.05 1.031.041.05\\n1.031.041.05 1.031.041.05\\nLLaMA-3-70B (FP16) on A100, TP=4\\nFP16\\nINT8\\nINT6\\nINT4\\nFigure 14.TTFT speed-up ratio of LLaMA-3-70B (INT8 vs. FP16) under various communication quantization bit widths on A100 with\\nTP=4.\\n15')]\n",
      "toatal pages loaaded 15\n",
      "page 1 content is: Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\n",
      "Large Language Model Infere\n",
      "metadata :{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'trapped': '/False', 'source': 'data/pdf/amy.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "##MOST RECOMMENDED TECHNIQUE\n",
    "#PyPDFLoader\n",
    "\n",
    "print(\"PyPDFLoader\")\n",
    "try:\n",
    "    pypdfloader= PyPDFLoader(\"data/pdf/amy.pdf\")\n",
    "    py_doc=pypdfloader.load()\n",
    "    print(py_doc)\n",
    "    print(f\"toatal pages loaaded {len(py_doc)}\")\n",
    "    print(f\"page 1 content is: {py_doc[0].page_content[:100]}\")\n",
    "    print(f\"metadata :{py_doc[0].metadata}\")\n",
    "except Exception as e:\n",
    "    print(f\"error : {e}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c53e0",
   "metadata": {},
   "source": [
    "##TECHNIQUE 2\n",
    "#PymuPDFloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f52838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PymuPDFLoader\n",
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 0}, page_content='Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\\nLarge Language Model Inference\\nQingyuan Li 1 Bo Zhang 1 Liang Ye 1 Yifan Zhang 1 Wei Wu 1 Yerui Sun 1 Lin Ma 1 Yuchen Xie 1\\nAbstract\\nThe ever-increasing sizes of large language mod-\\nels necessitate distributed solutions for fast infer-\\nence that exploit multi-dimensional parallelism,\\nwhere computational loads are split across vari-\\nous accelerators such as GPU clusters. However,\\nthis approach often introduces significant com-\\nmunication overhead, especially on devices with\\nlimited bandwidth. In this paper, we introduce\\nFlash Communication, a novel low-bit compres-\\nsion technique designed to alleviate the tensor-\\nparallelism communication bottleneck during in-\\nference. Our method substantially boosts intra-\\nnode communication speed by more than 3× and\\nreduces the time-to-first-token by 2×, with nearly\\nno sacrifice in model accuracy. Extensive experi-\\nments on various up-to-date LLMs demonstrate\\nthe effectiveness of our approach.\\n1. Introduction\\nTo date, the number of parameters of large language models\\nhas tremendously increased. For instance, GPT-3 (Brown\\net al., 2020) has 175B, DeepSeek V2 (Liu et al., 2024)\\nutilizes 236B, LLaMA-3 (Dubey et al., 2024) reaches 450B.\\nTheir enormous sizes create big challenges for both training\\nand inference.\\nTo tackle the scaling difficulties of large language models,\\nthe research community now resorts to multiple parallelism\\nstrategies across a large group of computing accelerators.\\nSince previous parallelism methods focus on resolving the\\ntraining challenges, we quickly review these methods for\\na background check. Particularly, data parallelism (Dean\\net al., 2012; Ben-Nun & Hoefler, 2019) is first introduced\\nto allocate the training samples onto multiple GPUs where\\neach GPU retains a duplicate of the model and processes\\nits own given batch of samples. Synchronization is hence\\nrequired at the end of each iteration to update the model pa-\\n*Equal contribution 1Meituan. Correspondence to: Qingyuan\\nLi <liqingyuan02@meituan.com>.\\nPreprint.\\nL40 FP16/FP16\\nL40 FP16/INT4\\nL40 INT8/FP16\\nL40 INT8/INT4\\nA100 FP16/FP16\\nA100 FP16/INT4\\nA100 INT8/FP16\\nA100 INT8/INT4\\n0\\n20\\n40\\n60\\n80\\n100\\nCost Percentage\\n52.2\\n71.3\\n21.9\\n40.1\\n75.1\\n80.6\\n59.2\\n65.3\\n42.4\\n22.3\\n65.9\\n37.7\\n18.4\\n11.6\\n25.1\\n17.4\\n3.4\\n3.9\\n5.6\\n10.1\\n3.8\\n4.0\\n2.8\\n6.2\\n2.0\\n2.5\\n6.6\\n12.0\\n2.7\\n3.9\\n12.9\\n11.1\\nLLaMA-3-70B Latency Breakdown\\nFP16 GEMM\\nINT8 GEMM\\nFP16 AllReduce\\nFlash AllReduce\\nNorm&Act\\nOthers\\nFigure 1. Prefill cost breakdown of LLaMA-3-70B operations\\nwith and without Flash Communication, as measured by\\nNSys (NVIDIA, 2024b). Tested on 4×L40/A100 GPUs (TP=4)\\nwith a batch size of 8, each with 1024 input and 64 output tokens.\\nNCCL (NVIDIA, 2024d)’s Ring All-Reduce is applied. The notion\\nof x-ticks (e.g. L40 FP16/FP16) denotes GPU type, model weight\\nprecision, and communication precision, respectively.\\nrameters. In the LLM era, ZeRO (Rajbhandari et al., 2020)\\nand FSDP (Zhao et al., 2023) renovate data parallelism by\\nsharding models on all devices but virtually rendering a\\nwhole model on a single device through All-Gather com-\\nmunication. In contrast, pipeline parallelism partitions se-\\nquential layers onto different GPUs where point-to-point\\ncommunication is adopted to transmit activation and gradi-\\nents. However, it creates data dependency which leads to\\nsubstantial GPU idle time, called bubbles. To improve GPU\\nutilization, GPipe (Huang et al., 2019) schedules micro-\\nbatches in a pipeline with forward passes and then followed\\nby backward passes. PipeDream (Harlap et al., 2018) pro-\\nposes one-forward one-backward (1F1B) to further reduce\\nthe bubble ratio. Megatron-LM (Narayanan et al., 2021)\\nadvances PipeDream by allowing each device to perform\\ncomputation for multiple non-contiguous subsets of layers.\\nAnother dimension to split the model is tensor parallelism\\nwhich splits the tensors of each layer and performs All-\\nReduce to aggregate the activation and gradients from all\\ndevices. Megatron-LM (Shoeybi et al., 2019; Narayanan\\n1\\narXiv:2412.04964v1  [cs.AI]  6 Dec 2024'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 1}, page_content='Flash Communication\\net al., 2021) is such an example that devises delicate con-\\njugate tensor slicing schemes (e.g. column-wise first and\\nrow-wise next for the MLP layer) to remove unnecessary\\nsynchronization inside a transformer block.\\nWith the rise of LLMs developed for the long-context sce-\\nnario, sequential parallelism (Korthikanti et al., 2023) is pro-\\nposed to divide activation of LayerNorm and Dropout layers\\nin the sequence dimension as they are sequence-independent.\\nIt also jointly combines with tensor-parallelism by replacing\\ntwo All-Reduce operations with one All-Gather and one\\nReduce-Scatter to merge the communication cost. However,\\nself-attention and MLP layers are left untouched for sequen-\\ntial parallelism. In this regard, context parallelism (NVIDIA,\\n2024b) is designed to separate all layers in sequence dimen-\\nsion. To break the sequence dependency in self-attention,\\nRing Attention (Liu et al., 2023) applies blockwise self-\\nattention and feedforward (Dao, 2023; Liu & Abbeel, 2023)\\nin a distributed environment with point-to-point communi-\\ncation. On top of this, Deepspeed-Ulysses (Jacobs et al.,\\n2023) exchanges point-to-point communication for All2All\\nfor faster speed.\\nAnother emerging direction is sparse architectures repre-\\nsented by mixture-of-experts models (Jiang et al., 2024;\\nTeam, 2024; Liu et al., 2024). Expert parallelism (Fedus\\net al., 2022) parallelizes the experts on different GPUs which\\nrequires All2All communication. Deepspeed-MoE (Rajb-\\nhandari et al., 2022) propose hierarchical All2All communi-\\ncation to reduce the number of communication hops.\\nAs large language models continue to scale up, modern\\nframeworks like DeepSpeed (Microsoft, 2024), and Mega-\\ntron (NVIDIA, 2024a) tend to make joint use of the afore-\\nmentioned parallelism to accelerate the training process.\\nNevertheless, they easily meet communication bottlenecks\\nas they require many collective operations. This overhead\\ngrows as the model becomes larger.\\nMeanwhile, the communication bottleneck is also pro-\\nnounced when serving large language models in the cloud.\\nConstrained by strict service level objectives (SLOs), multi-\\nple parallelism schemes are adopted to speed up the infer-\\nence, where tensor parallelism is the most popular option\\namong all. Besides, due to the lower bandwidth of infer-\\nence GPUs, communication can account for more than half\\nof the prefill inference cost where an 80-layer LLaMA-3-\\n70B (Dubey et al., 2024) carries out 160 all-reduce opera-\\ntions at each forward pass on 4×L40 GPUs, as shown in\\nFigure 1. Therefore, to enable a faster speed, we must make\\nefficient use of limited intra-node bandwidth.\\nIn this work, we design a novel technique to reduce the com-\\nmunication cost introduced by tensor parallelism without\\nsubstantially sacrificing accuracy. Our contributions are,\\n1. Through detailed measurements, we unveil the com-\\nmunication bottleneck problem that also recurs in large\\nlanguage model (LLM) inference. For instance, com-\\nmunication can account for up to 65% of the total\\nlatency on NVIDIA L40 GPUs (Fig. 1).\\n2. We design an efficient communication mechanism\\ncalled Flash Communication, which applies low-bit\\nfine-grained quantization on activations to reduce com-\\nmunication volume and employs a two-step all-reduce\\nstrategy to minimize communication hops.\\n3. We implement a fused CUDA kernel called Flash All-\\nReduce to perform Flash Communication, achieving\\nup to a 2× reduction in time-to-first-token (TTFT)\\non NVIDIA L40 GPUs. Even on A100 GPUs with\\nhigher communication bandwidth, we observe notable\\nlatency reductions, demonstrating the effectiveness of\\nour method.\\n2. Related Work\\nBefore diving into the investigated problem, we cover\\nsome fundamental knowledge required for discussion in\\nAppendix A. We suggest that readers without prior experi-\\nence quickly review the content.\\nCommunication efficiency is crucial to distributed training\\nand serving, as it directly affects the total processing time\\nand cost. Several techniques have been proposed to opti-\\nmize communication in distributed training in recent years,\\nincluding topology optimization, pipeline optimization, and\\ncompression methods.\\n2.1. Topology Optmization\\nTopology optimization adjusts communication patterns to\\nmatch the physical topology of hardware to reduce com-\\nmunication latency/hops, mainly ring-based and tree-based.\\nRing-All-Reduce (Baidu Research, 2024) organizes work-\\ners in a ring topology so that the overall communication\\nlatency is constant regardless of the number of workers. Say\\na worker transmits data of volume M to a group of N −1\\nworkers, the total communication volume is 2M(N −1)/N,\\nwhich is approximately 2M when N >> 1. However, it\\ndoesn’t take the physical topology into account, where intra-\\nand inter-node communication have different bandwidths.\\nHence the average speed depends largely on the lowest band-\\nwidth in such a strategy. Hierarchical Ring-All-Reduce (Jia\\net al., 2018) highlights the importance of hierarchical struc-\\ntures in managing overheads, which employs three-phase\\nall-reduce for separate intra- and inter-node communication.\\nLater, 2D-Torus (Mikami et al., 2018) organizes GPUs in a\\n2D-grid of (X, Y ) so that the inter-node horizontal commu-\\nnication volume is X times smaller than that of hierarchical\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 2}, page_content='Flash Communication\\nRing-All-Reduce. NCCL (NVIDIA, 2019) introduces dou-\\nble binary trees (Sanders et al., 2009) provides logarithmic\\nlatency by reducing hops from 2(N −1) to 2log(N). How-\\never, it is more prone to result in suboptimal bandwidth\\nutilization, as only a subset of nodes are engaged in any\\ngiven communication step.\\nWith the rise of sparse architectures like mixture-of-experts,\\nAll2All collective operation is common for communication\\nin expert parallelism. DeepSpeed-MoE (Rajbhandari et al.,\\n2022) and HetuMoE (Nie et al., 2022) both utilize a scalable\\nhierarchical scheme to reduce all-to-all communication hops\\nfor faster speed.\\nBesides, NVLink SHARP (NVIDIA, 2023) is a hardware\\nimprovement that offloads collective operations from GPUs\\nto the network devices, hence eliminating the need to send\\ndata multiple times between endpoints.\\n2.2. Pipeline Optimization\\nPipelining optimization aims to maximize resource utiliza-\\ntion with optimized scheduling strategies, mainly by over-\\nlapping computation with communication. Domino (Wang\\net al., 2024) breaks data dependency in Tensor-parallelism\\nby splitting activations row-wisely and weights column-\\nwisely into smaller independent parts. FLUX (Chang et al.,\\n2024) divides computation and communication operations\\ninto much finer-grained operations and later merges them\\nin a larger kernel to effectively hide communication. Dis-\\ntributedGEMM (Hassani et al., 2024) provides an imple-\\nmentation based on CUTLUSS (NVIDIA, 2024f) using P2P\\ncommunication. ScMoE (Cai et al., 2024) implements a\\nshortcut-connected MoE architecture to effectively decouple\\ncommunication from its conventional sequence, allowing\\nfor a substantial overlap.\\n2.3. Communication Compression\\nCompression techniques like sparsification and quantiza-\\ntion are proposed to balance communication reduction with\\nacceptable performance degradation. Sparse Communica-\\ntion (Aji & Heafield, 2017) observes that gradient updates\\nare mostly close to zero and maps them directly to zero to\\nonly exchange sparse matrices among distributed nodes. In\\ncontrast, DISCO (Qin et al., 2024) aims to achieve sparse\\ncommunication by gradually pruning the network to gen-\\nerate sparse features. QSDP (Markov et al., 2023) remove\\nFSDP’s communication bottleneck by performing both gra-\\ndient and weight quantization. ZeRO++ (Wang et al., 2023)\\napplies All-Gather with blockwise weight quantization and\\nan All2All-based gradient quantization to reduce the com-\\nmunication volume when collecting weights and gradients.\\n3. Method\\n3.1. Motivation\\nTensor Parallelism (TP) is now supported in almost all main-\\nstream inference frameworks like TensorRT-LLM (NVIDIA,\\n2023), vLLM (Kwon et al., 2023), SGLang (sgl-project,\\n2024), and LMDeploy (LMDeploy, 2023), becoming the\\nmost adopted scheme in LLM inference. However, TP\\ncomes at a non-negligible cost due to heavy communica-\\ntion, which in the case of larger language models creates an\\nexcessive communication overhead. For example, the com-\\nmunication overhead of LLaMA-3-70B on L40 GPUs easily\\nmeets the bottleneck as the input token length increases,\\nshown by the cost breakdown of LLaMA-3-70B operations\\nin Figure 2. Although high-end training-purpose accelera-\\ntors like NVIDIA A100 where GPUs are connected through\\nNVLink (NVIDIA, 2024e), the communication overhead\\nstill reaches a notable 20%. We can easily conclude that TP\\ncommunication is the inference bottleneck.\\n4\\n8\\n16\\n32\\n64\\n128\\nSequence Length\\n0\\n20\\n40\\n60\\n80\\n100\\nCost Percentage\\n78.4\\n73.6\\n65.1\\n56.3\\n55.6\\n54.8\\n17.9\\n22.1\\n31.0\\n40.0\\n41.8\\n42.8\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n1.2\\n3.1\\n3.6\\n3.0\\n2.8\\n1.6\\n1.2\\nLLaMA-3-70B Latency Breakdown\\nGEMM\\nAllReduce\\nNorm&Act\\nOthers\\nFigure 2. Prefill cost breakdown of LLaMA-3-70B operations at\\nvarious sequence lengths. Tested on 4×L40/A100 GPUs (TP=4)\\nwith a batch size of 8.\\nFor communication optimization, one can think of overlap-\\nping the communication with computation to hide overhead\\nbut it requires sophisticated design with scheduling which\\nis harder to implement in any given inference framework,\\nfor which reason NanoFlow (Zhu et al., 2024) invents a\\nnew serving framework to circumvent the difficulty. On\\nthe contrary, compression is a handy option but none of\\nthe above-mentioned methods investigated communication\\nquantization for LLM inference. It could be due to the acti-\\nvation quantization challenge posed at the inference stage,\\nwhich is pointed out by LLM.int8() (Dettmers et al., 2022)\\nand SmoothQuant (Xiao et al., 2024) that activations are\\nharder to quantize because of outliers. The communication\\nquantization method for training doesn’t suffer from this\\nproblem as it only quantizes weights and gradients (Wang\\net al., 2023). Besides, during training, communication quan-\\ntization degradation can be compensated by further learning.\\nWhereas at inference, the quantization loss is nearly irre-\\nversible.\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 3}, page_content='Flash Communication\\nAt the same time, the existing Ring All-Reduce operation\\nadopted in tensor parallelism remains a bottleneck at infer-\\nence since it is inclined to be constrained by lower band-\\nwidth. Furthermore, to integrate quantization with Ring\\nAll-Reduce, also shown by ZeRO++ (Wang et al., 2023),\\nit requires N times of sequential quantization and dequan-\\ntization in a complete Reduce-Scatter, which worsens the\\nlatency.\\nThe above issues call for a delicate orchestration of the\\nquantization approach and a better All-Reduce scheme.\\n3.2. Flash Communication\\nMotivated by the above, we approach the inference chal-\\nlenges in the distributed scenario with quantization and\\ntopology optimization. In the paper, we specifically exam-\\nine tensor parallelism as it is the most popular paradigm.\\nTake tensor parallelism in LLaMA-3 (Dubey et al., 2024) as\\nan example, the tensors of QKV projections are first sliced\\ncolumn-wisely and then the output projection row-wisely.\\nAfter that, an All-Reduce operation is required to collect\\nactivations on each device, shown in Fig. 3. Similarly in\\nthe feedforward network, the gate projection and the up\\nprojection are split by column and the down projection\\nby row, then another All-Reduce is needed to sum up the\\nactivations from both devices. To reduce the communication\\nvolume, we are left to compress the activation from the\\noutput projection and the down projection.\\nRMSNorm\\nGate \\nw/ Swish\\nUp\\nDown\\nRMSNorm\\nW1\\nW2\\nSelfAttention\\nQKV\\nOut\\n+\\nFlash \\nAll-Reduce\\nW1\\nW2\\n+\\n×\\nFlash \\nAll-Reduce\\nW1\\nW2\\nW1\\nW2\\nW1\\nW2\\nFigure 3. Tensor parallelism for a LLaMA-3 transformer block.\\nOur Flash All-Reduce is applied to speed up communication.\\n3.2.1. QUANTIZATION CHALLENGE\\nTo obtain an optimal trade-off between accuracy and latency,\\nwe choose to apply low-bit quantization. From Fig. 4, we\\nobserve that fine granularity is necessary since per-token\\nquantization at larger block sizes suffers from performance\\ncollapse in terms of C4 perplexity, albeit the asymmetric\\nversion is relatively better.\\nHowever, we discover that it is non-trivial to apply low-bit\\n8192\\n4096\\n2048\\n1024\\n512\\n256\\n128\\nBlock Size\\n102\\n104\\n106\\nC4 Perplexity\\nper-token \\nPer-token Fine-granularity\\nINT4Sym\\nINT4Asym\\nFigure 4. Activation quantization with various block sizes of\\nLLaMA-3-8B on C4. Starting from 4096 (the length of hidden\\ndimension), the granularity becomes finer till 128.\\nactivation quantization in this scenario. To investigate the\\nquantization sensitivity, we calculate the layerwise mean\\nsquared errors (MSE) before and after activation quantiza-\\ntion on LLaMA-3-8B, as depicted in Figure 5. We find that\\nthe down projection dproj is much harder to quantize than\\nthe output projection oproj, as the former MSEs are quite\\ndistinct even on a logarithmic scale. This phenomenon is\\nalso discovered by (Li et al., 2023; Ashkboos et al., 2023;\\nYu et al., 2024).\\n0\\n10\\n20\\n30\\nLayer\\n10\\n6\\n10\\n5\\n10\\n4\\n10\\n3\\n10\\n2\\nMSE\\ndproj\\noproj\\n0\\n10\\n20\\n30\\nLayer\\n10\\n7\\n10\\n6\\n10\\n5\\n10\\n4\\n10\\n3\\nMSE\\nRS (INT4)\\nAG (INT8)\\nAG (INT4)\\nFigure 5. Left: Comparison of oproj and dproj All-Reduce Quan-\\ntization MSE. Right: MSE of quantization before Reduce-Scatter\\n(RS) vs. All-Gather (AG).\\nBesides, an All-Reduce comprises a pair of Reduce-Scatter\\nand All-Gather operations, where the quantization corre-\\nsponding to each operation exhibits different levels of diffi-\\nculty, see Fig. 5 right. This is as expected since the quantiza-\\ntion before Reduce-Scatter only introduces rounding errors\\nwhile in the case of All-Gather, it includes both rounding\\nand accumulated errors. Alternatively, we could use a higher\\nprecision for the quantization before All-Gather to improve\\naccuracy.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 4}, page_content='Flash Communication\\nGPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nA0\\nA1\\nA2\\nA3\\nB0\\nB1\\nB2\\nB3\\nC0\\nC1\\nC2\\nC3\\nD0\\nD1\\nD2\\nD3\\nQ\\nA0\\nB0\\nC0\\nD0\\nA1\\nB1\\nC1\\nD1\\nA2\\nB2\\nC2\\nD2\\nA3\\nB3\\nC3\\nD3\\nDQ\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nDQ\\nA0+B0 \\n+ \\nC0+D0\\nA1+B1\\n+ \\nC1+D1\\nA2+B2 \\n+ \\nC2+D2\\nA3+B3 \\n+ \\nC3+D3\\nQ\\nAll2All\\nReduce \\nSum\\nAll \\nGather\\nFigure 6. Flash communication’s two-step All-Reduce. The communication volume is quantized and dequantized only twice.\\n3.2.2. ALGORITHM\\nConsidering the above issues, we design a two-step quanti-\\nzation strategy to replace vanilla Ring All-Reduce, as por-\\ntrayed in Fig. 6. Its integration with tensor parallelism is\\nshown in Fig.3. We name our overall strategy as two-step\\nAll-Reduce.\\nFig. 6 illustrates how Flash Communication works. First,\\nwe divide the computation volume (activation) on each GPU\\nby the number of ranks. After fine-grained quantization on\\nactivation, we perform All2All communication so that each\\ndevice receives its computation load for reduction. After\\non-device reduction, the sum is again quantized to speed up\\nthe transmission. We then perform All-Gather to collect all\\nresults and dequantization to recover float values on each\\ndevice. This two-step workflow is also formulated by Alg. 1.\\nAlgorithm 1 Flash All-Reduce\\nInput: Communication volume M, world size N, chunk\\nsize C, quantization bit-width b, group size g\\nOutput: Reduced sum Sdq\\nDivide M into T = ⌈M/C⌉chunks.\\nfor 1 ≤i ≤T do\\n// Quantize volume to obtain zeros and scales\\nM q\\ni , zi, si = FinegrainedQuantize(Mi, b, g);\\n// Each device sends and receives volume from others\\nAll2All(M q\\ni , zi, si, N);\\nfor 1 ≤j ≤N do\\nM dq\\nij = Dequantize(M q\\nij, zij, ij);\\nend for\\nSi = ReduceSum(M dq\\ni0 , M dq\\ni1 , · · · , M dq\\niN );\\nSq\\ni , zs\\ni , ss\\ni = FinegrainedQuantize(Si, b, g);\\n// Each device collects the reduced sum from others\\nAll-Gather(Sq\\ni , zs\\ni , ss\\ni, N);\\nfor 1 ≤j ≤N do\\nSdq\\nij = Dequantize(Sq\\ni , zs\\ni , ss\\ni);\\nend for\\nend for\\nTable 1. Comparison of Ring All-Reduce vs. Flash All-Reduce\\nMETHOD\\nRING ALL-REDUCE FLASH ALL-REDUCE\\nTOTAL VOLUME\\n2M(N −1)/N\\n2M(N −1)/N\\nREDUCE STEP\\nN −1\\n1\\nREDUCE-SCATTER\\nM/N\\nM(N −1)/N\\nGATHER STEP\\nN −1\\n1\\nALL-GATHER\\nM/N\\nM(N −1)/N\\nQDQ STEP\\nN\\n2\\n3.2.3. KERNEL DESIGN\\nFor efficiency, we implement a fused Flash All-Reduce\\nkernel to encompass all the above collective communication\\noperations and quantization steps. Compared with Ring\\nAll-Reduce in Table 1, Flash All-Reduce cuts quantization-\\ndequantization steps from N to 2, and Reduce/Gather steps\\nfrom N −1 to 1. Although the size of total volumes remains\\nthe same, each of our volumes is quantized to lower bits,\\nsubstantially reducing the amount of data to transmit. We\\nsummarize three key aspects in designing our kernel below.\\nFast Fine-grained Quantization The total communication\\nvolume M for each rank is divided into T chunks for trans-\\nmission. Given a chunk size C, we draw how GPU threads\\nare organized in parallel to process the chunk information\\nin Fig. 7. A chunk is split into N blocks and each block\\ncorresponds to 32 warps, where each warp is a collection of\\n32 threads and each thread can process eight FP16 elements.\\nTake our asymmetric quantization with a group size of 128\\nas an example, we perform quantization on each group of\\n128 elements using 16 threads. Specifically, we leverage\\nthe CUDA API function\\nshfl xor sync to iteratively\\nexchange information among these warp threads to achieve\\nmax/min reduction efficiently.\\nFast Communication. Instead of calling All2All primitive,\\nwe utilize GPU peer direct memory access from CUDA Run-\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 5}, page_content='Flash Communication\\nTotal 1024 threads\\nQuantized \\nVolume\\nscales & zeros\\nWarp1\\nWarp0\\n……\\nWarp31\\nWarp30\\nWarp0 Warp1\\nWarp30 Warp31\\n……\\n……\\n……\\n……\\n……\\n……\\nWarp0 Warp1\\nWarp30 Warp31\\nBlock 0\\nBlock 1\\nBlock N\\n8192 x INT8\\n128 x FP16\\nFigure 7. Thread mapping of fast fine-grained quantization.\\ntime API (NVIDIA Corporation, 2024) to transmit quan-\\ntized volume, where we can directly fetch data from differ-\\nent ranks, substantially boosting the communication speed.\\nFast Dequantization. Once quantization volumes are re-\\nceived, we have to dequantize them into FP16 for sum re-\\nduction. As na¨ıve INT4 to FP16 conversion incurs over-\\nhead, we utilize the dequantization layout in (Kim et al.,\\n2022). To coordinate its ordering online, we also employ\\nfast INT4 packing (LMDeploy, 2023), as illustrated in\\nFig. 8. Given that two 32-bit unsigned integers U0 and\\nU1 holding 4 INT4-quantized activations (each stored in\\nthe lower 4 bits out of 8 bits) to transmit, we first perform\\nthe right shift by 12 bits, and then apply bitwise OR to\\nitself. Later we select the target bits from these two inte-\\ngers with CUDA Math API\\nbyte perm. In this way,\\nwe can pack 8 4-bit integers in a convenient order to de-\\nquantize. Next we apply lop3.b321 to perform logical\\noperation (0xF0 & 0xCC)|0xAA on the packed vari-\\nable with mask 0x000F000F and 0x64006400, then we\\nsubtract it with 0x64006400, which effectively represents\\nW1 and W0 in FP16. The dequantization can be performed\\niteratively by varying the masks for the rest INT4 integers.\\nfW1\\nfW0\\nW7\\nW6\\nW5\\nW4\\nW3\\nW2\\nW1\\nW0\\nlop3.b32 (\\n, 0x000F000F, 0x64006400,\\n(0xF0 & 0xCC) | 0xAA ) \\nW5\\nW7\\nW1\\nW3\\nW4\\nW6\\nW0\\nW2\\n, 0x64006400)\\nsub.f16x2 (\\n0x4\\n0x6\\nW1\\n0x0\\n0x4\\n0x6\\nW0\\n0x0\\nW7\\nW6\\nW5\\nW7\\nW4\\nW6\\nW3\\nW2\\nW1\\nW3\\nW0\\nW2\\n__byte_perm (\\n, 0x5140)\\n,\\nU0 |= U0 >> 12\\nU1 |= U1 >> 12\\nW7\\nW6\\nW3\\nW2\\n}\\nFigure 8. Fast INT4 packing and dequantization. For simplicity,\\nonly the dequantization of W1 and W0 is shown. An empty square\\nmeans zero.\\n1https://docs.nvidia.com/cuda/parallel-\\nthread-execution/#logic-and-shift-\\ninstructions-lop3\\nINT6 Quantization. Given that quantization prior to All-\\nGather leads to greater loss, as illustrated in Figure 5 (left),\\nwe opt for an INT8 bit-width for All-Gather operations and\\nmaintain INT4 for ReduceSum, effectively creating an INT6\\nsolution. As later shown in Table 3, the INT6 configuration\\nstrikes a commendable balance between performance and\\ncommunication efficiency.\\n4. Experiments\\n4.1. Setup\\nUnless otherwise noted, we use an input token length of\\n1024 and an output token length of 64 for the inference mea-\\nsurement. Latencies are tested on NVIDIA L40 and A100\\nSXM GPUs. The baseline uses FP16 for communication.\\n4.2. Accuracy Comparison\\nFP16 Weights. We evaluate the accuracy of LLaMA-2 and\\nLLaMA-3 models on PIQA (Bisk et al., 2020), ARCC and\\nARCE (Clark et al., 2018), HellaSwag (Zellers et al., 2019),\\nWinoGrande (Sakaguchi et al., 2021) in various commu-\\nnication quantization bit widths, shown in Table 2. In all\\ncases, asymmetric INT8 quantization obtains the best ac-\\ncuracies. Asymmetric INT4 is also better than symmetric\\nINT4. C4 (Raffel et al., 2020) and WikiText (Merity et al.,\\n2016) results are shown in Table 6 of Appendix B.1.\\nTable 2. Accuracy of LLaMA models with various communication\\nquantization strategies. All model weights are kept in FP16 pre-\\ncision. Prec: Communication precision. All INT quantization is\\nasymmetrical. HS: HellaSwag, WG: WinoGrande.\\nMODEL PREC PIQA ARCC ARCE\\nHS\\nWG\\nAVG\\n2-7B\\nFP16 79.11 46.33\\n74.58 76.01 69.30 64.83\\nINT8 79.11 45.99\\n74.75 76.10 69.06 64.77\\nINT6 78.78 45.99\\n74.79 75.75 68.75 64.68\\nINT4 78.02 45.82\\n74.49 75.63 67.25 64.23\\n2-13B\\nFP16 80.52 49.15\\n77.48 79.39 72.14 67.83\\nINT8 80.69 49.06\\n77.61 79.34 71.59 67.78\\nINT6 79.98 49.32\\n77.06 79.17 71.11 67.11\\nINT4 79.60 48.21\\n76.89 78.92 71.90 67.04\\n2-70B\\nFP16 82.70 57.34\\n81.02 83.80 77.98 72.43\\nINT8 82.75 57.68\\n80.93 83.80 77.98 72.47\\nINT6 82.48 57.00\\n80.85 83.77 76.87 72.23\\nINT4 82.92 57.25\\n80.43 83.60 77.27 71.99\\n3-8B\\nFP16 80.79 53.41\\n77.69 79.16 72.77 68.64\\nINT8 80.58 52.47\\n77.40 79.09 73.09 68.45\\nINT6 79.98 51.37\\n77.65 78.73 73.16 68.06\\nINT4 80.09 51.02\\n75.84 78.11 70.48 66.92\\n3-70B\\nFP16 84.55 64.33\\n85.86 84.89 80.35 75.25\\nINT8 84.55 63.91\\n85.82 84.90 80.82 75.27\\nINT6 84.11 61.69\\n85.44 84.87 80.35 74.76\\nINT4 83.13 61.35\\n83.33 84.69 78.93 73.82\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 6}, page_content='Flash Communication\\n8\\n16\\n32\\n64\\nBatch Size\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nTTFT Speed-up Ratio\\n1.48\\n1.72\\n2.06\\n1.45\\n1.68\\n1.99\\n1.39\\n1.58\\n1.82\\n1.37\\n1.55\\n1.78\\nLLaMA-3-8B (INT8) on L40, TP=4\\nFP16\\nINT8\\nINT6\\nINT4\\n8\\n16\\n32\\n64\\nBatch Size\\n0.6\\n0.8\\n1.0\\n1.2\\nTTFT Speed-up Ratio\\n1.10\\n1.16\\n1.19\\n1.10\\n1.16\\n1.19\\n1.10\\n1.15\\n1.19\\n1.09\\n1.15\\n1.18\\nLLaMA-3-70B (INT8) on A100, TP=8\\nFP16\\nINT8\\nINT6\\nINT4\\nFigure 9. TTFT speed-up ratio of 8-bit LLaMA-3-8B under various communication quantization bit widths on L40 with TP=4 (left), and\\n8-bit LLaMA-3-70B on A100 with TP=8 (right).\\nINT8 Weights. As model weights are quantized, the im-\\npact of communication overhead becomes more pronounced.\\nThis observation motivates us to explore communication\\nquantization in this context. We begin by quantizing the\\nweights of the LLaMA model using Smoothquant (Xiao\\net al., 2024). Subsequently, we implement fine-grained com-\\nmunication quantization to assess its impact on performance\\nas detailed in Table 3. Results on C4 (Raffel et al., 2020),\\nWikiText-2 (Merity et al., 2016) is shown in Appendix B.1.\\nTable 3. Accuracy of LLaMA models with various communication\\nquantization strategies (denoted as ‘Comm Prec’). All model\\nweights are quantized into INT8 precision with SmoothQuant\\n(α = 0.85 except 0.9 for LLaMA2-70B). Prec: Communication\\nprecision. All INT quantization is asymmetrical.\\nMODEL PREC PIQA ARCC ARCE\\nHS\\nWG\\nAVG\\n2-7B\\nFP16 79.00 46.16\\n74.24 75.89 68.75 68.81\\nINT8 79.27 45.65\\n74.37 75.89 68.51 68.74\\nINT6 78.56 45.39\\n74.75 75.75 68.67 68.62\\nINT4 77.53 45.31\\n74.20 75.51 68.59 68.23\\n2-13B\\nFP16 80.25 49.49\\n77.27 79.37 71.59 71.59\\nINT8 80.41 49.32\\n77.53 79.21 72.22 71.74\\nINT6 80.09 49.40\\n77.02 79.14 71.74 71.48\\nINT4 79.11 49.06\\n76.30 78.89 71.27 70.93\\n2-70B\\nFP16 83.08 57.94\\n80.98 83.54 77.66 76.64\\nINT8 83.08 57.76\\n80.68 83.55 78.14 76.64\\nINT6 82.81 57.94\\n80.85 83.78 76.80 76.44\\nINT4 82.92 56.83\\n80.35 83.41 78.14 76.33\\n3-8B\\nFP16 80.36 51.96\\n77.69 78.71 73.48 72.44\\nINT8 80.09 52.90\\n77.57 78.79 73.09 72.49\\nINT6 80.03 52.56\\n79.12 78.38 71.82 72.38\\nINT4 78.94 50.17\\n77.10 77.87 70.56 70.93\\n3-70B\\nFP16 84.44 63.99\\n85.56 84.55 79.72 79.65\\nINT8 83.84 63.48\\n85.56 84.67 79.79 79.47\\nINT6 83.57 61.26\\n83.67 84.66 80.58 78.75\\nINT4 82.70 61.60\\n83.29 84.51 77.82 77.98\\n4.3. Latency and Throughput Performance\\nFig. 9 illustrates weight-quantized LLaMA-3-8B and\\nLLaMA-3-70B’s TTFT comparison with and without Flash\\ncommunication. The lowest quantization bit yields the most\\ngain, i.e. 2.06× and 1.19× on L40 and A100 respectively.\\nMore measurements are listed in Appendix C.\\n5. Ablation Study\\n5.1. Integer vs. Low-bit Float\\nTable. 4 shows the difference between INTx and FPx com-\\nmunication quantization. In general, INT8 performs compa-\\nrably with FP8, while the asymmetric version of INT8 is the\\nbest among all. FP6 is a mixed version of FP8 (Micikevicius\\net al., 2022) and FP4 (Rouhani et al., 2023), which is a fair\\ncomparison with similarly mixed INT6.\\nTable 4. LLaMA models’ C4 Perplexity with INTx vs FPx quanti-\\nzation with a group size of 128. Weights are in FP16.\\nPREC\\n2-7B\\n2-13B\\n2-70B\\n3-8B\\n3-70B\\nFP8Sym\\n6.98\\n6.47\\n5.52\\n8.90\\n6.75\\nINT8Sym\\n6.98\\n6.47\\n5.52\\n8.90\\n6.74\\nINT8Asym\\n6.98\\n6.47\\n5.52\\n8.89\\n6.74\\nFP6Sym\\n7.09\\n6.52\\n5.56\\n9.22\\n6.87\\nINT6Sym\\n7.15\\n6.57\\n5.59\\n9.48\\n6.94\\nINT6Asym\\n7.08\\n6.51\\n5.55\\n9.20\\n6.86\\nFP4Sym\\n7.24\\n6.60\\n5.61\\n9.72\\n7.07\\nINT4Asym\\n7.50\\n6.71\\n5.69\\n10.51\\n7.30\\nINT4Asym\\n7.21\\n6.58\\n5.60\\n9.68\\n7.04\\n5.2. Na¨ıve vs. Rotation-based Quantization\\nAs previously shown in Fig. 4, the C4 perplexity of\\ncoarse quantization suffers performance collapse, while fine-\\ngrained quantization gradually resolves the problem as the\\ngroup size increases. We investigate whether the Hadamard\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 7}, page_content='Flash Communication\\ntransform popularized by QuaRot (Ashkboos et al., 2024)\\ncould alleviate the issue in the coarse setting in Table 5. It\\nturns out that Hadamard transform with coarse quantization\\nreadily performs well, however, it loses its advantage in\\nfiner granularity cases as compared with na¨ıve asymmetric\\nquantization. Besides, it doesn’t exhibit gains on FP8.\\nTable 5. LLaMA models’ C4 perplexity with communication quan-\\ntization (na¨ıve vs. rotation) at various granularity levels and pre-\\ncision formats. Asymmetric quantization is used for INT4 and\\nsymmetric for FP8. HT: Hadamard Transform\\nMODELS GROUP\\nINT4\\nW/ HT FP8 W/ HT\\n3-8B\\n8192\\n2363367.8 10.61 8.91\\n8.96\\n2048\\n284.56\\n10.11 8.91\\n8.96\\n128\\n9.68\\n9.67\\n8.90\\n8.94\\n3-70B\\n8192\\n7417.79\\n7.86\\n6.75\\n6.81\\n1024\\n10.47\\n7.76\\n6.75\\n6.81\\n128\\n7.04\\n7.64\\n6.75\\n6.82\\n2-7B\\n8192\\n47601520\\n7.86\\n7.01\\n7.01\\n1024\\n8.78\\n7.35\\n6.98\\n7.01\\n128\\n7.21\\n7.24\\n6.98\\n7.01\\n2-13B\\n8192\\n306.39\\n6.80\\n6.47\\n6.49\\n1024\\n7.43\\n6.68\\n6.47\\n6.49\\n128\\n6.58\\n6.62\\n6.47\\n6.49\\n2-70B\\n8192\\n49.96\\n5.71\\n5.52\\n5.54\\n1024\\n6.17\\n5.67\\n5.52\\n5.54\\n128\\n5.60\\n5.64\\n5.52\\n5.54\\n5.3. Flash All-Reduce vs. Ring All-Reduce\\n64MB\\n128MB\\n256MB\\n512MB\\n1GB\\nCommunication Volume\\n0\\n20000\\n40000\\n60000\\nLatency ( s)\\n1.74x\\n2.27x\\n3.18x\\nAll-Reduce Latency on L40, TP=4\\nNCCL Ring AR\\nFlash AR INT8\\nFlash AR INT6\\nFlash AR INT4\\nFigure 10. Flash Communication’s All-Reduce (Flash AR) Perfor-\\nmance compared with NCCL’s ring version. NCCL’s latency is\\ntested with nccl-test (NVIDIA, 2016-2024).\\nAssembling several boosting techniques, the speed of our\\nFlash All-Reduce kernel surpasses that of Ring All-Reduce\\nby a large margin. Fig. 10 exhibits the latency measurement\\ngiven a certain amount of communication volume. When the\\ncommunication volume becomes obvious (e.g. larger than\\n64MB), our quantized All-Reduce is crucial to reduce the\\ncost, where the INT4 version brings at most 3.18× kernel\\nlatency reduction. Noticeably, the mixed precision version\\nINT6 obtains a good trade-off between INT8 and INT4.\\nWe further show that the number of streaming processors\\n(SMs) matters in Fig. 11. When the communication volume\\nis of small size, a smaller number of SMs is beneficial as\\nless kernel launch and inter-block synchronization overhead\\nis produced. When the volume gets larger, more SMs are\\nrequired for calculation. A configuration of 48 SMs strikes\\na better balance between communication and computation.\\n16KB\\n32KB\\n64KB\\n128KB\\n256KB\\n512KB\\n1MB\\n20\\n40\\nLatency ( s)\\nAll-Reduce Latency on L40, TP=4\\nSM=4\\nSM=48\\nSM=128\\n2MB\\n4MB\\n8MB\\n16MB\\n32MB\\n64MB\\n128MB\\n256MB\\n512MB\\n1GB\\nCommunication Volume\\n0\\n20000\\nLatency ( s)\\nSM=4\\nSM=48\\nSM=128\\nFigure 11. The number of SMs affects the communication latency\\nat different sizes of communication volume. SM=48 is similar to\\nSM=128 in larger volumes.\\n6. Conclusion\\nOur work presents a novel technique to reduce the commu-\\nnication volume associated with tensor parallelism while\\nmaintaining accuracy. Our key contributions include a com-\\nprehensive analysis that reveals the communication bottle-\\nneck in Large Language Model (LLM) inference, the design\\nof a fast communication mechanism known as Flash Com-\\nmunication, and the demonstration of its implementation,\\nwhich has been shown to achieve up to a 2× TTFT reduction.\\nFlash Communication employs fine-grained quantization on\\nactivations and a two-step All-Reduce strategy to decrease\\ncommunication volumes significantly. We have conducted\\nextensive experiments on NVIDIA L40 and A100 GPUs\\nacross various configurations and with several state-of-the-\\nart LLMs, which have consistently demonstrated the effec-\\ntiveness of our approach. These findings address a critical\\nchallenge in parallel computing and pave the way for more\\nefficient and scalable LLM inference.\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 8}, page_content='Flash Communication\\nReferences\\nAji, A. F. and Heafield, K.\\nSparse communication for\\ndistributed gradient descent.\\nIn Proceedings of the\\n2017 Conference on Empirical Methods in Natural Lan-\\nguage Processing. Association for Computational Lin-\\nguistics, 2017.\\ndoi: 10.18653/v1/d17-1045.\\nURL\\nhttp://dx.doi.org/10.18653/v1/D17-1045.\\nAshkboos, S., Markov, I., Frantar, E., Zhong, T., Wang, X.,\\nRen, J., Hoefler, T., and Alistarh, D. Towards end-to-\\nend 4-bit inference on generative large language models.\\narXiv preprint arXiv:2310.09259, 2023.\\nAshkboos, S., Mohtashami, A., Croci, M. L., Li, B.,\\nCameron, P., Jaggi, M., Alistarh, D., Hoefler, T., and\\nHensman, J. Quarot: Outlier-free 4-bit inference in ro-\\ntated llms. arXiv preprint arXiv:2404.00456, 2024.\\nBaidu Research. baidu-allreduce: A C++ library demonstrat-\\ning ring allreduce and ring allgather techniques. GitHub\\nrepository, 2024.\\nURL https://github.com/\\nbaidu-research/baidu-allreduce.\\nBen-Nun, T. and Hoefler, T. Demystifying parallel and dis-\\ntributed deep learning: An in-depth concurrency analysis.\\nACM Computing Surveys (CSUR), 52(4):1–43, 2019.\\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\\nabout physical commonsense in natural language. In Pro-\\nceedings of the AAAI conference on artificial intelligence,\\nvolume 34, pp. 7432–7439, 2020.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\n2020.\\nCai, W., Jiang, J., Qin, L., Cui, J., Kim, S., and Huang,\\nJ.\\nShortcut-connected expert parallelism for accel-\\nerating mixture-of-experts, 2024.\\nURL https://\\narxiv.org/abs/2404.05019.\\nChang, L.-W., Bao, W., Hou, Q., Jiang, C., Zheng, N.,\\nZhong, Y., Zhang, X., Song, Z., Yao, C., Jiang, Z., Lin,\\nH., Jin, X., and Liu, X. Flux: Fast software-based com-\\nmunication overlap on gpus through kernel fusion, 2024.\\nURL https://arxiv.org/abs/2406.06858.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\nquestion answering? try arc, the ai2 reasoning challenge.\\narXiv preprint arXiv:1803.05457, 2018.\\nDao, T.\\nFlashattention-2:\\nFaster attention with bet-\\nter parallelism and work partitioning. arXiv preprint\\narXiv:2307.08691, 2023.\\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M.,\\nMao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K.,\\net al. Large scale distributed deep networks. Advances in\\nneural information processing systems, 25, 2012.\\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,\\nL. Llm.int8(): 8-bit matrix multiplication for transform-\\ners at scale, 2022. URL https://arxiv.org/abs/\\n2208.07339.\\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\\nA., et al. The llama 3 herd of models. arXiv preprint\\narXiv:2407.21783, 2024.\\nFacebook. Gloo: Collective communications library with\\nvarious primitives for multi-machine training. https:\\n//github.com/facebookincubator/gloo,\\n2024. Accessed: 2024-11-23.\\nFedus, W., Zoph, B., and Shazeer, N. Switch transformers:\\nScaling to trillion parameter models with simple and ef-\\nficient sparsity. Journal of Machine Learning Research,\\n23(120):1–39, 2022.\\nHarlap, A., Narayanan, D., Phanishayee, A., Seshadri, V.,\\nDevanur, N., Ganger, G., and Gibbons, P. Pipedream:\\nFast and efficient pipeline parallel dnn training. arXiv\\npreprint arXiv:1806.03377, 2018.\\nHassani,\\nA.,\\nIsaev,\\nM.,\\nMcDonald,\\nN.,\\nRen,\\nJ.,\\nThakkar, V., Wu, H., and Shi, H.\\nDistributed\\ngemm, 2024. URL https://blog.shi-labs.com/\\ndistributed-gemm-88be6a481e2b. Accessed:\\n2024-12-04.\\nHidayetoglu, M., de Gonzalo, S. G., Slaughter, E., Surana,\\nP., Hwu, W.-m., Gropp, W., and Aiken, A. Hiccl: A hier-\\narchical collective communication library. arXiv preprint\\narXiv:2408.05962, 2024.\\nHuang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen,\\nM., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe:\\nEfficient training of giant neural networks using pipeline\\nparallelism. Advances in neural information processing\\nsystems, 32, 2019.\\nIEEE.\\nIEEE Standard for Floating-Point Arithmetic.\\nTechnical Report IEEE 754-1985, Institute of Elec-\\ntrical and Electronics Engineers, New York, NY,\\n1985.\\nURL https://standards.ieee.org/\\nieee/754/6210/.\\nJacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song,\\nS. L., Rajbhandari, S., and He, Y. Deepspeed ulysses:\\nSystem optimizations for enabling training of extreme\\nlong sequence transformer models.\\narXiv preprint\\narXiv:2309.14509, 2023.\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 9}, page_content='Flash Communication\\nJia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F.,\\nXie, L., Guo, Z., Yang, Y., Yu, L., et al. Highly scal-\\nable deep learning training system with mixed-precision:\\nTraining imagenet in four minutes.\\narXiv preprint\\narXiv:1807.11205, 2018.\\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,\\nB., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna,\\nE. B., Bressand, F., et al. Mixtral of experts. arXiv\\npreprint arXiv:2401.04088, 2024.\\nKim, Y. J., Henry, R., Fahim, R., and Awadalla, H. H.\\nWho says elephants can’t run: Bringing large scale\\nmoe models into cloud scale production. arXiv preprint\\narXiv:2211.10017, 2022.\\nKorthikanti, V. A., Casper, J., Lym, S., McAfee, L., Ander-\\nsch, M., Shoeybi, M., and Catanzaro, B. Reducing acti-\\nvation recomputation in large transformer models. Pro-\\nceedings of Machine Learning and Systems, 5:341–353,\\n2023.\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\\nmemory management for large language model serving\\nwith pagedattention. In Proceedings of the ACM SIGOPS\\n29th Symposium on Operating Systems Principles, 2023.\\nLi, Q., Zhang, Y., Li, L., Yao, P., Zhang, B., Chu, X., Sun,\\nY., Du, L., and Xie, Y. Fptq: Fine-grained post-training\\nquantization for large language models. arXiv preprint\\narXiv:2308.15987, 2023.\\nLiu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C.,\\nDengr, C., Ruan, C., Dai, D., Guo, D., et al. Deepseek-v2:\\nA strong, economical, and efficient mixture-of-experts\\nlanguage model. arXiv preprint arXiv:2405.04434, 2024.\\nLiu, H. and Abbeel, P. Blockwise parallel transformers for\\nlarge context models. In Oh, A., Naumann, T., Globerson,\\nA., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances\\nin Neural Information Processing Systems, volume 36,\\npp. 8828–8844. Curran Associates, Inc., 2023.\\nLiu, H., Zaharia, M., and Abbeel, P. Ring attention with\\nblockwise transformers for near-infinite context. arXiv\\npreprint arXiv:2310.01889, 2023.\\nLMDeploy.\\nLmdeploy: A toolkit for compressing, de-\\nploying, and serving llm.\\nhttps://github.com/\\nInternLM/lmdeploy, 2023.\\nMarkov, I., Vladu, A., Guo, Q., and Alistarh, D. Quantized\\ndistributed training of large models with convergence\\nguarantees, 2023. URL https://arxiv.org/abs/\\n2302.02390.\\nMerity, S., Xiong, C., Bradbury, J., and Socher, R.\\nPointer sentinel mixture models.\\narXiv preprint\\narXiv:1609.07843, 2016.\\nMicikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey,\\nP., Grisenthwaite, R., Ha, S., Heinecke, A., Judd, P.,\\nKamalu, J., et al. Fp8 formats for deep learning. arXiv\\npreprint arXiv:2209.05433, 2022.\\nMicrosoft.\\nDeepSpeed: Deep learning optimization li-\\nbrary.\\nGitHub repository, 2024.\\nURL https://\\ngithub.com/microsoft/DeepSpeed.\\nMicrosoft.\\nMicrosoft collective communication library\\n(msccl).\\nhttps://github.com/microsoft/\\nmsccl, 2024. Accessed: 2024-11-23.\\nMikami, H., Suganuma, H., Tanaka, Y., Kageyama, Y., et al.\\nMassively distributed sgd: Imagenet/resnet-50 training in\\na flash. arXiv preprint arXiv:1811.05233, 2018.\\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Pat-\\nwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P.,\\nBernauer, J., Catanzaro, B., et al. Efficient large-scale\\nlanguage model training on gpu clusters using megatron-\\nlm. In Proceedings of the International Conference for\\nHigh Performance Computing, Networking, Storage and\\nAnalysis, pp. 1–15, 2021.\\nNie, X., Zhao, P., Miao, X., Zhao, T., and Cui, B. Hetumoe:\\nAn efficient trillion-scale mixture-of-expert distributed\\ntraining system, 2022. URL https://arxiv.org/\\nabs/2203.14685.\\nNVIDIA.\\nNCCL Tests.\\nhttps://github.com/\\nNVIDIA/nccl-tests, 2016-2024. Accessed: 2024-\\n12-06.\\nNVIDIA.\\nMassively scale your deep learning training\\nwith nccl 2.4. https://developer.nvidia.com/\\nblog/massively-scale-deep-learning-\\ntraining-nccl-2-4/, 2019. Accessed: 2024-11-\\n23.\\nNVIDIA.\\nNVIDIA Scalable Hierarchical Aggrega-\\ntion and Reduction Protocol (SHARP) v2.6.1 Release\\nNotes. Technical Report Revision 2.6.1, NVIDIA, 2023.\\nURL https://docs.nvidia.com/networking/\\ndisplay/sharpv261/release+notes. Last up-\\ndated on May 23, 2023.\\nNVIDIA.\\nTensorRT-LLM.\\nGitHub\\nrepository,\\n2023.\\nURL\\nhttps://github.com/NVIDIA/\\nTensorRT-LLM.\\nNVIDIA. Megatron-LM: Ongoing research training trans-\\nformer models at scale. GitHub repository, 2024a. URL\\nhttps://github.com/NVIDIA/Megatron-LM.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 10}, page_content='Flash Communication\\nNVIDIA.\\nNVIDIA Nsight Systems.\\nWeb Page,\\n2024b. URL https://developer.nvidia.com/\\nnsight-systems.\\nNVIDIA.\\nCollective\\noperations.\\nhttps:\\n//docs.nvidia.com/deeplearning/\\nnccl/user-guide/docs/usage/\\ncollectives.html,\\n2024a.\\nAccessed:\\n2024-\\n11-23.\\nNVIDIA.\\nContext parallelism overview.\\nhttps:\\n//docs.nvidia.com/megatron-core/\\ndeveloper-guide/latest/api-guide/\\ncontext parallel.html,\\n2024b.\\nAccessed:\\n2024-11-23.\\nNVIDIA. Nvidia l40: Delivering unprecedented visual\\ncomputing performance for the data center.\\nhttps:\\n//images.nvidia.cn/content/Solutions/\\ndata-center/vgpu-L40-datasheet.pdf,\\n2024c. Accessed: 2024-11-23.\\nNVIDIA. Optimized primitives for collective multi-gpu\\ncommunication. https://github.com/NVIDIA/\\nnccl, 2024d. Accessed: 2024-11-23.\\nNVIDIA. Nvlink & nvswitch for advanced multi-gpu com-\\nmunication.\\nhttps://www.nvidia.com/en-us/\\ndata-center/nvlink/, 2024e. Accessed: 2024-\\n11-26.\\nNVIDIA.\\nCuda templates for linear algebra subrou-\\ntines, 2024f. URL https://github.com/NVIDIA/\\ncutlass. Accessed: 2024-12-04.\\nNVIDIA Corporation.\\nPeer Device Memory Ac-\\ncess.\\nTechnical report, NVIDIA, 2024.\\nURL\\nhttps://docs.nvidia.com/cuda/cuda-\\nruntime-api/group CUDART PEER.html.\\nAccessed: 2024-12-04.\\nQin, M., Sun, C., Hofmann, J., and Vucinic, D. Disco:\\nDistributed inference with sparse communications. In\\nProceedings of the IEEE/CVF Winter Conference on Ap-\\nplications of Computer Vision, pp. 2432–2440, 2024.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y., Li, W., and Liu, P. Exploring\\nthe limits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research, 21\\n(140):1–67, 2020.\\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero:\\nMemory optimizations toward training trillion parameter\\nmodels. In SC20: International Conference for High Per-\\nformance Computing, Networking, Storage and Analysis,\\npp. 1–16. IEEE, 2020.\\nRajbhandari, S., Li, C., Yao, Z., Zhang, M., Aminabadi,\\nR. Y., Awan, A. A., Rasley, J., and He, Y. Deepspeed-moe:\\nAdvancing mixture-of-experts inference and training to\\npower next-generation ai scale. In International con-\\nference on machine learning, pp. 18332–18346. PMLR,\\n2022.\\nRouhani, B. D., Zhao, R., More, A., Hall, M., Khodamoradi,\\nA., Deng, S., Choudhary, D., Cornea, M., Dellinger, E.,\\nDenolf, K., et al. Microscaling data formats for deep\\nlearning. arXiv preprint arXiv:2310.10537, 2023.\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\\nWinogrande: An adversarial winograd schema challenge\\nat scale. Communications of the ACM, 64(9):99–106,\\n2021.\\nSanders, P., Speck, J., and Tr¨aff, J. L. Two-tree algorithms\\nfor full bandwidth broadcast, reduction and scan. Parallel\\nComputing, 35(12):581–594, 2009.\\nSergeev, A. and Balso, M. D.\\nHorovod: fast and easy\\ndistributed deep learning in TensorFlow. arXiv preprint\\narXiv:1802.05799, 2018.\\nsgl-project. SGLang: A Fast Serving Framework for Large\\nLanguage Models and Vision Language Models. GitHub\\nrepository, 2024. URL https://github.com/sgl-\\nproject/sglang. Accessed: 2024-12-01.\\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\\nJ., and Catanzaro, B.\\nMegatron-lm: Training multi-\\nbillion parameter language models using model paral-\\nlelism. arXiv preprint arXiv:1909.08053, 2019.\\nTeam, Q.\\nQwen1.5-moe: Matching 7b model perfor-\\nmance with 1/3 activated parameters”, February 2024.\\nURL https://qwenlm.github.io/blog/qwen-\\nmoe/.\\nWang, G., Qin, H., Jacobs, S. A., Holmes, C., Rajbhandari,\\nS., Ruwase, O., Yan, F., Yang, L., and He, Y. Zero++:\\nExtremely efficient collective communication for giant\\nmodel training. arXiv preprint arXiv:2306.10209, 2023.\\nWang, G., Zhang, C., Shen, Z., Li, A., and Ruwase, O.\\nDomino: Eliminating communication in llm training\\nvia generic tensor slicing and overlapping, 2024. URL\\nhttps://arxiv.org/abs/2409.15241.\\nXiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han,\\nS. Smoothquant: Accurate and efficient post-training\\nquantization for large language models, 2024.\\nURL\\nhttps://arxiv.org/abs/2211.10438.\\nYing, C., Kumar, S., Chen, D., Wang, T., and Cheng, Y. Im-\\nage classification at supercomputer scale. arXiv preprint\\narXiv:1811.06992, 2018.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 11}, page_content='Flash Communication\\nYu, M., Wang, D., Shan, Q., and Wan, A.\\nThe su-\\nper weight in large language models. arXiv preprint\\narXiv:2411.07191, 2024.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\\nY. Hellaswag: Can a machine really finish your sentence?\\narXiv preprint arXiv:1905.07830, 2019.\\nZhang, T., Lin, Z., Yang, G., and Sa, C. D. Qpytorch: A\\nlow-precision arithmetic simulation framework, 2019.\\nZhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M.,\\nWright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al.\\nPytorch fsdp: experiences on scaling fully sharded data\\nparallel. arXiv preprint arXiv:2304.11277, 2023.\\nZhu, K., Zhao, Y., Zhao, L., Zuo, G., Gu, Y., Xie, D.,\\nGao, Y., Xu, Q., Tang, T., Ye, Z., Kamahori, K., Lin,\\nC.-Y., Wang, S., Krishnamurthy, A., and Kasikci, B.\\nNanoflow: Towards optimal large language model serv-\\ning throughput, 2024. URL https://arxiv.org/\\nabs/2408.12757.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 12}, page_content='Flash Communication\\nA. Background\\nA.1. GPU Topology\\nModern inference GPUs are connected via various hardware configurations. Figure 12 shows a typical simplified physical\\ntopology where every node contains 8 GPUs. Every two adjacent GPUs are connected via a PCIe of 64GB/s band-\\nwidth (NVIDIA, 2024c). Cross-GPU communication may take several different paths, e.g. GPU 0 to 1 has the shortest route,\\nbut from GPU 0 to 4 it has to go across two NUMA (Non-uniform memory access) nodes. Cross-node communication relies\\non NICs (Network interface cards) that transmit data via Ethernet, whose bandwidth is usually 100Gbps.\\nGPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nGPU 4\\nGPU 5\\nGPU 6\\nGPU 7\\nPCIe\\nPCIe\\nPCIe\\nPCIe\\nCPU\\nCPU\\nGPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nGPU 4\\nGPU 5\\nGPU 6\\nGPU 7\\nPCIe\\nPCIe\\nPCIe\\nPCIe\\nCPU\\nCPU\\nNIC\\nNIC\\n100Gbps\\n64GB/s\\n64GB/s\\nFigure 12. Physical topology of two NVIDIA 8× L40 GPU nodes connected for inference. Each node has 8 GPUs interconnected with\\nPCI Switches and 2 NUMA nodes. For simplicity, NIC is shown to only connect with the last PCI.\\nFor high-performance large-scale training, high-end GPUs like A100 SXM GPUs enjoy a much wider bandwidth due to\\nthe combination use of NVLink (NVIDIA, 2024e), NVSwitch, and InfiniBand NIC. Each GPU in the same node directly\\nconnected with all other GPUs could reach 600GB/s via NVSwitch, and inter-node communication could have a bandwidth\\nof 200Gbps. These advanced hardware configurations tremendously accelerate the training of large language models.\\nWarp scheduling is critical for GPU utilization. L40 is shipped with 142 streaming multiprocessors (SM) while A100\\nhas 108. A warp consists of a group of 32 threads, which is the minimum scheduling unit for SM. Multiple warps can be\\nsimultaneously executed on an SM.\\nA.2. Collective Communication\\nLibraries. Due to the hierarchical design of GPU clusters, collective communication methods are crucial in distributed\\ntraining and inference. To synchronize the workloads across GPUs, communication libraries like NCCL (NVIDIA, 2024d),\\nMSCCL (Microsoft, 2024), HiCCL (Hidayetoglu et al., 2024), Gloo (Facebook, 2024), and Horovod (Sergeev & Balso,\\n2018) are developed to provide efficient collective communication operations for a group of devices. These libraries usually\\nhide the physical topology and organize GPUs in a ring (Mikami et al., 2018; Jia et al., 2018; Ying et al., 2018) or a tree. In\\nring-based topology, GPUs are connected hand by hand to create a logical circle, which maximizes the utilization of the full\\nbandwidth. In contrast, tree-based topology, especially double binary tree (Sanders et al., 2009), guarantees logarithmic\\ncommunication hops. Therefore, it is more beneficial to use ring-based communication for intra-GPUs and a tree-based\\napproach for inter-GPU clusters.\\nOperations.\\nCollective operations such as broadcast, aggregation (Reduce/All-Reduce/Reduce-Scatter), collection\\n(Gather/All-Gather), and All2All are shipped out-of-box in most collective communication libraries. For instance, NCCL\\nprovides a series of such collective operations (NVIDIA, 2024a) where each rank processes or transmits the same amount of\\ndata. Reduce-Scatter sums data across nodes and then scatters the result to corresponding nodes. All-Gather collects data\\nfrom all nodes to all nodes. All-Reduce is a many-to-many reduce operation, where the same reduce operation is applied\\non all nodes. All2All exchanges data between all nodes, with each node sending and receiving an equal amount of data.\\nAll2All can be implemented with multiple point-to-point communication operations.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 13}, page_content='Flash Communication\\nA.3. Quantization Fundamentals\\nQuantization is a mapping from floating numbers to integers. We utilize asymmetric quantization which is formulated below,\\ns = Xmax −Xmin\\n2n −1\\n, z = ⌈−Xmin\\ns\\n⌉\\n(1)\\nQ(X) = clamp(⌈X/s⌉+ z, 0, 2n −1)\\n(2)\\nwhere Xmax and Xmin denotes the maximum and minimum value of X, n is the quantization bit-width, s is called the\\nscale and z the zero point. Q(x) quantizes float X to integer to the target bitwidth.\\nSymmetric quantization is formulated as follows,\\ns = |X|max\\n2n−1 −1\\n(3)\\nQ(X) = clamp(⌈X/s⌉, −2n−1, 2n−1 −1)\\n(4)\\nIEEE 754 standards for FP16. IEEE 754 (IEEE, 1985) FP16 includes 16 bits in total, which comprises 1 bit for the sign\\n(S), 5 bits for the exponent (E), and 10 bits for the mantissa or fraction (F). The bias for the exponent is 15, which means\\nthat the actual exponent value must be added to 15 to get the stored exponent value. Also, notice there’s an assumed leading\\n1 in the fractional part.\\nFP8 and FP4 Format. FP8 (Micikevicius et al., 2022) format is designed to advance FP16 with two encodings, E4M3\\n(4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa). E5M3 follows IEEE 754 conventions.\\nFP4 (Rouhani et al., 2023) is of E2M1. For quantization to FP4, we utilize QPyTorch (Zhang et al., 2019) for simulation.\\nB. Additional Experiments\\nB.1. C4 and WikiText\\nThe C4 and WikiText perplexity of FP16-weight LLaMA models is given in Table 6 while the INT8-weight version is shown\\nin Table 7. Both communication volumes are quantized with Flash communication.\\nTable 6. LLaMA models’ perplexity of C4 (upper rows) and WikiText2 (lower rows) with fine-grained communication quantization with a\\ngroup size of 128.\\nMODEL\\nINT8Asym\\nFP8\\nINT6Asym\\nINT4Asym\\n3-8B\\n8.89\\n8.90\\n9.20\\n9.68\\n3-70B\\n6.74\\n6.75\\n6.85\\n7.04\\n2-7B\\n6.98\\n6.98\\n7.07\\n7.21\\n2-13B\\n6.47\\n6.47\\n6.50\\n6.58\\n2-70B\\n5.52\\n5.52\\n5.55\\n5.60\\n3-8B\\n6.14\\n6.15\\n6.37\\n6.70\\n3-70B\\n2.86\\n2.87\\n3.00\\n3.21\\n2-7B\\n5.47\\n5.48\\n5.55\\n5.66\\n2-13B\\n4.88\\n4.89\\n4.93\\n4.99\\n2-70B\\n3.32\\n3.32\\n3.35\\n3.40\\nC. Additional Latency Measurements\\nWe list the latency measurements of LLaMA models under various configurations (weight precision, tensor parallelism,\\nGPU cards) in the following Fig. 13 and Fig. 14.\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 14}, page_content='Flash Communication\\nTable 7. The 8-bit LLaMA models’ perplexity of C4 (upper rows) and WikiText2 (lower rows) with fine-grained communication\\nquantization with a group size of 128.\\nMODEL\\nINT8Asym\\nFP8\\nINT6Asym\\nINT4Asym\\nLLAMA-2-7B\\n7.00\\n7.01\\n7.10\\n7.24\\nLLAMA-2-13B\\n6.51\\n6.51\\n6.55\\n6.63\\nLLAMA-2-70B\\n5.54\\n5.54\\n5.57\\n5.62\\nLLAMA-3-8B\\n9.01\\n9.02\\n9.33\\n9.85\\nLLAMA-3-70B\\n6.82\\n6.83\\n6.95\\n7.13\\nLLAMA-2-7B\\n5.50\\n5.51\\n5.59\\n5.69\\nLLAMA-2-13B\\n4.92\\n4.92\\n4.97\\n5.03\\nLLAMA-2-70B\\n3.35\\n3.35\\n3.39\\n3.43\\nLLAMA-3-8B\\n6.25\\n6.26\\n6.49\\n6.84\\nLLAMA-3-70B\\n2.96\\n2.97\\n3.13\\n3.32\\n8\\n16\\n32\\n64\\nBatch Size\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\nTTFT Speed-up Ratio\\n1.29\\n1.40\\n1.53\\n1.25\\n1.36\\n1.48\\n1.21\\n1.29\\n1.39\\n1.20\\n1.28\\n1.37\\nLLaMA-3-8B (INT8) on L40, TP=2\\nFP16\\nINT8\\nINT6\\nINT4\\n8\\n16\\n32\\n64\\nBatch Size\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nTTFT Speed-up Ratio\\n1.091.081.10\\n1.041.051.07\\n1.021.031.05\\n1.081.101.11\\nLLaMA-3-8B (FP16) on L40, TP=2\\nFP16\\nINT8\\nINT6\\nINT4\\nFigure 13. TTFT speed-up ratio of LLaMA-3-8B (INT8 vs. FP16) under various communication quantization bit widths on L40 with\\nTP=2.\\n8\\n16\\n32\\n64\\nBatch Size\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1\\nTTFT Speed-up Ratio\\n1.06\\n1.091.11\\n1.06\\n1.081.10\\n1.05\\n1.071.09\\n1.04\\n1.071.08\\nLLaMA-3-70B (INT8) on A100, TP=4\\nFP16\\nINT8\\nINT6\\nINT4\\n8\\n16\\n32\\n64\\nBatch Size\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1\\nTTFT Speed-up Ratio\\n1.031.041.05\\n1.031.041.05\\n1.031.041.05\\n1.031.041.05\\nLLaMA-3-70B (FP16) on A100, TP=4\\nFP16\\nINT8\\nINT6\\nINT4\\nFigure 14. TTFT speed-up ratio of LLaMA-3-70B (INT8 vs. FP16) under various communication quantization bit widths on A100 with\\nTP=4.\\n15')]\n",
      "toatal pages loaaded 15\n",
      "page 1 content is: Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\n",
      "Large Language Model Infere\n",
      "metadata :{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-09T01:35:50+00:00', 'source': 'data/pdf/amy.pdf', 'file_path': 'data/pdf/amy.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast Large Language Model Inference', 'author': 'Qingyuan Li, Bo Zhang, Liang Ye, Yifan Zhang, Wei Wu, Yerui Sun, Lin Ma, Yuchen Xie', 'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'keywords': 'Machine Learning, ICML', 'moddate': '2024-12-09T01:35:50+00:00', 'trapped': '', 'modDate': 'D:20241209013550Z', 'creationDate': 'D:20241209013550Z', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"PymuPDFLoader\")\n",
    "try:\n",
    "    pymupdfloader= PyMuPDFLoader(\"data/pdf/amy.pdf\")\n",
    "    pymu_doc=pymupdfloader.load()\n",
    "    print(pymu_doc)\n",
    "    print(f\"toatal pages loaaded {len(pymu_doc)}\")\n",
    "    print(f\"page 1 content is: {pymu_doc[0].page_content[:100]}\")\n",
    "    print(f\"metadata :{pymu_doc[0].metadata}\")\n",
    "except Exception as e:\n",
    "    print(f\"error : {e}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc63067",
   "metadata": {},
   "source": [
    "#HANDLING PDF CHALLENGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0273b085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncleaned text data\n",
      "'Retrieval Augmented Generation \\n\\n\\n       (RAG) is a technique\\n\\n   that improves LLM responses\\n\\n     '\n",
      "cleaned text data\n",
      "'Retrieval Augmented Generation (RAG) is a technique that improves LLM responses by retrieving releva'\n"
     ]
    }
   ],
   "source": [
    "raw_pdf_text=\"\"\"Retrieval Augmented Generation \n",
    "\n",
    "\n",
    "       (RAG) is a technique\n",
    "\n",
    "   that improves LLM responses\n",
    "\n",
    "         by retrieving relevant documents\n",
    "\n",
    "   from an external knowledge base.\n",
    "   page 1 0f 100\n",
    "   \"\"\"\n",
    "#now handling(cleaning the pdf text)\n",
    "\n",
    "def clean(text):\n",
    "    #removing white spaces\n",
    "    text=\" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "cleaned=clean(raw_pdf_text)\n",
    "print(\"uncleaned text data\")\n",
    "print(repr(raw_pdf_text[:100]))\n",
    "print(\"cleaned text data\")\n",
    "print(repr(cleaned[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65f8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "class PDFprocessor:\n",
    "    \"\"\"pdf parser with error handling\"\"\"\n",
    "    def __init__(self,chunk_size=1000,chunk_overlap=100):\n",
    "        self.chunk_size=chunk_size\n",
    "        self.chunk_overlap=chunk_overlap\n",
    "        self.text_splitter=RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\" \"]\n",
    "\n",
    "        )\n",
    "    \n",
    "    def process_pdf(self,pdf_path:str)->List[Document]:\n",
    "        \"\"\"Process pdf with smart chunking and metadata enhancement\"\"\"\n",
    "\n",
    "        #load the PDF first\n",
    "        loader=PyPDFLoader(pdf_path)\n",
    "        pages=loader.load()\n",
    "\n",
    "        #Process each page\n",
    "        processed_chunks=[]\n",
    "\n",
    "        for page_num,page in enumerate(pages):\n",
    "            ##clean text\n",
    "            cleaned_text=self._clean_text(page.page_content)\n",
    "\n",
    "\n",
    "            #skip the empty page\n",
    "            if len(cleaned_text.strip())<50:\n",
    "                continue\n",
    "\n",
    "            #\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51fd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
