{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87dd68b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "'''\n",
    "HuggingfaceEmbeddings library is used to call any Embedding model from hugginface ....so we dont use API's to access huggingface.\n",
    "'''\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b9249e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:Iam sri, and i love to work with data and build things\n",
      "embedding length:384\n",
      "[-0.020332854241132736, -0.04270913824439049, 0.08428642898797989, 0.04762633517384529, 0.000777849112637341, -0.06750527024269104, 0.03414014354348183, -0.022578556090593338, -0.0009445329778827727, 0.0993163213133812]\n"
     ]
    }
   ],
   "source": [
    "text=\"Iam sri, and i love to work with data and build things\"\n",
    "#as we r using sentence transformer --so we r converting sentence to a vector not word into a vector\n",
    "\n",
    "embedding=embeddings.embed_query(text)\n",
    "#if we r giving a single sentence --we use embed query \n",
    "#if set of sentences then we use embed_documents\n",
    "\n",
    "print(f\"text:{text}\")\n",
    "print(f\"embedding length:{len(embedding)}\") #to now the length of the vetor\n",
    "print(embedding[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c01b9c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.010001374408602715, -0.07492920011281967, -0.025106344372034073, -0.04580504074692726, 0.04348403215408325, 0.0859014242887497, -0.04405798763036728, -0.009629819542169571, 0.05409308522939682, -0.06795546412467957]\n",
      "[-0.054930057376623154, 0.028621304780244827, -0.049060359597206116, -0.011415311135351658, 0.0010668424656614661, -0.07176735252141953, -0.027192212641239166, -0.004794688895344734, 0.08260942250490189, 0.0007992819882929325]\n",
      "[-0.008873152546584606, -0.038087379187345505, -0.047977838665246964, -0.048341862857341766, 0.06846880912780762, -0.0006188902189023793, -0.0056940545327961445, -0.0030105444602668285, 0.043129052966833115, -0.06474781036376953]\n"
     ]
    }
   ],
   "source": [
    "content=[\n",
    "   \"Vector embeddings are numerical representations of data (like text, images, or audio) that capture their meaning in a mathematical form.\",\n",
    "   \"They convert complex information into vectors (lists of numbers) so that machines can understand and compare them.\",\n",
    "   \"Similar items have embeddings that are closer together in vector space, while different items are farther apart.\",\n",
    "   \"In NLP, words or sentences with similar meanings have similar embeddings.\",\n",
    "   \"Vector embeddings are widely used in search engines, recommendation systems, clustering, and RAG (Retrieval-Augmented Generation).\",\n",
    "   \"They help models find relevance, similarity, and context efficiently.\"\n",
    "]\n",
    "\n",
    "embed=embeddings.embed_documents(content)\n",
    "print(embed[0][:10])\n",
    "print(embed[1][:10])\n",
    "print(embed[2][:10])\n",
    "\n",
    "#each sentence is converted into one vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f806382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
